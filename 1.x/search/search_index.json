{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Health Record Ingestion service \u00b6 The HRI is a deployment ready service for streaming Health-related data into the IBM Cloud. It provides a \u201cfront door\u201d for \u201cData Integrators\u201d to send data into the cloud, while supporting both batch-processing and data streaming workflows. It provides features to initiate and track the movement of a dataset for both \u201cData Integrators\u201d and \u201cData Consumers\u201d . The key features are: Streaming - all data is streamed Batch support - a collection of health data records can be streamed and processed together Multitenancy - supports segregation of data by tenant and Data Integrator Key Technologies \u00b6 Event Streams , an IBM Cloud-based Apache Kafka managed service, is the technology used for producing and consuming the data streams Using IBM Cloud Functions , HRI exposes a Serverless RESTful Management API that is used to control and configure the system Elasticsearch is the distributed NoSQL data store that is used to store information about batches IBM Cloud Dependencies \u00b6 The HRI was developed on the IBM Cloud and currently does not support running on other public or private clouds. However, as a part of Alvearie, the goal is to support other public and private clouds, which the team continues to work towards. Core Architecture \u00b6 Topics \u00b6 Health data, which may include PHI , is written to and read from the Kafka topics. There must be separate topics for each tenant and Data Integrator in order to meet data separability requirements. A set of four topics is used per Stream of data that flows through the HRI. Data Integrators write data to and Data Consumers read data from the *.in topic. Batch status notifications are written to the *.notification topic. Batches \u00b6 Health Record Datasets often have requirements to be processed together \u201cas a set\u201d (partially or in their entirety) when moving the data into the cloud. Hence, HRI has been built with support to process a dataset as a Batch . See Batch for a detailed definition. How much data goes in a batch is really up to the solution. The HRI Management API provides support for starting, completing, terminating, and searching for batches. Any change to a batch results in a message being written to the associated notification topic in Kafka. Data Format \u00b6 HRI does not impose any requirements on the format of the Health Data records written to Kafka. There is a separate effort to define a common FHIR model for PHI data. However, The HRI does require the batchId to be in the record header. Data Integrators may include any number of additional custom header values that they wish to pass onto data consumers. An example of a custom header value might be something like originating_producer_id , an originating data producer (or org) ID value that may need to be communicated to the data consumers. Additional Reading \u00b6 Processing Flows API specification Multi-tenancy Deployment Administration Authorization Monitoring & Logging Troubleshooting Releases Glossary Questions \u00b6 Please contact these team members for further questions: David N. Perkins, Team Lead: david.n.perkins@ibm.com Aram S. Openden, Maintainer: aram.openden1@ibm.com Contributors \u00b6 Alisa Kotliarova: alisa@ibm.com Daniel Baxter: djbaxter@us.ibm.com Fred Ricci: fjricci@us.ibm.com","title":"Introduction"},{"location":"#health-record-ingestion-service","text":"The HRI is a deployment ready service for streaming Health-related data into the IBM Cloud. It provides a \u201cfront door\u201d for \u201cData Integrators\u201d to send data into the cloud, while supporting both batch-processing and data streaming workflows. It provides features to initiate and track the movement of a dataset for both \u201cData Integrators\u201d and \u201cData Consumers\u201d . The key features are: Streaming - all data is streamed Batch support - a collection of health data records can be streamed and processed together Multitenancy - supports segregation of data by tenant and Data Integrator","title":"Health Record Ingestion service"},{"location":"#key-technologies","text":"Event Streams , an IBM Cloud-based Apache Kafka managed service, is the technology used for producing and consuming the data streams Using IBM Cloud Functions , HRI exposes a Serverless RESTful Management API that is used to control and configure the system Elasticsearch is the distributed NoSQL data store that is used to store information about batches","title":"Key Technologies"},{"location":"#ibm-cloud-dependencies","text":"The HRI was developed on the IBM Cloud and currently does not support running on other public or private clouds. However, as a part of Alvearie, the goal is to support other public and private clouds, which the team continues to work towards.","title":"IBM Cloud Dependencies"},{"location":"#core-architecture","text":"","title":"Core Architecture"},{"location":"#topics","text":"Health data, which may include PHI , is written to and read from the Kafka topics. There must be separate topics for each tenant and Data Integrator in order to meet data separability requirements. A set of four topics is used per Stream of data that flows through the HRI. Data Integrators write data to and Data Consumers read data from the *.in topic. Batch status notifications are written to the *.notification topic.","title":"Topics"},{"location":"#batches","text":"Health Record Datasets often have requirements to be processed together \u201cas a set\u201d (partially or in their entirety) when moving the data into the cloud. Hence, HRI has been built with support to process a dataset as a Batch . See Batch for a detailed definition. How much data goes in a batch is really up to the solution. The HRI Management API provides support for starting, completing, terminating, and searching for batches. Any change to a batch results in a message being written to the associated notification topic in Kafka.","title":"Batches"},{"location":"#data-format","text":"HRI does not impose any requirements on the format of the Health Data records written to Kafka. There is a separate effort to define a common FHIR model for PHI data. However, The HRI does require the batchId to be in the record header. Data Integrators may include any number of additional custom header values that they wish to pass onto data consumers. An example of a custom header value might be something like originating_producer_id , an originating data producer (or org) ID value that may need to be communicated to the data consumers.","title":"Data Format"},{"location":"#additional-reading","text":"Processing Flows API specification Multi-tenancy Deployment Administration Authorization Monitoring & Logging Troubleshooting Releases Glossary","title":"Additional Reading"},{"location":"#questions","text":"Please contact these team members for further questions: David N. Perkins, Team Lead: david.n.perkins@ibm.com Aram S. Openden, Maintainer: aram.openden1@ibm.com","title":"Questions"},{"location":"#contributors","text":"Alisa Kotliarova: alisa@ibm.com Daniel Baxter: djbaxter@us.ibm.com Fred Ricci: fjricci@us.ibm.com","title":"Contributors"},{"location":"admin/","text":"HRI Administration \u00b6 HRI Administration tasks include: Managing Tenants Onboarding Data Integrators Manually through the Event Streams UI Using the Management API Stream Endpoints HRI Management User Authorization Managing Tenants \u00b6 Every Tenant has a separate index in Elasticsearch . Indexes are named <tenantId>-batches . For example, if the tenant ID is 24 , the new index name will be 24-batches . Some solutions may include a tenant prefix, e.g. tenant24-batches . The tenant ID may contain any lowercase alphanumeric strings, - , and _ . Whatever pattern you use, this will determine the tenant ID path parameter required in most of the Management API endpoints , and will need to be communicated to Data Integrators for that tenant. If you are using an existing deployment, check with team managing the instance on naming conventions. There are four Management API endpoints that support Tenant management in Elasticsearch for HRI: Create, Get (all tenants), Get (specific tenant) and Delete. Please note that all four of these endpoints require IAM authentication - you will need to pass in an IAM Bearer token as part of the authorization header in the requests. Create Tenant \u00b6 Use the Management API Create Tenant endpoint to create new Tenants. This will create a new index for the Tenant in Elasticsearch. The Create Tenant endpoint takes in one path parameter tenantId , which may only contain lowercase alphanumeric characters, - , and _ . For example, for the tenantId \u201c24\u201d you would use the following curl command: curl -X POST \\ <hri_base_url>/tenants/24 \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json' Get Tenants \u00b6 The Get endpoint takes in no parameters and returns a list of all tenantIds that have an Elastic index. Assuming the above Create was run, then the following cURL command (HTTP/Get operation) would return a list containing the single tenantId \u201c24\u201d: curl -X GET \\ <hri_base_url>/tenants \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json' Get Tenant \u00b6 The GetTenant endpoint can also take in a tenantId and will return a list of information on the associated index. Assuming the above Create was run, then the following cURL command (HTTP/Get operation) would return a list of information on the index for \u201c24\u201d: curl -X GET \\ <hri_base_url>/tenants/24 \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json' Delete Tenant \u00b6 Like Create , the Delete Tenant endpoint takes in tenantId . The following curl command will delete the elastic index for 24 : curl -X DELETE \\ <hri_base_url>/tenants/24 \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json' Onboarding New Data Integrators \u00b6 Every unique combination of Tenant and Data Integrator must have a separate \u2018stream\u2019, path of data through the HRI, in order to satisfy HIPAA data isolation requirements. See Multi-tenancy for more details. Every stream includes two topics, and they can be added manually through the Event Streams UI and API or automatically through the Management API. Topic Naming Conventions: \u00b6 Please note that HRI uses the following naming conventions for topics: ingest.<tenantId>.<dataIntegratorId>[.metadataTag].in ingest.<tenantId>.<dataIntegratorId>[.metadataTag].notification The metadataTag is an optional field that can be set to any user defined value. For example, with tenant id 24 , Data Integrator id data-int-1 , and metadata tag claims , the topics would be: ingest.24.data-int-1.claims.in ingest.24.data-int-1.claims.notification The tenant ID must be consistent with the Elasticsearch index tenant ID . Only use lowercase alphanumeric characters, - , and _ . Topic Sizing Recommendations \u00b6 Topic sizing mainly consists of the number of partitions and the retention policy (time and amount of data), and mainly depends on three factors: the size of batches the frequency of batches the throughput of Data Consumers The number of partitions determines how many Data Consumer processes can read and process the data in parallel, with one process per partition. The retention policy determines how much data the topic will store before removing it due to time or size constraints. The throughput of Data Consumers is mainly dependent on what is done with the data after reading it. At a minimum, the Data Consumers must be faster than the long term rate of incoming data. Otherwise, data may be removed from the topic before it is consumed, resulting in data loss. For example, if a 10 GB batch is written by Data Integrators every 4 hours, the Data Consumers must have a process rate greater than 2.5 GBs per hour. Otherwise, they will continually fall behind and eventually not process some data before it is removed from the topic. For initial sizing, estimate the peak data load size (could include multiple batches depending on your solution) and divide by 1 GB (compressed) to get the number of partitions. For example, if the peak data load is 10 GBs, then start with 10 partitions. Also set the retention size to 10 GBs. In production environments, the retention time is typically 3-7 days. With longer times, you may need to also increase the retention size. As the frequency and size of data loads increase, the number of partitions and retention policy should also increase. As the throughput of Data Consumers increases, the number of partitions and retention policy should decrease. NOTE: if creating topics manually, only 1 partition is needed for the *.notification topic. Manually through the Event Streams UI \u00b6 Create the required Kafka topics described above using the IBM Cloud Event Streams UI. Create the new Topic \u00b6 To Create the new topics, in your IBM Cloud account navigate to your Event Streams service. Click on the Topics tab/link on the left of your screen. Next, click on the \u201cCreate Topic\u201d button on the upper right-hand side of the screen: Enter your topic name, as defined by the naming conventions above, the number of partitions, and the retention time. Note that you must toggle Show all available options to see the partition and retention policy options. Using the Management API stream endpoints \u00b6 There are three Management API Stream endpoints: Create , Get , and Delete . Please note that all three of these endpoints require IAM authentication - you will need to pass in an IAM Bearer token as part of the authorization header in the requests. In the case of Create and Delete , the IAM bearer token must be associated with a user who has Manager role permissions. For Get , the bearer token must be associated with a user who has at least Reader role permissions. See Event Streams documentation for details on permissions. Create Stream \u00b6 Instead of manually creating the topics for the Tenant and Data Integrator pairing, you may choose to use the Management API Create Stream endpoint which will create the topics for you, and take into account if validation is enabled. The Create Stream endpoint takes in two path parameters, tenantId and streamId , where streamId is made up of the Data Integrator ID and an optional qualifier, delimited by \u2018.\u2019. Both tenantId and streamdId may only contain lowercase alphanumeric characters, - , and _ . streamdId may also contain one \u2018.\u2019. For example, for the tenantId \u201ctenant24\u201d, Data Integrator ID \u201cdata-int-1\u201d and optional qualifier \u201cqualifier1\u201d, you could use the following curl command: curl -X POST \\ <hri_base_url>/tenants/tenant24/streams/data-int-1.qualifier1 \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json' \\ -d '{ \"numPartitions\":1, \"retentionMs\":86400000 }' This will create the following topics: ingest.tenant24.data-int-1.qualifier1.in ingest.tenant24.data-int-1.qualifier1.notification Note: numPartitions and retentionMs topic configurations are required. There are other optional configurations that can also be passed in, see the Stream Api Spec for more details on these optional fields. The numPartitions parameter is applied to the *.in topic, but the *.notification topic is set to 1. Get Streams \u00b6 The Get Streams endpoint takes in tenantId as a path parameter, and returns a list of all streamId \u2018s associated with that tenant. Assuming the above Create was run, then the following cURL command (HTTP/Get operation) would return a list containing the single streamId data-int-1.qualifier1 : curl -X GET \\ <hri_base_url>/tenants/tenant24/streams \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json' Delete Stream \u00b6 Like Create , the Delete Stream endpoint takes in two path parameters, tenantId and streamId . The following curl command will delete both the ingest.tenant24.data-int-1.qualifier1.in and ingest.tenant24.data-int-1.qualifier1.notification topics: curl -X DELETE \\ <hri_base_url>/tenants/tenant24/streams/data-int-1.qualifier1 \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json' Note that HRI topic naming conventions require topics to start with the prefix \u201cingest\u201d and end with the suffix \u201cin\u201d or \u201cnotification\u201d. Both the Get and Delete endpoints will ignore any topics that don\u2019t follow this convention. Creating Service Credentials for Kafka Permissions \u00b6 You have to create an Event Streams (Kafka) Service Credential for every client that will need to read from or write to one or more topics. Typically, every Data Integrator and downstream Data Consumer will need their own service credential. A service credential can be configured with IAM policies to just grant read and or write access to specific topics and consumer groups, so only one service credential is needed for each entity. You do not need to create a service credential for every topic. Each service credential will initially have read or write access to all topics when created depending on whether the \u2018Reader\u2019 or \u2018Writer\u2019 role is selected respectively. But they can be configured with IAM policies to just grant read and or write access to specific topics and consumer groups regardless of which role is selected. It\u2019s good practice to select \u2018Writer\u2019 for Data Integrators and \u2018Reader\u2019 for downstream consumers. To create a service credential, navigate to the Event Streams - Service Credentials page, and then click on the \u201cNew Credential\u201d button on the right-hand side of your screen: You can see in the Screenshot below an example of creating a Data Integrator service credential with the Writer role: Next, go to the IBM Cloud (Access) IAM Management tools to further restrict the service credential, by using the \u201cManage\u201d drop-down menu at the top of your screen and choosing \u2018Access (IAM)\u2019. Then select \u2018Service IDs\u2019 from the left menu. Next select the Service ID for the credential you created. If you selected \u2018Auto Generate\u2019 when creating the credential, it will have the same name, but be careful, because there can be multiple Service IDs with the same name. After selecting the Service ID, go to the \u2018Access policies\u2019 tab. You should see one policy that is created by default, which allows read or write access to all topics. To restrict access to particular topics, you have to modify the existing policy and create several new ones. Below are rules about what policies to create for specific access. Create a policy with \u2018Reader\u2019 service access and \u2018Resource type\u2019 set to cluster . This will allow the Service ID to access the Event Streams brokers. To allow read & write permissions to a particular topic, create a policy with \u2018Reader\u2019 and \u2018Writer\u2019 service access, \u2018Resource type\u2019 set to topic , and \u2018Resource ID\u2019 set to the topic name. To allow just read permissions to a particular topic, create a policy with \u2018Reader\u2019 service access, \u2018Resource type\u2019 set to topic , and \u2018Resource ID\u2019 set to the topic name. To allow subscribing to topics, the Service ID must be given permissions to create consumer groups. This is the standard way of consuming messages from Kafka. Create a policy with \u2018Reader\u2019 service access, \u2018Resource type\u2019 set to group , and \u2018Resource ID\u2019 set to a unique ID for this client followed by a * using \u2018string matches\u2019, e.g. data-int-1* . This allows the client to only create consumer groups that begin with this ID when connecting to Event Streams. This also prevents clients who are reading from the same topics from interfering with each other. To allow the use of transactions when writing to topics, create a policy with \u2018Writer\u2019 service access and the \u2018Resource type\u2019 set to txnid . We highly encourage the use of transactions for exactly-once write semantics. Duplicate messages will cause validation failures or problems for downstream consumers. Note : policies support wildcards at the beginning and/or end of the \u2018Resource ID\u2019 field when using the \u2018string matches\u2019 qualifier. This enables a single policy to allow access to multiple topics when they share a common substring. For example, ingest.24.data-int-1.* could be used to allow access to the ingest.24.data-int-1.in and ingest.24.data-int-1.notification topics. The Data Integrator will need read & write access to the input topic, but only read access to the notification topic. This requires five IAM policies total. Below is an example. A downstream consumer will need just read access to the input and notification topics. This requires three IAM policies total. Below is an example. More detailed documentation on how to configure IAM policies for Event Streams can be found here . HRI Management User Authorization \u00b6 In your authorization service, create a new scope for this tenant and assign it to the Data Integrators and Consumers that need access. See Authorization for more details. If you are using the IBM App ID Service, please see Authorization - \u201cAdding Data Integrators and Consumers\u201d .","title":"Administration"},{"location":"admin/#hri-administration","text":"HRI Administration tasks include: Managing Tenants Onboarding Data Integrators Manually through the Event Streams UI Using the Management API Stream Endpoints HRI Management User Authorization","title":"HRI Administration"},{"location":"admin/#managing-tenants","text":"Every Tenant has a separate index in Elasticsearch . Indexes are named <tenantId>-batches . For example, if the tenant ID is 24 , the new index name will be 24-batches . Some solutions may include a tenant prefix, e.g. tenant24-batches . The tenant ID may contain any lowercase alphanumeric strings, - , and _ . Whatever pattern you use, this will determine the tenant ID path parameter required in most of the Management API endpoints , and will need to be communicated to Data Integrators for that tenant. If you are using an existing deployment, check with team managing the instance on naming conventions. There are four Management API endpoints that support Tenant management in Elasticsearch for HRI: Create, Get (all tenants), Get (specific tenant) and Delete. Please note that all four of these endpoints require IAM authentication - you will need to pass in an IAM Bearer token as part of the authorization header in the requests.","title":"Managing Tenants"},{"location":"admin/#create-tenant","text":"Use the Management API Create Tenant endpoint to create new Tenants. This will create a new index for the Tenant in Elasticsearch. The Create Tenant endpoint takes in one path parameter tenantId , which may only contain lowercase alphanumeric characters, - , and _ . For example, for the tenantId \u201c24\u201d you would use the following curl command: curl -X POST \\ <hri_base_url>/tenants/24 \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json'","title":"Create Tenant"},{"location":"admin/#get-tenants","text":"The Get endpoint takes in no parameters and returns a list of all tenantIds that have an Elastic index. Assuming the above Create was run, then the following cURL command (HTTP/Get operation) would return a list containing the single tenantId \u201c24\u201d: curl -X GET \\ <hri_base_url>/tenants \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json'","title":"Get Tenants"},{"location":"admin/#get-tenant","text":"The GetTenant endpoint can also take in a tenantId and will return a list of information on the associated index. Assuming the above Create was run, then the following cURL command (HTTP/Get operation) would return a list of information on the index for \u201c24\u201d: curl -X GET \\ <hri_base_url>/tenants/24 \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json'","title":"Get Tenant"},{"location":"admin/#delete-tenant","text":"Like Create , the Delete Tenant endpoint takes in tenantId . The following curl command will delete the elastic index for 24 : curl -X DELETE \\ <hri_base_url>/tenants/24 \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json'","title":"Delete Tenant"},{"location":"admin/#onboarding-new-data-integrators","text":"Every unique combination of Tenant and Data Integrator must have a separate \u2018stream\u2019, path of data through the HRI, in order to satisfy HIPAA data isolation requirements. See Multi-tenancy for more details. Every stream includes two topics, and they can be added manually through the Event Streams UI and API or automatically through the Management API.","title":"Onboarding New Data Integrators"},{"location":"admin/#topic-naming-conventions","text":"Please note that HRI uses the following naming conventions for topics: ingest.<tenantId>.<dataIntegratorId>[.metadataTag].in ingest.<tenantId>.<dataIntegratorId>[.metadataTag].notification The metadataTag is an optional field that can be set to any user defined value. For example, with tenant id 24 , Data Integrator id data-int-1 , and metadata tag claims , the topics would be: ingest.24.data-int-1.claims.in ingest.24.data-int-1.claims.notification The tenant ID must be consistent with the Elasticsearch index tenant ID . Only use lowercase alphanumeric characters, - , and _ .","title":"Topic Naming Conventions:"},{"location":"admin/#topic-sizing-recommendations","text":"Topic sizing mainly consists of the number of partitions and the retention policy (time and amount of data), and mainly depends on three factors: the size of batches the frequency of batches the throughput of Data Consumers The number of partitions determines how many Data Consumer processes can read and process the data in parallel, with one process per partition. The retention policy determines how much data the topic will store before removing it due to time or size constraints. The throughput of Data Consumers is mainly dependent on what is done with the data after reading it. At a minimum, the Data Consumers must be faster than the long term rate of incoming data. Otherwise, data may be removed from the topic before it is consumed, resulting in data loss. For example, if a 10 GB batch is written by Data Integrators every 4 hours, the Data Consumers must have a process rate greater than 2.5 GBs per hour. Otherwise, they will continually fall behind and eventually not process some data before it is removed from the topic. For initial sizing, estimate the peak data load size (could include multiple batches depending on your solution) and divide by 1 GB (compressed) to get the number of partitions. For example, if the peak data load is 10 GBs, then start with 10 partitions. Also set the retention size to 10 GBs. In production environments, the retention time is typically 3-7 days. With longer times, you may need to also increase the retention size. As the frequency and size of data loads increase, the number of partitions and retention policy should also increase. As the throughput of Data Consumers increases, the number of partitions and retention policy should decrease. NOTE: if creating topics manually, only 1 partition is needed for the *.notification topic.","title":"Topic Sizing Recommendations"},{"location":"admin/#manually-through-the-event-streams-ui","text":"Create the required Kafka topics described above using the IBM Cloud Event Streams UI.","title":"Manually through the Event Streams UI"},{"location":"admin/#create-the-new-topic","text":"To Create the new topics, in your IBM Cloud account navigate to your Event Streams service. Click on the Topics tab/link on the left of your screen. Next, click on the \u201cCreate Topic\u201d button on the upper right-hand side of the screen: Enter your topic name, as defined by the naming conventions above, the number of partitions, and the retention time. Note that you must toggle Show all available options to see the partition and retention policy options.","title":"Create the new Topic"},{"location":"admin/#using-the-management-api-stream-endpoints","text":"There are three Management API Stream endpoints: Create , Get , and Delete . Please note that all three of these endpoints require IAM authentication - you will need to pass in an IAM Bearer token as part of the authorization header in the requests. In the case of Create and Delete , the IAM bearer token must be associated with a user who has Manager role permissions. For Get , the bearer token must be associated with a user who has at least Reader role permissions. See Event Streams documentation for details on permissions.","title":"Using the Management API stream endpoints"},{"location":"admin/#create-stream","text":"Instead of manually creating the topics for the Tenant and Data Integrator pairing, you may choose to use the Management API Create Stream endpoint which will create the topics for you, and take into account if validation is enabled. The Create Stream endpoint takes in two path parameters, tenantId and streamId , where streamId is made up of the Data Integrator ID and an optional qualifier, delimited by \u2018.\u2019. Both tenantId and streamdId may only contain lowercase alphanumeric characters, - , and _ . streamdId may also contain one \u2018.\u2019. For example, for the tenantId \u201ctenant24\u201d, Data Integrator ID \u201cdata-int-1\u201d and optional qualifier \u201cqualifier1\u201d, you could use the following curl command: curl -X POST \\ <hri_base_url>/tenants/tenant24/streams/data-int-1.qualifier1 \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json' \\ -d '{ \"numPartitions\":1, \"retentionMs\":86400000 }' This will create the following topics: ingest.tenant24.data-int-1.qualifier1.in ingest.tenant24.data-int-1.qualifier1.notification Note: numPartitions and retentionMs topic configurations are required. There are other optional configurations that can also be passed in, see the Stream Api Spec for more details on these optional fields. The numPartitions parameter is applied to the *.in topic, but the *.notification topic is set to 1.","title":"Create Stream"},{"location":"admin/#get-streams","text":"The Get Streams endpoint takes in tenantId as a path parameter, and returns a list of all streamId \u2018s associated with that tenant. Assuming the above Create was run, then the following cURL command (HTTP/Get operation) would return a list containing the single streamId data-int-1.qualifier1 : curl -X GET \\ <hri_base_url>/tenants/tenant24/streams \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json'","title":"Get Streams"},{"location":"admin/#delete-stream","text":"Like Create , the Delete Stream endpoint takes in two path parameters, tenantId and streamId . The following curl command will delete both the ingest.tenant24.data-int-1.qualifier1.in and ingest.tenant24.data-int-1.qualifier1.notification topics: curl -X DELETE \\ <hri_base_url>/tenants/tenant24/streams/data-int-1.qualifier1 \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json' Note that HRI topic naming conventions require topics to start with the prefix \u201cingest\u201d and end with the suffix \u201cin\u201d or \u201cnotification\u201d. Both the Get and Delete endpoints will ignore any topics that don\u2019t follow this convention.","title":"Delete Stream"},{"location":"admin/#creating-service-credentials-for-kafka-permissions","text":"You have to create an Event Streams (Kafka) Service Credential for every client that will need to read from or write to one or more topics. Typically, every Data Integrator and downstream Data Consumer will need their own service credential. A service credential can be configured with IAM policies to just grant read and or write access to specific topics and consumer groups, so only one service credential is needed for each entity. You do not need to create a service credential for every topic. Each service credential will initially have read or write access to all topics when created depending on whether the \u2018Reader\u2019 or \u2018Writer\u2019 role is selected respectively. But they can be configured with IAM policies to just grant read and or write access to specific topics and consumer groups regardless of which role is selected. It\u2019s good practice to select \u2018Writer\u2019 for Data Integrators and \u2018Reader\u2019 for downstream consumers. To create a service credential, navigate to the Event Streams - Service Credentials page, and then click on the \u201cNew Credential\u201d button on the right-hand side of your screen: You can see in the Screenshot below an example of creating a Data Integrator service credential with the Writer role: Next, go to the IBM Cloud (Access) IAM Management tools to further restrict the service credential, by using the \u201cManage\u201d drop-down menu at the top of your screen and choosing \u2018Access (IAM)\u2019. Then select \u2018Service IDs\u2019 from the left menu. Next select the Service ID for the credential you created. If you selected \u2018Auto Generate\u2019 when creating the credential, it will have the same name, but be careful, because there can be multiple Service IDs with the same name. After selecting the Service ID, go to the \u2018Access policies\u2019 tab. You should see one policy that is created by default, which allows read or write access to all topics. To restrict access to particular topics, you have to modify the existing policy and create several new ones. Below are rules about what policies to create for specific access. Create a policy with \u2018Reader\u2019 service access and \u2018Resource type\u2019 set to cluster . This will allow the Service ID to access the Event Streams brokers. To allow read & write permissions to a particular topic, create a policy with \u2018Reader\u2019 and \u2018Writer\u2019 service access, \u2018Resource type\u2019 set to topic , and \u2018Resource ID\u2019 set to the topic name. To allow just read permissions to a particular topic, create a policy with \u2018Reader\u2019 service access, \u2018Resource type\u2019 set to topic , and \u2018Resource ID\u2019 set to the topic name. To allow subscribing to topics, the Service ID must be given permissions to create consumer groups. This is the standard way of consuming messages from Kafka. Create a policy with \u2018Reader\u2019 service access, \u2018Resource type\u2019 set to group , and \u2018Resource ID\u2019 set to a unique ID for this client followed by a * using \u2018string matches\u2019, e.g. data-int-1* . This allows the client to only create consumer groups that begin with this ID when connecting to Event Streams. This also prevents clients who are reading from the same topics from interfering with each other. To allow the use of transactions when writing to topics, create a policy with \u2018Writer\u2019 service access and the \u2018Resource type\u2019 set to txnid . We highly encourage the use of transactions for exactly-once write semantics. Duplicate messages will cause validation failures or problems for downstream consumers. Note : policies support wildcards at the beginning and/or end of the \u2018Resource ID\u2019 field when using the \u2018string matches\u2019 qualifier. This enables a single policy to allow access to multiple topics when they share a common substring. For example, ingest.24.data-int-1.* could be used to allow access to the ingest.24.data-int-1.in and ingest.24.data-int-1.notification topics. The Data Integrator will need read & write access to the input topic, but only read access to the notification topic. This requires five IAM policies total. Below is an example. A downstream consumer will need just read access to the input and notification topics. This requires three IAM policies total. Below is an example. More detailed documentation on how to configure IAM policies for Event Streams can be found here .","title":"Creating Service Credentials for Kafka Permissions"},{"location":"admin/#hri-management-user-authorization","text":"In your authorization service, create a new scope for this tenant and assign it to the Data Integrators and Consumers that need access. See Authorization for more details. If you are using the IBM App ID Service, please see Authorization - \u201cAdding Data Integrators and Consumers\u201d .","title":"HRI Management User Authorization"},{"location":"apispec/","text":"HRI API Specification \u00b6 The HRI consists of two separate APIs: the Management API and Apache Kafka. Management API Specification \u00b6 The Management API is defined using the OpenAPI 3.0 specification: management.yml . You can open the file directly or use a program such as Swagger UI to view it. HRI Tenants & Elasticsearch Indices \u00b6 HRI has been designed with a multi-tenant cloud architecture . The API mainly contains methods for managing Tenants like creating, getting, and deleting. Each of these calls takes in the tenantId. The ID is appended with the suffix -batches to create an index in Elasticsearch, where all the batch metadata is stored. A Get call without a tenantId will return a list of all tenants. The Get call, when given a tenantId, will return information on the elastic index of a specific tenant. Below is a table of the fields returned by this call: Field Description health health of the Elastic cluster status status of the index, can be open or closed index the name of the index, which will be the tenantId with -batches appended to it uuid universally unique identifier pri number of primary shards rep number of replicas docs.count number of batches documents stored in the index docs.deleted number of batches documents deleted from the index store.size store size taken by primary and replica shards pri.store.size store size taken only by primary shards Batches \u00b6 The API contains methods for managing batches like creating, getting, and updating. Below is a table of the fields: Field Description id auto generated unique ID name name of the batch, provided by the Data Integrator integratorId unique ID of the Data Integrator that created this batch topic Event Streams (Kafka) topic that contains the data, provided by the Data Integrator dataType the type of data, provided by the Data Integrator status status of the batch: [ started, completed, terminated ] startDate the date and time the batch was started endDate the date and time the batch was completed or terminated recordCount the number of records in the batch, provided by the Data Integrator when completed metadata custom json value, optional Only the name , topic , and dataType fields are required when creating a batch. The metadata field is optional and allows the Data Integrator to include any additional information about the batch that Data Consumers might request. This information will be included in all notification messages. The recordCount is provided by the Data Integrator when the batch is completed, and thus not always present. All other fields are generated by the API. Streams \u00b6 The API also contains methods for managing Event Streams topics like creating, getting, and deleting. Below is a table of the fields: Field Description id stream ID, consisting of a data integrator and optional qualifier, delimited by \u2018.\u2019 numPartitions the number of partitions on the topic retentionMs length of time in milliseconds before log segments are automatically discarded from a partition retentionBytes optional maximum size in bytes that a partition can grow before discarding log segments cleanupPolicy optional retention policy on old log segments segmentMs optional time in milliseconds after which Kafka will force the log to roll even if the segment file isn\u2019t full segmentBytes optional log segment file size in bytes segmentIndexBytes optional size in bytes of the index that maps offsets to file positions Only the numPartitions and retentionMs fields are required when creating a stream. The rest of the topic configurations ( retentionBytes , cleanupPolicy , segmentMs , segmentBytes , and segmentIndexBytes ) are optional. Below is a table of the default values and acceptable ranges for these optional fields: Field Default value Acceptable values/ranges retentionBytes 1073741824 [10485760..1073741824] cleanupPolicy delete [ delete, compact ] segmentMs nil [300000..2592000000] segmentBytes 536870912 [10485760..536870912] segmentIndexBytes nil [102400..104857600] If the cleanupPolicy field is set to compact, it will disable deletion based on time, ignoring the value set for the field retentionMs . Event Streams is an IBM Cloud managed version of Apache Kafka. It uses standard Kafka libraries to read and write data to topics. See the IBM documentation for details on connection parameters. Apache Kafka \u00b6 Apache Kafka has its own API and clients are available for most languages. If using IBM Event Streams, see their documentation for details on connection parameters. Below are the requirements on the records written to and read from Kafka. Health Input Data - FHIR Model \u00b6 HRI does not impose any requirements on the format of the content of the Health (data) records written to Kafka, although Alvearie has selected FHIR as the preferred data model for all Health Data. See their FHIR implementation guide for more details. Data Integrators and Data Consumers must work together to agree on the specifics of the input data such as format and frequency. HRI-Specific Requirements \u00b6 The HRI does have the following requirements and recommendations: Batch ID Header - every record must have a header entry with the batch ID that uses the key batchId . Data Integrators may include any additional header values, which will get passed downstream to consumers. Zstd Compression - use zstd compression when writing to Kafka by setting the compression.type producer configuration. Event Streams throttles network usage and limits Kafka messages to 1 MB. Using compression will help prevent an Event Streams bottleneck. 1 MB Message Limit - Event Streams limits messages to 1 MB. There is not a way to directly set the max message size after compression is applied in the Kafka producer. The message.max.bytes producer configuration is applied before compression. The batch.size producer configuration can be set to limit the batching of records, but it can also affect performance. We recommend doing performance testing to determine appropriate values based on your data. For records over 1 MB compressed, there are two strategies: External References - for records that have large binary attachments like images or pdfs, you may provide a reference to the large resource that is included in the message, rather than the (large) resource itself. For example, you could put a COS Object URL, or some other external data store URL, and key into the message. Splitting up Records - records can be split into smaller parts, sent through the HRI, and re-assembled by down stream consumers. Notification Messages \u00b6 The notification messages are json-encoded batches. They match the schema returned by the Management API described above, which is also defined here: batchNotification.json .","title":"API Specification"},{"location":"apispec/#hri-api-specification","text":"The HRI consists of two separate APIs: the Management API and Apache Kafka.","title":"HRI API Specification"},{"location":"apispec/#management-api-specification","text":"The Management API is defined using the OpenAPI 3.0 specification: management.yml . You can open the file directly or use a program such as Swagger UI to view it.","title":"Management API Specification"},{"location":"apispec/#hri-tenants-elasticsearch-indices","text":"HRI has been designed with a multi-tenant cloud architecture . The API mainly contains methods for managing Tenants like creating, getting, and deleting. Each of these calls takes in the tenantId. The ID is appended with the suffix -batches to create an index in Elasticsearch, where all the batch metadata is stored. A Get call without a tenantId will return a list of all tenants. The Get call, when given a tenantId, will return information on the elastic index of a specific tenant. Below is a table of the fields returned by this call: Field Description health health of the Elastic cluster status status of the index, can be open or closed index the name of the index, which will be the tenantId with -batches appended to it uuid universally unique identifier pri number of primary shards rep number of replicas docs.count number of batches documents stored in the index docs.deleted number of batches documents deleted from the index store.size store size taken by primary and replica shards pri.store.size store size taken only by primary shards","title":"HRI Tenants &amp; Elasticsearch Indices"},{"location":"apispec/#batches","text":"The API contains methods for managing batches like creating, getting, and updating. Below is a table of the fields: Field Description id auto generated unique ID name name of the batch, provided by the Data Integrator integratorId unique ID of the Data Integrator that created this batch topic Event Streams (Kafka) topic that contains the data, provided by the Data Integrator dataType the type of data, provided by the Data Integrator status status of the batch: [ started, completed, terminated ] startDate the date and time the batch was started endDate the date and time the batch was completed or terminated recordCount the number of records in the batch, provided by the Data Integrator when completed metadata custom json value, optional Only the name , topic , and dataType fields are required when creating a batch. The metadata field is optional and allows the Data Integrator to include any additional information about the batch that Data Consumers might request. This information will be included in all notification messages. The recordCount is provided by the Data Integrator when the batch is completed, and thus not always present. All other fields are generated by the API.","title":"Batches"},{"location":"apispec/#streams","text":"The API also contains methods for managing Event Streams topics like creating, getting, and deleting. Below is a table of the fields: Field Description id stream ID, consisting of a data integrator and optional qualifier, delimited by \u2018.\u2019 numPartitions the number of partitions on the topic retentionMs length of time in milliseconds before log segments are automatically discarded from a partition retentionBytes optional maximum size in bytes that a partition can grow before discarding log segments cleanupPolicy optional retention policy on old log segments segmentMs optional time in milliseconds after which Kafka will force the log to roll even if the segment file isn\u2019t full segmentBytes optional log segment file size in bytes segmentIndexBytes optional size in bytes of the index that maps offsets to file positions Only the numPartitions and retentionMs fields are required when creating a stream. The rest of the topic configurations ( retentionBytes , cleanupPolicy , segmentMs , segmentBytes , and segmentIndexBytes ) are optional. Below is a table of the default values and acceptable ranges for these optional fields: Field Default value Acceptable values/ranges retentionBytes 1073741824 [10485760..1073741824] cleanupPolicy delete [ delete, compact ] segmentMs nil [300000..2592000000] segmentBytes 536870912 [10485760..536870912] segmentIndexBytes nil [102400..104857600] If the cleanupPolicy field is set to compact, it will disable deletion based on time, ignoring the value set for the field retentionMs . Event Streams is an IBM Cloud managed version of Apache Kafka. It uses standard Kafka libraries to read and write data to topics. See the IBM documentation for details on connection parameters.","title":"Streams"},{"location":"apispec/#apache-kafka","text":"Apache Kafka has its own API and clients are available for most languages. If using IBM Event Streams, see their documentation for details on connection parameters. Below are the requirements on the records written to and read from Kafka.","title":"Apache Kafka"},{"location":"apispec/#health-input-data-fhir-model","text":"HRI does not impose any requirements on the format of the content of the Health (data) records written to Kafka, although Alvearie has selected FHIR as the preferred data model for all Health Data. See their FHIR implementation guide for more details. Data Integrators and Data Consumers must work together to agree on the specifics of the input data such as format and frequency.","title":"Health Input Data - FHIR Model"},{"location":"apispec/#hri-specific-requirements","text":"The HRI does have the following requirements and recommendations: Batch ID Header - every record must have a header entry with the batch ID that uses the key batchId . Data Integrators may include any additional header values, which will get passed downstream to consumers. Zstd Compression - use zstd compression when writing to Kafka by setting the compression.type producer configuration. Event Streams throttles network usage and limits Kafka messages to 1 MB. Using compression will help prevent an Event Streams bottleneck. 1 MB Message Limit - Event Streams limits messages to 1 MB. There is not a way to directly set the max message size after compression is applied in the Kafka producer. The message.max.bytes producer configuration is applied before compression. The batch.size producer configuration can be set to limit the batching of records, but it can also affect performance. We recommend doing performance testing to determine appropriate values based on your data. For records over 1 MB compressed, there are two strategies: External References - for records that have large binary attachments like images or pdfs, you may provide a reference to the large resource that is included in the message, rather than the (large) resource itself. For example, you could put a COS Object URL, or some other external data store URL, and key into the message. Splitting up Records - records can be split into smaller parts, sent through the HRI, and re-assembled by down stream consumers.","title":"HRI-Specific Requirements"},{"location":"apispec/#notification-messages","text":"The notification messages are json-encoded batches. They match the schema returned by the Management API described above, which is also defined here: batchNotification.json .","title":"Notification Messages"},{"location":"auth/","text":"Authorization \u00b6 Apache Kafka Authorization \u00b6 Apache Kafka supports several types of authentication , but managed cloud services tend to limit it to one or two methods and implement their own authorization mechanism. For IBM Event Streams, see their connecting and security documentation and our administration page for more details. HRI Management API \u00b6 The Management API uses OAuth 2.0 , OIDC , and JWT standards for authorization. The specific types of authorization tokens used by the Management API endpoints are as follows: Endpoint Path Authorization Requirements tenant /tenants require IAM tokens with permissions to the Elasticsearch instance stream /tenants/<tenantId>/streams require IAM tokens with permissions to the Event Streams instance batch /tenants/<tenantId>/batches are optionally authorized by using JWT tokens from an OIDC-compliant authorization service, or with a custom authentication proxy service. More details on batch endpoint authorization below. healthcheck /healthcheck does not require any authorization Batch Endpoint Authorization \u00b6 Regarding batch endpoint authorization with JWT Tokens, the HRI does not include an authorization or authentication service, so users must provide one themselves and configure it as per the specification below. Oauth 2 and OIDC are widely used standards which allow solutions to use their own authorization & authentication. Your token issuer must be OIDC compliant, because the OIDC defined well-known endpoints are how the Management API will validate access tokens. HRI uses IBM Cloud App ID for its reference implementation of an OIDC-compliant authorization service. There are instructions provided below for how HRI users can configure their own instance of IBM Cloud App ID. Required Token Scopes: \u00b6 You must configure your authorization service to include HRI roles and tenant scopes in the access tokens (Note: scope is a standard claim that is a space-separated list of strings). HRI Role Scopes: \u00b6 hri_data_integrator - Data Integrator role; allows access to create, query, and update batches. Results are filtered to batches that the Data Integrator created. Note that the sub claim in the access token is used to identify the Data Integrator and stored in the batch integratorId field. hri_consumer - Data Consumer role; allows access to query batches. Results are not filtered. Tenant Scopes \u00b6 A Tenant scope matching the tenant ID is required every time you call a \u201cbatch\u201d endpoint in the Management API. The scope must have tenant_ as a prefix to the tenant ID. For example, if a data integrator tries to create a batch by making an HTTP POST call to tenants/24/batches , the token must contain scope tenant_24 , where the 24 is the tenantId. Also, see Multi-tenancy for more information about tenants and roles. See the API spec for more details about required roles for specific endpoints. Configuring the Token Issuer \u00b6 The token issuer is a configuration parameter of the Management API, and only one issuer may be set. The issuer must match the iss claim in the access token. See deployment for more details. Using App ID as Your Authorization Service \u00b6 IBM Cloud App ID is an IBM Cloud managed authorization and authentication service. App ID supports custom claims/scopes for client credentials by creating \u2018Applications\u2019 with scopes and assigning them to other \u2018Applications\u2019 via roles. Below are the manual steps to configure App ID for your reference, but you will probably want to automate some or all of these steps for your solution using the App ID API . (Note all examples here will be of the AppID-Landing App ID service instance.) Initial Configuration \u00b6 Create an \u201cHRI\u201d Application and its associated scopes. This application represents the HRI as a \u2018protected\u2019 resource (API) that other clients will be granted permissions to access. It is not a client that will connect to the HRI, and it\u2019s credentials should never be used. After opening your App ID service instance in your cloud account, note the App ID actions menu on the left-hand side of your screen: Click on the \u201cApplications\u201d menu item to see the Applications Screen: Click on the blue \u201cAdd application\u201d button on the top-right of the screen: Enter the Name ( HRI ), keep the default Type of \u201cRegular web application\u201d and add both of the required scopes using the \u201c+\u201d button ( hri_data_integrator , hri_consumer ; see HRI Scopes above) and click the \u201cSave\u201d button: Now you should be able to view your newly created \u201cHRI\u201d Application, and it\u2019s details (note: that you will now see the two scopes in the \u201cscopes\u201d section): Create Tenant scopes: In the App ID \u201cApplications\u201d screen, find the \u201cHRI\u201d Application created in step 1 above. Expand It\u2019s \u201c\u2026\u201d menu on the far right and select \u201cEdit\u201d. Using the scopes \u201c+\u201d button, create a scope with the naming convention tenant_<tenantId> for every current tenant in your solution, then click the \u201cSave\u201d button. Please note that we added a scope named \u201ctenant_test\u201d, where the test portion of this scope string exactly matches the actual Tenant Name that was created when Adding/Creating the new tenant : Create new roles for each of the scopes you created in steps 1 & 2. You will create roles for the HRI role scopes and for every existing Tenant in your solution. Click on the \u201cProfiles and roles\u201d sub-menu (under \u201cManage Authentication\u201d) and then on the \u201cRoles\u201d sub-menu under that: Click on the blue \u201cCreate Role\u201d button on the top-right of the screen. Enter a role name and description (optional). In the \u201cScopes\u201d field, enter the HRI Application name followed by a \u2018/\u2019 and then the scope name. Then click the \u201c+\u201d button. Each role will have a 1-to-1 relationship with one scope. Last, click the \u201cSave\u201d button. Below is example for the \u201ctest\u201d Tenant role: A example \u201ctest\u201d Tenant role: An example HRI Data Integrator role: An example HRI Consumer role: Adding Data Integrators and Consumers \u00b6 Create a new Application for every HRI client (Data Integrator or Data Consumer) that uses the Management API. For example, here is a screenshot that includes five (5) different user-instance Applications created for the HRI Integration Test (client): An example of what one of these new Applications would look like after created: Please note that the \u201cscopes\u201d property is empty and that this new \u201ccredential\u201d Application has been created for the \u201ctenant_test\u201d and a \u201cdata integrator\u201d. Hence, in Step 2 below, it will have the roles of \u201cTenant Test\u201d and \u201cHRI Data Integrator\u201d assigned to it. Assign roles to each of the Credential Applications you created in step 1 to grant access to the HRI and specific tenants. Currently, this can only be done via making HTTP calls directly to the App ID API, specifically the endpoint /management/v4/{tenantId}/applications/{clientId}/roles (see App ID API Specification section \u201cManagement API - Applications\u201d). For example, if you have just created a new Data Integrator credential Application, assign the HRI Data Integrator role, as well as a role for every tenant that data integrator produces data for, by taking the following actions: Using the command-line, Login to the IBM CLI and obtain an OAuth token: ibmcloud iam oauth-tokens That will return a message like this: IAM token: Bearer eyJraWQi.......{Very long string} Next, export that long string (starting after the \u201cBearer \u201d section) to a bash/shell variable named TOKEN: export TOKEN=eyJraWQi.......{Very long string} Compose the correct HTTP/REST endpoint URL for the Assignment API call: Find the correct App ID HTTP/REST endpoint URL root for managing the instance. In service credentials, it is the value of the managementUrl field (e.g https://us-south.appid.cloud.ibm.com/management/v4/ec9bcf93-4863-46a7-bf66-19009b4e2ed8 ): To complete the URL you will need to add the following: /applications/[client_id_of_application_assigning_to]/roles where \u201cclient_id_of_application_assigning_to\u201d will be replaced with clientId value from the Application, such as the \u201chri_integration_tenant_test_data_integrator\u201d Application. That clientId ends in 5f7ea : The completed full example App ID HTTP PUT url would look like this: https://us-south.appid.cloud.ibm.com/management/v4/ec9bcf93-4863-46a7-bf66-19009b4e2ed8/applications/d55258ed-e765-4f83-939b-71dc7775f7ea/roles Construct your JSON string of roles that you will be associating with this \u201chri_integration_tenant_test_data_integrator\u201d Application. For our example, as the name of our new Credential Application suggests, we will be assigning the \u201cTenant Test\u201d and \u201cHRI Data Integrator\u201d roles to the \u201chri_integration_tenant_test_data_integrator\u201d Application. You will need to use the \u201cid\u201d field value of each Role you are associating with this particular application. In our case, that means we will need the role \u201cid\u201ds ending in d9c16 and 26d12 : Your roles JSON string would then look like the following: {\"roles\":{\"ids\":[\"0d1e0b8f-0c8d-499e-87bb-82e6dded9c16\",\"8dd46d09-eb58-4881-b6bf-fd32f4d26d12\"]}} (Note that this is a list of n-number of roles, to which you may add more role IDs.) Finally, you can assemble your API HTTP request. Using a tool such as cURL at the command-line or Postman , create your REST/HTTP request. Using cURL at the command-line (assumes you have curl installed; see https://curl.haxx.se/download.html ): curl -X PUT https://us-south.appid.cloud.ibm.com/management/v4/ec9bcf93-4863-46a7-bf66-19009b4e2ed8/applications/d55258ed-e765-4f83-939b-71dc7775f7ea/roles \\ -H \"Authorization: Bearer $TOKEN\" \\ -H 'Content-Type: application/json' \\ -d '{\"roles\":{\"ids\":[\"0d1e0b8f-0c8d-499e-87bb-82e6dded9c16\",\"8dd46d09-eb58-4881-b6bf-fd32f4d26d12\"]}}' Note that your roles JSON string should be surrounded by single-quotes (\u2018). Authorization Workflow Example \u00b6 To present an example, Data Integrators and Consumers using App ID would need to request an access token from the App ID service using the OAuth 2.0 \u201cclient credentials\u201d grant flow for reference, see IBM App ID Documentation . The request must include the desired scopes and the HRI Application ID as the audience. This is what an example cURL statement would look like to request the access token: curl -X POST https://us-south.appid.cloud.ibm.com/oauth/v4/ec9bcf93-4863-46a7-bf66-19009b4e2ed8/token \\ -H 'Content-Type: application/x-www-form-urlencoded' \\ -H 'Authorization: Basic <client_id:client_password>' \\ -d 'grant_type=client_credentials&scope=tenant_test hri_data_integrator&audience=b8f85fbe-b00a-4296-b54b-e9ec09a5b2f3' ## Notes: For basic authentication you have to base64 encode the client_id and password like this echo -n '<client_id>:<client_password>' | base64 And, here is an example access token for a Data Integrator Application with access to tenant \u2018test\u2019, produced by App ID: { \"iss\" : \"https://us-south.appid.cloud.ibm.com/oauth/v4/ec9bcf93-4863-46a7-bf66-19009b4e2ed8\" , \"exp\" : 1598459309 , \"aud\" : [ \"b8f85fbe-b00a-4296-b54b-e9ec09a5b2f3\" ], \"sub\" : \"d55258ed-e765-4f83-939b-71dc7775f7ea\" , \"amr\" : [ \"appid_client_credentials\" ], \"iat\" : 1598455709 , \"tenant\" : \"ec9bcf93-4863-46a7-bf66-19009b4e2ed8\" , \"scope\" : \"tenant_test hri_data_integrator\" } Note the included scope(s): tenant_test hri_data_integrator","title":"Authorization"},{"location":"auth/#authorization","text":"","title":"Authorization"},{"location":"auth/#apache-kafka-authorization","text":"Apache Kafka supports several types of authentication , but managed cloud services tend to limit it to one or two methods and implement their own authorization mechanism. For IBM Event Streams, see their connecting and security documentation and our administration page for more details.","title":"Apache Kafka Authorization"},{"location":"auth/#hri-management-api","text":"The Management API uses OAuth 2.0 , OIDC , and JWT standards for authorization. The specific types of authorization tokens used by the Management API endpoints are as follows: Endpoint Path Authorization Requirements tenant /tenants require IAM tokens with permissions to the Elasticsearch instance stream /tenants/<tenantId>/streams require IAM tokens with permissions to the Event Streams instance batch /tenants/<tenantId>/batches are optionally authorized by using JWT tokens from an OIDC-compliant authorization service, or with a custom authentication proxy service. More details on batch endpoint authorization below. healthcheck /healthcheck does not require any authorization","title":"HRI Management API"},{"location":"auth/#batch-endpoint-authorization","text":"Regarding batch endpoint authorization with JWT Tokens, the HRI does not include an authorization or authentication service, so users must provide one themselves and configure it as per the specification below. Oauth 2 and OIDC are widely used standards which allow solutions to use their own authorization & authentication. Your token issuer must be OIDC compliant, because the OIDC defined well-known endpoints are how the Management API will validate access tokens. HRI uses IBM Cloud App ID for its reference implementation of an OIDC-compliant authorization service. There are instructions provided below for how HRI users can configure their own instance of IBM Cloud App ID.","title":"Batch Endpoint Authorization"},{"location":"auth/#required-token-scopes","text":"You must configure your authorization service to include HRI roles and tenant scopes in the access tokens (Note: scope is a standard claim that is a space-separated list of strings).","title":"Required Token Scopes:"},{"location":"auth/#hri-role-scopes","text":"hri_data_integrator - Data Integrator role; allows access to create, query, and update batches. Results are filtered to batches that the Data Integrator created. Note that the sub claim in the access token is used to identify the Data Integrator and stored in the batch integratorId field. hri_consumer - Data Consumer role; allows access to query batches. Results are not filtered.","title":"HRI Role Scopes:"},{"location":"auth/#tenant-scopes","text":"A Tenant scope matching the tenant ID is required every time you call a \u201cbatch\u201d endpoint in the Management API. The scope must have tenant_ as a prefix to the tenant ID. For example, if a data integrator tries to create a batch by making an HTTP POST call to tenants/24/batches , the token must contain scope tenant_24 , where the 24 is the tenantId. Also, see Multi-tenancy for more information about tenants and roles. See the API spec for more details about required roles for specific endpoints.","title":"Tenant Scopes"},{"location":"auth/#configuring-the-token-issuer","text":"The token issuer is a configuration parameter of the Management API, and only one issuer may be set. The issuer must match the iss claim in the access token. See deployment for more details.","title":"Configuring the Token Issuer"},{"location":"auth/#using-app-id-as-your-authorization-service","text":"IBM Cloud App ID is an IBM Cloud managed authorization and authentication service. App ID supports custom claims/scopes for client credentials by creating \u2018Applications\u2019 with scopes and assigning them to other \u2018Applications\u2019 via roles. Below are the manual steps to configure App ID for your reference, but you will probably want to automate some or all of these steps for your solution using the App ID API . (Note all examples here will be of the AppID-Landing App ID service instance.)","title":"Using App ID as Your Authorization Service"},{"location":"auth/#initial-configuration","text":"Create an \u201cHRI\u201d Application and its associated scopes. This application represents the HRI as a \u2018protected\u2019 resource (API) that other clients will be granted permissions to access. It is not a client that will connect to the HRI, and it\u2019s credentials should never be used. After opening your App ID service instance in your cloud account, note the App ID actions menu on the left-hand side of your screen: Click on the \u201cApplications\u201d menu item to see the Applications Screen: Click on the blue \u201cAdd application\u201d button on the top-right of the screen: Enter the Name ( HRI ), keep the default Type of \u201cRegular web application\u201d and add both of the required scopes using the \u201c+\u201d button ( hri_data_integrator , hri_consumer ; see HRI Scopes above) and click the \u201cSave\u201d button: Now you should be able to view your newly created \u201cHRI\u201d Application, and it\u2019s details (note: that you will now see the two scopes in the \u201cscopes\u201d section): Create Tenant scopes: In the App ID \u201cApplications\u201d screen, find the \u201cHRI\u201d Application created in step 1 above. Expand It\u2019s \u201c\u2026\u201d menu on the far right and select \u201cEdit\u201d. Using the scopes \u201c+\u201d button, create a scope with the naming convention tenant_<tenantId> for every current tenant in your solution, then click the \u201cSave\u201d button. Please note that we added a scope named \u201ctenant_test\u201d, where the test portion of this scope string exactly matches the actual Tenant Name that was created when Adding/Creating the new tenant : Create new roles for each of the scopes you created in steps 1 & 2. You will create roles for the HRI role scopes and for every existing Tenant in your solution. Click on the \u201cProfiles and roles\u201d sub-menu (under \u201cManage Authentication\u201d) and then on the \u201cRoles\u201d sub-menu under that: Click on the blue \u201cCreate Role\u201d button on the top-right of the screen. Enter a role name and description (optional). In the \u201cScopes\u201d field, enter the HRI Application name followed by a \u2018/\u2019 and then the scope name. Then click the \u201c+\u201d button. Each role will have a 1-to-1 relationship with one scope. Last, click the \u201cSave\u201d button. Below is example for the \u201ctest\u201d Tenant role: A example \u201ctest\u201d Tenant role: An example HRI Data Integrator role: An example HRI Consumer role:","title":"Initial Configuration"},{"location":"auth/#adding-data-integrators-and-consumers","text":"Create a new Application for every HRI client (Data Integrator or Data Consumer) that uses the Management API. For example, here is a screenshot that includes five (5) different user-instance Applications created for the HRI Integration Test (client): An example of what one of these new Applications would look like after created: Please note that the \u201cscopes\u201d property is empty and that this new \u201ccredential\u201d Application has been created for the \u201ctenant_test\u201d and a \u201cdata integrator\u201d. Hence, in Step 2 below, it will have the roles of \u201cTenant Test\u201d and \u201cHRI Data Integrator\u201d assigned to it. Assign roles to each of the Credential Applications you created in step 1 to grant access to the HRI and specific tenants. Currently, this can only be done via making HTTP calls directly to the App ID API, specifically the endpoint /management/v4/{tenantId}/applications/{clientId}/roles (see App ID API Specification section \u201cManagement API - Applications\u201d). For example, if you have just created a new Data Integrator credential Application, assign the HRI Data Integrator role, as well as a role for every tenant that data integrator produces data for, by taking the following actions: Using the command-line, Login to the IBM CLI and obtain an OAuth token: ibmcloud iam oauth-tokens That will return a message like this: IAM token: Bearer eyJraWQi.......{Very long string} Next, export that long string (starting after the \u201cBearer \u201d section) to a bash/shell variable named TOKEN: export TOKEN=eyJraWQi.......{Very long string} Compose the correct HTTP/REST endpoint URL for the Assignment API call: Find the correct App ID HTTP/REST endpoint URL root for managing the instance. In service credentials, it is the value of the managementUrl field (e.g https://us-south.appid.cloud.ibm.com/management/v4/ec9bcf93-4863-46a7-bf66-19009b4e2ed8 ): To complete the URL you will need to add the following: /applications/[client_id_of_application_assigning_to]/roles where \u201cclient_id_of_application_assigning_to\u201d will be replaced with clientId value from the Application, such as the \u201chri_integration_tenant_test_data_integrator\u201d Application. That clientId ends in 5f7ea : The completed full example App ID HTTP PUT url would look like this: https://us-south.appid.cloud.ibm.com/management/v4/ec9bcf93-4863-46a7-bf66-19009b4e2ed8/applications/d55258ed-e765-4f83-939b-71dc7775f7ea/roles Construct your JSON string of roles that you will be associating with this \u201chri_integration_tenant_test_data_integrator\u201d Application. For our example, as the name of our new Credential Application suggests, we will be assigning the \u201cTenant Test\u201d and \u201cHRI Data Integrator\u201d roles to the \u201chri_integration_tenant_test_data_integrator\u201d Application. You will need to use the \u201cid\u201d field value of each Role you are associating with this particular application. In our case, that means we will need the role \u201cid\u201ds ending in d9c16 and 26d12 : Your roles JSON string would then look like the following: {\"roles\":{\"ids\":[\"0d1e0b8f-0c8d-499e-87bb-82e6dded9c16\",\"8dd46d09-eb58-4881-b6bf-fd32f4d26d12\"]}} (Note that this is a list of n-number of roles, to which you may add more role IDs.) Finally, you can assemble your API HTTP request. Using a tool such as cURL at the command-line or Postman , create your REST/HTTP request. Using cURL at the command-line (assumes you have curl installed; see https://curl.haxx.se/download.html ): curl -X PUT https://us-south.appid.cloud.ibm.com/management/v4/ec9bcf93-4863-46a7-bf66-19009b4e2ed8/applications/d55258ed-e765-4f83-939b-71dc7775f7ea/roles \\ -H \"Authorization: Bearer $TOKEN\" \\ -H 'Content-Type: application/json' \\ -d '{\"roles\":{\"ids\":[\"0d1e0b8f-0c8d-499e-87bb-82e6dded9c16\",\"8dd46d09-eb58-4881-b6bf-fd32f4d26d12\"]}}' Note that your roles JSON string should be surrounded by single-quotes (\u2018).","title":"Adding Data Integrators and Consumers"},{"location":"auth/#authorization-workflow-example","text":"To present an example, Data Integrators and Consumers using App ID would need to request an access token from the App ID service using the OAuth 2.0 \u201cclient credentials\u201d grant flow for reference, see IBM App ID Documentation . The request must include the desired scopes and the HRI Application ID as the audience. This is what an example cURL statement would look like to request the access token: curl -X POST https://us-south.appid.cloud.ibm.com/oauth/v4/ec9bcf93-4863-46a7-bf66-19009b4e2ed8/token \\ -H 'Content-Type: application/x-www-form-urlencoded' \\ -H 'Authorization: Basic <client_id:client_password>' \\ -d 'grant_type=client_credentials&scope=tenant_test hri_data_integrator&audience=b8f85fbe-b00a-4296-b54b-e9ec09a5b2f3' ## Notes: For basic authentication you have to base64 encode the client_id and password like this echo -n '<client_id>:<client_password>' | base64 And, here is an example access token for a Data Integrator Application with access to tenant \u2018test\u2019, produced by App ID: { \"iss\" : \"https://us-south.appid.cloud.ibm.com/oauth/v4/ec9bcf93-4863-46a7-bf66-19009b4e2ed8\" , \"exp\" : 1598459309 , \"aud\" : [ \"b8f85fbe-b00a-4296-b54b-e9ec09a5b2f3\" ], \"sub\" : \"d55258ed-e765-4f83-939b-71dc7775f7ea\" , \"amr\" : [ \"appid_client_credentials\" ], \"iat\" : 1598455709 , \"tenant\" : \"ec9bcf93-4863-46a7-bf66-19009b4e2ed8\" , \"scope\" : \"tenant_test hri_data_integrator\" } Note the included scope(s): tenant_test hri_data_integrator","title":"Authorization Workflow Example"},{"location":"deployment/","text":"HRI Dependencies Configuration/Setup \u00b6 This section is intended to help guide you to configure the dependent services that HRI uses in your own (public/private) IBM Cloud account. Create Elasticsearch cloud resource \u00b6 HRI Requires an Elasticsearch service deployment in your IBM Cloud account. Navigate to the Resource List in your Cloud account. Click the Create resource button in the top right corner. Enter \u201cElasticsearch\u201d in the catalog search bar and then select the Databases for Elasticsearch tile. Select the appropriate region and then configure the resource by providing a service name and resource group (Note: for all configuration examples below, the Resource Group is \u201cYOUR_Resource_GRP\u201d). You will also need to specify the desired resource allocations for Elasticsearch. Depending on your expected usage, your values may differ, but the values shown below will be sufficient in most cases. Then click the Create button. Once the Elasticsearch instance becomes active, you will need to set an \u201cadmin\u201d password. This is done from the Settings page of the Elasticsearch instance. Click the Service credentials link, and then click the New credential button. Provide a name for the service credential and then add it. This will be needed by the HRI Management API deployment. Create Event Streams cloud resource \u00b6 HRI also Requires an Event Streams(Kafka) service deployment in your IBM Cloud account. Navigate to the Resource List in your Cloud account. If an instance of Event Streams already exists in your Cloud account, then the HRI may be able to share that existing instance. If an Event Streams instance does not already exist, then create one by clicking the Create resource button in the top right corner. Enter \u201cEvent Streams\u201d in the catalog search bar and then select the Event Streams tile. Fill in an appropriate region , service name , and resource group . The Enterprise pricing plan (with custom key management via Key Protect) is required for HIPAA data processing. After creating an Enterprise instance of Event Streams, custom key management via Key Protect will need to be explicitly enabled (See Event Streams documentation ). NOTE: The Event Streams Enterprise plan is expensive, which is why we recommend sharing an instance, if possible. In non-Production environments, a Standard plan may be used for testing with non-HIPAA data if your organization\u2019s security team approves. Click the Service credentials link, and then click the New credential button to create a service credential with writer permissions. Provide a name for the service credential. This will be needed by the HRI Management API deployment. Create Authorization Service \u00b6 The HRI Management API requires an authorization service. Integration testing has been performed with IBM Cloud App ID , but any compliant service can be used. See Authorization for more details about the requirements and how to set up an App ID cloud service. Deploy the HRI Management API to IBM Functions \u00b6 The Management API is designed to run on IBM Functions and can be deployed using the IBM Cloud CLI Functions plug-in. The deploy.sh script automates the process by creating an IBM Functions namespace, deploying the code and API, setting configuration values, and binding Elasticsearch and Event Streams service credentials. There are also scripts for configuring Elasticsearch, elastic.sh , and performing initial configurations of App ID, appid.sh . These scripts are packaged into a docker container with the compiled code to support automated deployments and are available on GitHub . Below is a table of the environment variables used by the scripts. Name Description IBM_CLOUD_API_KEY The API key for IBM Cloud IBM_CLOUD_REGION Target IBM Cloud Region, e.g. \u2018ibm:yp:us-south\u2019 RESOURCE_GROUP Target IBM Cloud Resource Group NAMESPACE Target IBM Functions namespace ELASTIC_INSTANCE Name of Elasticsearch instance ELASTIC_SVC_ACCOUNT Name of Elasticsearch service ID KAFKA_INSTANCE Name of Event Streams (Kafka) instance KAFKA_SVC_ACCOUNT Name of Event Streams (Kafka) service ID OIDC_ISSUER The base URL of the OIDC issuer to use for OAuth authentication (e.g. https://us-south.appid.cloud.ibm.com/oauth/v4/<tenantId> ) APPID_PREFIX (Optional) Prefix string to append to the AppId applications and roles created during deployment SET_UP_APPID (Optional) defaults to true. Set to false if you do not want the App ID set-up enabled. What\u2019s Next \u00b6 To set up your first Tenant and Data Integrator go to the Administration page. For detailed info on how the concept of Tenants and the Data Integrator role underpin the HRI Multitenancy approach, see the Multitenancy page.","title":"Deployment"},{"location":"deployment/#hri-dependencies-configurationsetup","text":"This section is intended to help guide you to configure the dependent services that HRI uses in your own (public/private) IBM Cloud account.","title":"HRI Dependencies Configuration/Setup"},{"location":"deployment/#create-elasticsearch-cloud-resource","text":"HRI Requires an Elasticsearch service deployment in your IBM Cloud account. Navigate to the Resource List in your Cloud account. Click the Create resource button in the top right corner. Enter \u201cElasticsearch\u201d in the catalog search bar and then select the Databases for Elasticsearch tile. Select the appropriate region and then configure the resource by providing a service name and resource group (Note: for all configuration examples below, the Resource Group is \u201cYOUR_Resource_GRP\u201d). You will also need to specify the desired resource allocations for Elasticsearch. Depending on your expected usage, your values may differ, but the values shown below will be sufficient in most cases. Then click the Create button. Once the Elasticsearch instance becomes active, you will need to set an \u201cadmin\u201d password. This is done from the Settings page of the Elasticsearch instance. Click the Service credentials link, and then click the New credential button. Provide a name for the service credential and then add it. This will be needed by the HRI Management API deployment.","title":"Create Elasticsearch cloud resource"},{"location":"deployment/#create-event-streams-cloud-resource","text":"HRI also Requires an Event Streams(Kafka) service deployment in your IBM Cloud account. Navigate to the Resource List in your Cloud account. If an instance of Event Streams already exists in your Cloud account, then the HRI may be able to share that existing instance. If an Event Streams instance does not already exist, then create one by clicking the Create resource button in the top right corner. Enter \u201cEvent Streams\u201d in the catalog search bar and then select the Event Streams tile. Fill in an appropriate region , service name , and resource group . The Enterprise pricing plan (with custom key management via Key Protect) is required for HIPAA data processing. After creating an Enterprise instance of Event Streams, custom key management via Key Protect will need to be explicitly enabled (See Event Streams documentation ). NOTE: The Event Streams Enterprise plan is expensive, which is why we recommend sharing an instance, if possible. In non-Production environments, a Standard plan may be used for testing with non-HIPAA data if your organization\u2019s security team approves. Click the Service credentials link, and then click the New credential button to create a service credential with writer permissions. Provide a name for the service credential. This will be needed by the HRI Management API deployment.","title":"Create Event Streams cloud resource"},{"location":"deployment/#create-authorization-service","text":"The HRI Management API requires an authorization service. Integration testing has been performed with IBM Cloud App ID , but any compliant service can be used. See Authorization for more details about the requirements and how to set up an App ID cloud service.","title":"Create Authorization Service"},{"location":"deployment/#deploy-the-hri-management-api-to-ibm-functions","text":"The Management API is designed to run on IBM Functions and can be deployed using the IBM Cloud CLI Functions plug-in. The deploy.sh script automates the process by creating an IBM Functions namespace, deploying the code and API, setting configuration values, and binding Elasticsearch and Event Streams service credentials. There are also scripts for configuring Elasticsearch, elastic.sh , and performing initial configurations of App ID, appid.sh . These scripts are packaged into a docker container with the compiled code to support automated deployments and are available on GitHub . Below is a table of the environment variables used by the scripts. Name Description IBM_CLOUD_API_KEY The API key for IBM Cloud IBM_CLOUD_REGION Target IBM Cloud Region, e.g. \u2018ibm:yp:us-south\u2019 RESOURCE_GROUP Target IBM Cloud Resource Group NAMESPACE Target IBM Functions namespace ELASTIC_INSTANCE Name of Elasticsearch instance ELASTIC_SVC_ACCOUNT Name of Elasticsearch service ID KAFKA_INSTANCE Name of Event Streams (Kafka) instance KAFKA_SVC_ACCOUNT Name of Event Streams (Kafka) service ID OIDC_ISSUER The base URL of the OIDC issuer to use for OAuth authentication (e.g. https://us-south.appid.cloud.ibm.com/oauth/v4/<tenantId> ) APPID_PREFIX (Optional) Prefix string to append to the AppId applications and roles created during deployment SET_UP_APPID (Optional) defaults to true. Set to false if you do not want the App ID set-up enabled.","title":"Deploy the HRI Management API to IBM Functions"},{"location":"deployment/#whats-next","text":"To set up your first Tenant and Data Integrator go to the Administration page. For detailed info on how the concept of Tenants and the Data Integrator role underpin the HRI Multitenancy approach, see the Multitenancy page.","title":"What's Next"},{"location":"glossary/","text":"HRI Glossary of Terms \u00b6 A listing of key terms and phrases to help one\u2019s understanding of the Health Record Ingestion service. Batch: \u00b6 A Batch in an HRI context represents a collection of Health Data records that must be processed together in order for that dataset to be ingested correctly into a cloud-based solution. Only processing some of the data would result in a bad state for the data consumer. Likewise, if there is an error with processing part of the data, the entire batch may need to be rejected. Batch ID: \u00b6 A unique identifier in HRI, referring to one specific batch. Data Consumer: \u00b6 A \u201cdownstream\u201d service, process, or application receiving and further processing the data passing through the HRI. One such example might be an HRI Pipeline Adapter (based on NiFi) that persists data to COS (Cloud Object Storage). Data Integrator: \u00b6 An \u201cupstream\u201d service, process or application that \u201csends\u201d data into the HRI for processing. Elasticsearch: \u00b6 A distributed, open-source document store used to store various types of data. HRI uses Elasticsearch as its primary data store for metadata about batches. Event Streams: \u00b6 The IBM Cloud Event Streams service is a \u201cManaged\u201d service instance of Apache Kafka customized to work with the IBM Cloud. HRI: \u00b6 The Health Record Ingestion service provides a \u201cfront door\u201d for Data Integrators to send data into the cloud, thereby allowing that data to be used by authorized applications that are part of that cloud account. HRI Management API: \u00b6 A RESTful service layer with an OpenAPI 3.0 compliant REST API. The \u201cManagement API\u201d is the external API layer for accessing all public HRI operations that handle Batch, Streams, and Tenant management. Multitenancy: \u00b6 In the context of HRI, multitenancy is a software architecture pattern that allows multiple users or customers to use a single instance of a specific service, sharing computing resources across those customers. For reference, see this link . PHI: \u00b6 Protected Health Information is a term defined by the HIPAA Law and Privacy Rule which provides that \u201ccovered entities\u201d must protect certain sensitive personal information of patients and that patients have certain rights to that information. See this page for HIPAA definition and this page for more in-depth info on HIPAA Privacy Rule and Protected Health Information. Stream: \u00b6 An HRI Stream represents the entire flow through the HRI for a given tenant and Data Integrator. A Stream always has two kafka topics associated with it: an .in topic and a .notification topic. Tenant: \u00b6 A tenant represents one customer or organization that is a \u201cuser\u201d of HRI, on whose behalf a set of Health data is being processed. A tenant can be internal or external to the organization deploying HRI (e.g. IBM Watson Health). All data is isolated between tenants. Terraform: \u00b6 Terraform is an infrastructure as code tool that can be used to set up a cloud environment for an HRI deployment.","title":"Glossary of Terms"},{"location":"glossary/#hri-glossary-of-terms","text":"A listing of key terms and phrases to help one\u2019s understanding of the Health Record Ingestion service.","title":"HRI Glossary of Terms"},{"location":"glossary/#batch","text":"A Batch in an HRI context represents a collection of Health Data records that must be processed together in order for that dataset to be ingested correctly into a cloud-based solution. Only processing some of the data would result in a bad state for the data consumer. Likewise, if there is an error with processing part of the data, the entire batch may need to be rejected.","title":"Batch:"},{"location":"glossary/#batch-id","text":"A unique identifier in HRI, referring to one specific batch.","title":"Batch ID:"},{"location":"glossary/#data-consumer","text":"A \u201cdownstream\u201d service, process, or application receiving and further processing the data passing through the HRI. One such example might be an HRI Pipeline Adapter (based on NiFi) that persists data to COS (Cloud Object Storage).","title":"Data Consumer:"},{"location":"glossary/#data-integrator","text":"An \u201cupstream\u201d service, process or application that \u201csends\u201d data into the HRI for processing.","title":"Data Integrator:"},{"location":"glossary/#elasticsearch","text":"A distributed, open-source document store used to store various types of data. HRI uses Elasticsearch as its primary data store for metadata about batches.","title":"Elasticsearch:"},{"location":"glossary/#event-streams","text":"The IBM Cloud Event Streams service is a \u201cManaged\u201d service instance of Apache Kafka customized to work with the IBM Cloud.","title":"Event Streams:"},{"location":"glossary/#hri","text":"The Health Record Ingestion service provides a \u201cfront door\u201d for Data Integrators to send data into the cloud, thereby allowing that data to be used by authorized applications that are part of that cloud account.","title":"HRI:"},{"location":"glossary/#hri-management-api","text":"A RESTful service layer with an OpenAPI 3.0 compliant REST API. The \u201cManagement API\u201d is the external API layer for accessing all public HRI operations that handle Batch, Streams, and Tenant management.","title":"HRI Management API:"},{"location":"glossary/#multitenancy","text":"In the context of HRI, multitenancy is a software architecture pattern that allows multiple users or customers to use a single instance of a specific service, sharing computing resources across those customers. For reference, see this link .","title":"Multitenancy:"},{"location":"glossary/#phi","text":"Protected Health Information is a term defined by the HIPAA Law and Privacy Rule which provides that \u201ccovered entities\u201d must protect certain sensitive personal information of patients and that patients have certain rights to that information. See this page for HIPAA definition and this page for more in-depth info on HIPAA Privacy Rule and Protected Health Information.","title":"PHI:"},{"location":"glossary/#stream","text":"An HRI Stream represents the entire flow through the HRI for a given tenant and Data Integrator. A Stream always has two kafka topics associated with it: an .in topic and a .notification topic.","title":"Stream:"},{"location":"glossary/#tenant","text":"A tenant represents one customer or organization that is a \u201cuser\u201d of HRI, on whose behalf a set of Health data is being processed. A tenant can be internal or external to the organization deploying HRI (e.g. IBM Watson Health). All data is isolated between tenants.","title":"Tenant:"},{"location":"glossary/#terraform","text":"Terraform is an infrastructure as code tool that can be used to set up a cloud environment for an HRI deployment.","title":"Terraform:"},{"location":"monitorlog/","text":"HRI Monitoring & Logging \u00b6 HRI Management API \u00b6 Logging \u00b6 IBM Functions Tooling \u00b6 IBM Cloud Functions provides some built-in monitoring tools. On the IBM Cloud Functions main page, select the correct/appropriate Namespace for your particular Management API deployment. Then select the \u201cActivations Dashboard\u201d link from the left-hand side menu. The Activations Dashboard will show recent calls (limited to 7 days) to the Management API, their response times, and failure indicators. You can click on the activationId link for any given function invocation to view additional details. In the example below, for the create_batch function call, the activationId is the alpha-numeric identifier string starting with \u201c21fc4f038ddb\u201d: Clicking the activationId link will open up a new tab showing the details of that particular function call, including the path and any possible error event or description. LogDNA \u00b6 Logs from IBM Functions are also written to the LogDNA instance designated as \u201cPlatform Logs\u201d for your Cloud account. You view the list of LogDNA instances here: https://cloud.ibm.com/observe/logging . Open the dashboard to view the logs, which will also contain messages from many other IBM Cloud services. Choose the \u201cAll Sources\u201d menu and select the \u201cfunctions\u201d checkbox to filter only on IBM Functions log messages as seen in this Screen Cap: The search bar at the bottom can also be used to filter for specific messages. For example, since \u201c batches/ \u201d is on the path of the REST API requests for many of the Management API operations, searching for it will only show those related logs. Additionally, most error responses returned from the Management API and most logging messages within the Management API include an errorEventId . The errorEventId in an error response for a specific API invocation will match the errorEventId on logging messages from the same API invocation. Searching by the errorEventId is a recommended tactic for troubleshooting because it can illustrate what took place during the event call which produced the error. Monitoring \u00b6 The Management API has an /hri/healthcheck endpoint can that be used to monitor its health. It ensures the service is running and also checks the health of Elasticsearch and Event Streams. It requires no authentication and responds with a 200 code and an empty body on success. If there are any issues a 500 code and body describing the issue is returned. Event Streams \u00b6 Monitoring Topics and Consumers \u00b6 Event Streams provides minimal monitoring with SysDig. Captured metrics include the number of topics, the rate of bytes written and read from each topic, stable consumers, and inactive consumers. See Monitoring Event Streams metrics using IBM Cloud Monitoring with Sysdig for more details. User Data Access Logging \u00b6 HIPAA regulations require logging all access to PHI data . In the HRI, HIPAA data is only persisted in Event Streams, which will automatically log topic creation and deletion to Activity Tracker, see Activity Tracker events for more information. To view the access logs, go to the Activity Tracker instance for your account. It has a LogDNA interface where you can filter logs by source and/or application. Below is a screenshot of a topic creation log entry. When using the HRI to process PHI , additional audit events must be enabled, which requires the Enterprise plan. Audit events for read, write, and delete actions must be enabled on the *.in and *.out Kafka topics, which will result in events being created when that action is taken on the topic. This allows an offering team to audit Event Streams events relevant to potential PHI access. Information about how to enable message audit events can be found here .","title":"Monitoring & Logging"},{"location":"monitorlog/#hri-monitoring-logging","text":"","title":"HRI Monitoring &amp; Logging"},{"location":"monitorlog/#hri-management-api","text":"","title":"HRI Management API"},{"location":"monitorlog/#logging","text":"","title":"Logging"},{"location":"monitorlog/#ibm-functions-tooling","text":"IBM Cloud Functions provides some built-in monitoring tools. On the IBM Cloud Functions main page, select the correct/appropriate Namespace for your particular Management API deployment. Then select the \u201cActivations Dashboard\u201d link from the left-hand side menu. The Activations Dashboard will show recent calls (limited to 7 days) to the Management API, their response times, and failure indicators. You can click on the activationId link for any given function invocation to view additional details. In the example below, for the create_batch function call, the activationId is the alpha-numeric identifier string starting with \u201c21fc4f038ddb\u201d: Clicking the activationId link will open up a new tab showing the details of that particular function call, including the path and any possible error event or description.","title":"IBM Functions Tooling"},{"location":"monitorlog/#logdna","text":"Logs from IBM Functions are also written to the LogDNA instance designated as \u201cPlatform Logs\u201d for your Cloud account. You view the list of LogDNA instances here: https://cloud.ibm.com/observe/logging . Open the dashboard to view the logs, which will also contain messages from many other IBM Cloud services. Choose the \u201cAll Sources\u201d menu and select the \u201cfunctions\u201d checkbox to filter only on IBM Functions log messages as seen in this Screen Cap: The search bar at the bottom can also be used to filter for specific messages. For example, since \u201c batches/ \u201d is on the path of the REST API requests for many of the Management API operations, searching for it will only show those related logs. Additionally, most error responses returned from the Management API and most logging messages within the Management API include an errorEventId . The errorEventId in an error response for a specific API invocation will match the errorEventId on logging messages from the same API invocation. Searching by the errorEventId is a recommended tactic for troubleshooting because it can illustrate what took place during the event call which produced the error.","title":"LogDNA"},{"location":"monitorlog/#monitoring","text":"The Management API has an /hri/healthcheck endpoint can that be used to monitor its health. It ensures the service is running and also checks the health of Elasticsearch and Event Streams. It requires no authentication and responds with a 200 code and an empty body on success. If there are any issues a 500 code and body describing the issue is returned.","title":"Monitoring"},{"location":"monitorlog/#event-streams","text":"","title":"Event Streams"},{"location":"monitorlog/#monitoring-topics-and-consumers","text":"Event Streams provides minimal monitoring with SysDig. Captured metrics include the number of topics, the rate of bytes written and read from each topic, stable consumers, and inactive consumers. See Monitoring Event Streams metrics using IBM Cloud Monitoring with Sysdig for more details.","title":"Monitoring Topics and Consumers"},{"location":"monitorlog/#user-data-access-logging","text":"HIPAA regulations require logging all access to PHI data . In the HRI, HIPAA data is only persisted in Event Streams, which will automatically log topic creation and deletion to Activity Tracker, see Activity Tracker events for more information. To view the access logs, go to the Activity Tracker instance for your account. It has a LogDNA interface where you can filter logs by source and/or application. Below is a screenshot of a topic creation log entry. When using the HRI to process PHI , additional audit events must be enabled, which requires the Enterprise plan. Audit events for read, write, and delete actions must be enabled on the *.in and *.out Kafka topics, which will result in events being created when that action is taken on the topic. This allows an offering team to audit Event Streams events relevant to potential PHI access. Information about how to enable message audit events can be found here .","title":"User Data Access Logging"},{"location":"multitenancy/","text":"Multi-Tenancy \u00b6 Tenants \u00b6 The HRI supports the concept of multiple tenants. See the respective glossary page entries for more details on Tenants and Multi-Tenancy . Data Integrators \u00b6 Additionally, data is isolated between Data Integrators. A Data Integrator is the organization supplying the Health data on behalf of one or more tenants. Data Consumers \u00b6 Data Consumers are downstream processes (created by some \u201ccustomer\u201d org) that read data from the HRI. The HRI is designed so that a single data consumer would read data for a single tenant, but it does not prevent a consumer from reading data for multiple tenants. Data Consumers can see data provided by all Data Integrators . This diagram shows the flow of two different tenant\u2019s data through the HRI via coloring. Red indicates Tenant 1\u2019s data and blue indicates Tenant 2\u2019s data. Data Integrator B is both red and blue, because it\u2019s processing data for both tenants. Please note that authorization is used to control access to tenant\u2019s data both in Event Streams and in the Management API. See the sections below for more details. Streams \u00b6 A Stream is a path of data through the HRI and consists of Kafka topics. There must be at least one stream for every unique combination of tenant and Data Integrator to satisfy HIPAA requirements. This logically separates the data and enables access to be restricted by tenant and Data Integrator. Note that different Data Integrators for the same tenant are not allowed to access each other\u2019s data. To facilitate this, topics are named using the tenant and Data Integrator\u2019s name, e.g. ingest.tenant.data-integrator.* . In the example above, Integrator B is processing data from two tenants and writes data to two topics, separating them by tenant. Credentials provided to Data Integrators must be locked down to specific topics. Data Types \u00b6 HRI is agnostic to the type of data being written to Kafka. In practice, a Data Integrator often provides a specific type of data (claims, clinical, imagery, etc.) to the HRI. Users/Consumers of HRI also may want separate provided data by type. This can be done by creating additional topics and including another (data type) identifier at the end of the topic name before .in . For example, ingest.t1.di1.claims.in . Note that Inbound topics must end with .in . HRI Management API \u00b6 The Management API also stores metadata about batches in separate indexes (in its Elasticsearch data store). All API endpoints include a tenant ID to support data segregation by tenant. Batch operations are segregated by Data Integrator such that one Integrator cannot access or modify the batches that another Integrator owns. See the Authorization page for more details.","title":"Multi-Tenancy"},{"location":"multitenancy/#multi-tenancy","text":"","title":"Multi-Tenancy"},{"location":"multitenancy/#tenants","text":"The HRI supports the concept of multiple tenants. See the respective glossary page entries for more details on Tenants and Multi-Tenancy .","title":"Tenants"},{"location":"multitenancy/#data-integrators","text":"Additionally, data is isolated between Data Integrators. A Data Integrator is the organization supplying the Health data on behalf of one or more tenants.","title":"Data Integrators"},{"location":"multitenancy/#data-consumers","text":"Data Consumers are downstream processes (created by some \u201ccustomer\u201d org) that read data from the HRI. The HRI is designed so that a single data consumer would read data for a single tenant, but it does not prevent a consumer from reading data for multiple tenants. Data Consumers can see data provided by all Data Integrators . This diagram shows the flow of two different tenant\u2019s data through the HRI via coloring. Red indicates Tenant 1\u2019s data and blue indicates Tenant 2\u2019s data. Data Integrator B is both red and blue, because it\u2019s processing data for both tenants. Please note that authorization is used to control access to tenant\u2019s data both in Event Streams and in the Management API. See the sections below for more details.","title":"Data Consumers"},{"location":"multitenancy/#streams","text":"A Stream is a path of data through the HRI and consists of Kafka topics. There must be at least one stream for every unique combination of tenant and Data Integrator to satisfy HIPAA requirements. This logically separates the data and enables access to be restricted by tenant and Data Integrator. Note that different Data Integrators for the same tenant are not allowed to access each other\u2019s data. To facilitate this, topics are named using the tenant and Data Integrator\u2019s name, e.g. ingest.tenant.data-integrator.* . In the example above, Integrator B is processing data from two tenants and writes data to two topics, separating them by tenant. Credentials provided to Data Integrators must be locked down to specific topics.","title":"Streams"},{"location":"multitenancy/#data-types","text":"HRI is agnostic to the type of data being written to Kafka. In practice, a Data Integrator often provides a specific type of data (claims, clinical, imagery, etc.) to the HRI. Users/Consumers of HRI also may want separate provided data by type. This can be done by creating additional topics and including another (data type) identifier at the end of the topic name before .in . For example, ingest.t1.di1.claims.in . Note that Inbound topics must end with .in .","title":"Data Types"},{"location":"multitenancy/#hri-management-api","text":"The Management API also stores metadata about batches in separate indexes (in its Elasticsearch data store). All API endpoints include a tenant ID to support data segregation by tenant. Batch operations are segregated by Data Integrator such that one Integrator cannot access or modify the batches that another Integrator owns. See the Authorization page for more details.","title":"HRI Management API"},{"location":"processflow/","text":"Processing Flows \u00b6 This diagram depicts the \u201chappy-path\u201d flow through the HRI for a single batch . Steps \u00b6 The Data Integrator creates a new batch. The Management API writes a batch notification message to the associated notification topic. The Data Consumer receives the batch notification message. Data Integrator writes the data to the correct Kafka *.in topic. The Data Consumer may now begin reading the data from the Kafka topic but can choose to wait until step 8 to begin reading the data. The Data Integrator completes writing all data contained in this batch, and it then signals to the Management API that it completed sending the data for the batch. The Management API writes a batch notification message to the associated notification topic. The Data Consumer receives the batch notification message. Alternate Flows \u00b6 Batch Termination \u00b6 If the Data Integrator encounters an error after creating a batch in step 2, they may send a request to the Management API to terminate the batch. The Management API will then write a batch notification message to the associated notification topic, and the Consumer will receive it. Interleaved Batches \u00b6 The HRI does not prevent the Data Integrator from writing multiples batches into the same topic at the same time. Every record will have a header value that specifies the \u201cbatchId\u201d , which is returned from the Management API (see api-spec/management-api/management.yml ), so the Consumer can distinguish each one. In practice, the Data Integrator may only write one batch at a time. As necessary, additional input topics can be created to prevent the interleaving of batches or data types. However, please note that, in general, Kafka performs better with a small number of large topics .","title":"Processing Flows"},{"location":"processflow/#processing-flows","text":"This diagram depicts the \u201chappy-path\u201d flow through the HRI for a single batch .","title":"Processing Flows"},{"location":"processflow/#steps","text":"The Data Integrator creates a new batch. The Management API writes a batch notification message to the associated notification topic. The Data Consumer receives the batch notification message. Data Integrator writes the data to the correct Kafka *.in topic. The Data Consumer may now begin reading the data from the Kafka topic but can choose to wait until step 8 to begin reading the data. The Data Integrator completes writing all data contained in this batch, and it then signals to the Management API that it completed sending the data for the batch. The Management API writes a batch notification message to the associated notification topic. The Data Consumer receives the batch notification message.","title":"Steps"},{"location":"processflow/#alternate-flows","text":"","title":"Alternate Flows"},{"location":"processflow/#batch-termination","text":"If the Data Integrator encounters an error after creating a batch in step 2, they may send a request to the Management API to terminate the batch. The Management API will then write a batch notification message to the associated notification topic, and the Consumer will receive it.","title":"Batch Termination"},{"location":"processflow/#interleaved-batches","text":"The HRI does not prevent the Data Integrator from writing multiples batches into the same topic at the same time. Every record will have a header value that specifies the \u201cbatchId\u201d , which is returned from the Management API (see api-spec/management-api/management.yml ), so the Consumer can distinguish each one. In practice, the Data Integrator may only write one batch at a time. As necessary, additional input topics can be created to prevent the interleaving of batches or data types. However, please note that, in general, Kafka performs better with a small number of large topics .","title":"Interleaved Batches"},{"location":"releases/","text":"HRI Releases \u00b6 This page lists the releases with notes for the HRI with information about how to upgrade from one version to the next. If upgrading multiple versions, check the upgrade notes of all versions in between. Note that the HRI has an overall release number, which is included here. Unless stated otherwise in the release notes of a specific version, upgrading the HRI should be achievable without downtime. If you are only upgrading to a new patch version, simply upgrade your existing deployment with the patched version. Otherwise, the new version can be deployed and configured separately in a different namespace while the old HRI version is still active, and the old HRI version can be deleted when migration is complete. In this case, be sure to use the same Elasticsearch and Event Streams instances for both of the HRI versions. Again, please see the upgrade notes for all versions between your current and target versions for any additional requirements. v1.x \u00b6 Version 1.x uses IBM Functions to deploy the Management API and does not include validation processing. It is deprecated as of March 2022. Please upgrade to the latest version at your earliest convenience. v1.2.7 \u00b6 Release notes \u00b6 An HRI patch release to update Golang packages to their latest versions to address vulnerabilities. v1.2.6 \u00b6 Release notes \u00b6 First Alvearie release using new GitHub Actions workflow. There were no substantive changes to the API or deployed code. v1.2.5 \u00b6 Release notes \u00b6 Initial open source release of v1.x .","title":"Releases"},{"location":"releases/#hri-releases","text":"This page lists the releases with notes for the HRI with information about how to upgrade from one version to the next. If upgrading multiple versions, check the upgrade notes of all versions in between. Note that the HRI has an overall release number, which is included here. Unless stated otherwise in the release notes of a specific version, upgrading the HRI should be achievable without downtime. If you are only upgrading to a new patch version, simply upgrade your existing deployment with the patched version. Otherwise, the new version can be deployed and configured separately in a different namespace while the old HRI version is still active, and the old HRI version can be deleted when migration is complete. In this case, be sure to use the same Elasticsearch and Event Streams instances for both of the HRI versions. Again, please see the upgrade notes for all versions between your current and target versions for any additional requirements.","title":"HRI Releases"},{"location":"releases/#v1x","text":"Version 1.x uses IBM Functions to deploy the Management API and does not include validation processing. It is deprecated as of March 2022. Please upgrade to the latest version at your earliest convenience.","title":"v1.x"},{"location":"releases/#v127","text":"","title":"v1.2.7"},{"location":"releases/#release-notes","text":"An HRI patch release to update Golang packages to their latest versions to address vulnerabilities.","title":"Release notes"},{"location":"releases/#v126","text":"","title":"v1.2.6"},{"location":"releases/#release-notes_1","text":"First Alvearie release using new GitHub Actions workflow. There were no substantive changes to the API or deployed code.","title":"Release notes"},{"location":"releases/#v125","text":"","title":"v1.2.5"},{"location":"releases/#release-notes_2","text":"Initial open source release of v1.x .","title":"Release notes"},{"location":"troubleshooting/","text":"Troubleshooting \u00b6 HRI Management API Issues \u00b6 Authentication is possible but has failed or not yet been provided \u00b6 Issue: the Management API responds with this error: { \"code\" : \"2cde977c23b3ab78befed56a6aac3820\" , \"error\" : \"Authentication is possible but has failed or not yet been provided.\" } Cause: the IBM Functions API Gateway is unable to authenticate with the backend IBM Function Actions. Actions have an API key and the API Gateway must be configured to use that API key. Typically, this is due to the API Gateway missing or having the wrong API key. Fix: Option 1 . Try redeploying the Management API. This will generate a new API key and set it on the Actions and the API Gateway endpoints. Option 2 . Manually update the API endpoints. You will need the IBM Cloud CLI and Functions plugin. See these instructions for installing them. Set the CLI resource group to where the Management API is deployed. ibmcloud target -g <resource_group> Set the CLI Functions namespace to where the Management API is deployed. ibmcloud fn namespace target <namespace> Recreate all the API methods. Below is an example of the create batch endpoint. Reference mgmt-api-manifest.yml for a complete list of the endpoints and their associated actions. ibmcloud fn api create /hri /tenants/{tenantId}/batches post hri_mgmt_api/create_batch --response-type http Option 3 . Manually update the API gateway JSON configuration. Follow steps 1-3 above to set up the IBM CLI. Download the API json configuration. ibmcloud fn api get hri-batches > api.json Get the API key set for the Actions. Run ibmcloud fn package get hri_mgmt_api and the output should have a \"require-whisk-auth\" entry for each action. It should be the same value for every action. E.g: { \"key\": \"require-whisk-auth\", \"value\": \"hIadvQ8w4nkJeWa3i7OPDb9WqTAUV9d6\" }, Edit the api.json file and add or replace the API key. There will be a x-ibm-configuration element, and a couple of layers inside, an array of execute elements, one for each endpoint. Each of these needs to have a set-variable.actions element that sets message.headers.X-Require-Whisk-Auth to the API key. Make sure there is one for every endpoint and that they match the API key from step 3. { \"execute\" : [ { \"invoke\" : { \"target-url\" : \"https://us-south.functions.cloud.ibm.com/api/v1/web/a98e053a-4a77-46b3-9791-53d4dfa370fb/hri_mgmt_api/get_batches.http$(request.path)\" , \"verb\" : \"keep\" } }, { \"set-variable\" : { \"actions\" : [ { \"set\" : \"message.headers.X-Require-Whisk-Auth\" , \"value\" : \"hIadvQ8w4nkJeWa3i7OPDb9WqTAUV9d6\" } ] } } ], \"operations\" : [ \"getTenantsTenantidBatches\" ] } Event Streams Issues \u00b6 SSL Certificate Issues \u00b6 Issue: when attempting to connect to Event Streams you receive SSL errors. In Java, you might get this exception sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target . With curl, you get this error SSL Certificate: Invalid certificate chain . Cause: the Enterprise Event Streams brokers use a certificate signed by issuer=C = US, O = Let\u2019s Encrypt, CN = Let\u2019s Encrypt Authority X3 which is different from Standard versions. Whatever client is connecting to Event Streams is missing the Let\u2019s Encrypt public certificate. For Java, this certificate was added in version 8u101 . Fix: add the Let\u2019s Encrypt public certificate to your trusted root certificates. This varies depending on what technology you are using. Java comes with its own \u2018truststore\u2019 with root certificates and all operating systems also store root certificates. The Let\u2019s Encrypt certificates can be downloaded here . There are several guides online for adding certificates to the Java trust store. Here\u2019s one from Oracle. Here is a guide for several operating systems. Elasticsearch \u00b6 Setup Curl with IBM Elasticsearch \u00b6 Below are instructions for setting up Curl to interact with the Elasticsearch API directly. This can be useful when investigating issues or in some update scenarios like modifying existing index templates. Note: this requires the use of the command-line cURL tool In your IBM Cloud Elasticsearch (Document Store) account, you will need to have either: * an existing Service Credential you can access OR * create a new Service Credential that you will use below for the $USERNAME and $PASSWORD needed to run the Elasticsearch REST commands To create a new Service Credential: Navigate to the Elasticsearch service instance in your IBM Cloud account . Then click on the \u201cService Credentials\u201d link on the left-hand side Elasticsearch service menu: On the Service Credentials page click the New Credential button: You will name your new credential and click the \u201cAdd\u201d button. After that, your credential will be generated for you and you will be able to click on the \u201cView Credentials\u201d link. From this expanded service credentials window, you may retrieve your newly created Elasticsearch Username and Password that you will need for the Elasticsearch REST commands using cURL. Next, you will need to download the certificate and export it, so cURL can authenticate with the IBM Cloud Elasticsearch instance. To do this: Navigate to the Management screen for your Elasticsearch instance, which will look something like this (ID field obscured in the ScreenCap for security): Scroll down your screen, and in the \u201cConnections\u201d panel, click on the \u201cCLI\u201d toggle. You will be using cURL to run commands from your local environment on the IBM Cloud Elasticsearch instance. See this page in the IBM Cloud Documentation for more info on Connecting to Elasticsearch with cURL . In the \u201cConnections\u201d panel, there is a section for TLS Certificate . You will want to save the text from the \u201cContents\u201d panel in that TLS Certificate section to a local file such as: / Users /[ yourLocalUserName ]/ certs / hri - documentstore . crt Use the contents of this file with cURL by exporting it to CURL_CA_BUNDLE . export CURL_CA_BUNDLE =/ local - path / to / file / hri - documentstore . crt Find the base url in the \u201cPublic CLI endpoint\u201d textbox. In this example it starts with https://8165307 : You can now interact with the Elasticsearch REST API using cURL. For example, you can get the status of the cluster by performing a GET on the _cluster/health?pretty endpoint. curl -X GET -u <username>:<password> \\ https://8165307e-6130-4581-942d-20fcfc4e795d.bkvfvtld0lmh0umkfi70.databases.appdomain.cloud:30600/_cluster/health?pretty You can get the default index template by performing a GET to the _index_template/batches endpoint. curl -X GET -u <username>:<password> \\ https://8165307e-6130-4581-942d-20fcfc4e795d.bkvfvtld0lmh0umkfi70.databases.appdomain.cloud:30600/_index_template/batches You can also get the mapping for existing indexes (one per tenant) at <index>/_mapping . curl -X GET -u <username>:<password> \\ https://8165307e-6130-4581-942d-20fcfc4e795d.bkvfvtld0lmh0umkfi70.databases.appdomain.cloud:30600/test-batches/_mapping","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"troubleshooting/#hri-management-api-issues","text":"","title":"HRI Management API Issues"},{"location":"troubleshooting/#authentication-is-possible-but-has-failed-or-not-yet-been-provided","text":"Issue: the Management API responds with this error: { \"code\" : \"2cde977c23b3ab78befed56a6aac3820\" , \"error\" : \"Authentication is possible but has failed or not yet been provided.\" } Cause: the IBM Functions API Gateway is unable to authenticate with the backend IBM Function Actions. Actions have an API key and the API Gateway must be configured to use that API key. Typically, this is due to the API Gateway missing or having the wrong API key. Fix: Option 1 . Try redeploying the Management API. This will generate a new API key and set it on the Actions and the API Gateway endpoints. Option 2 . Manually update the API endpoints. You will need the IBM Cloud CLI and Functions plugin. See these instructions for installing them. Set the CLI resource group to where the Management API is deployed. ibmcloud target -g <resource_group> Set the CLI Functions namespace to where the Management API is deployed. ibmcloud fn namespace target <namespace> Recreate all the API methods. Below is an example of the create batch endpoint. Reference mgmt-api-manifest.yml for a complete list of the endpoints and their associated actions. ibmcloud fn api create /hri /tenants/{tenantId}/batches post hri_mgmt_api/create_batch --response-type http Option 3 . Manually update the API gateway JSON configuration. Follow steps 1-3 above to set up the IBM CLI. Download the API json configuration. ibmcloud fn api get hri-batches > api.json Get the API key set for the Actions. Run ibmcloud fn package get hri_mgmt_api and the output should have a \"require-whisk-auth\" entry for each action. It should be the same value for every action. E.g: { \"key\": \"require-whisk-auth\", \"value\": \"hIadvQ8w4nkJeWa3i7OPDb9WqTAUV9d6\" }, Edit the api.json file and add or replace the API key. There will be a x-ibm-configuration element, and a couple of layers inside, an array of execute elements, one for each endpoint. Each of these needs to have a set-variable.actions element that sets message.headers.X-Require-Whisk-Auth to the API key. Make sure there is one for every endpoint and that they match the API key from step 3. { \"execute\" : [ { \"invoke\" : { \"target-url\" : \"https://us-south.functions.cloud.ibm.com/api/v1/web/a98e053a-4a77-46b3-9791-53d4dfa370fb/hri_mgmt_api/get_batches.http$(request.path)\" , \"verb\" : \"keep\" } }, { \"set-variable\" : { \"actions\" : [ { \"set\" : \"message.headers.X-Require-Whisk-Auth\" , \"value\" : \"hIadvQ8w4nkJeWa3i7OPDb9WqTAUV9d6\" } ] } } ], \"operations\" : [ \"getTenantsTenantidBatches\" ] }","title":"Authentication is possible but has failed or not yet been provided"},{"location":"troubleshooting/#event-streams-issues","text":"","title":"Event Streams Issues"},{"location":"troubleshooting/#ssl-certificate-issues","text":"Issue: when attempting to connect to Event Streams you receive SSL errors. In Java, you might get this exception sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target . With curl, you get this error SSL Certificate: Invalid certificate chain . Cause: the Enterprise Event Streams brokers use a certificate signed by issuer=C = US, O = Let\u2019s Encrypt, CN = Let\u2019s Encrypt Authority X3 which is different from Standard versions. Whatever client is connecting to Event Streams is missing the Let\u2019s Encrypt public certificate. For Java, this certificate was added in version 8u101 . Fix: add the Let\u2019s Encrypt public certificate to your trusted root certificates. This varies depending on what technology you are using. Java comes with its own \u2018truststore\u2019 with root certificates and all operating systems also store root certificates. The Let\u2019s Encrypt certificates can be downloaded here . There are several guides online for adding certificates to the Java trust store. Here\u2019s one from Oracle. Here is a guide for several operating systems.","title":"SSL Certificate Issues"},{"location":"troubleshooting/#elasticsearch","text":"","title":"Elasticsearch"},{"location":"troubleshooting/#setup-curl-with-ibm-elasticsearch","text":"Below are instructions for setting up Curl to interact with the Elasticsearch API directly. This can be useful when investigating issues or in some update scenarios like modifying existing index templates. Note: this requires the use of the command-line cURL tool In your IBM Cloud Elasticsearch (Document Store) account, you will need to have either: * an existing Service Credential you can access OR * create a new Service Credential that you will use below for the $USERNAME and $PASSWORD needed to run the Elasticsearch REST commands To create a new Service Credential: Navigate to the Elasticsearch service instance in your IBM Cloud account . Then click on the \u201cService Credentials\u201d link on the left-hand side Elasticsearch service menu: On the Service Credentials page click the New Credential button: You will name your new credential and click the \u201cAdd\u201d button. After that, your credential will be generated for you and you will be able to click on the \u201cView Credentials\u201d link. From this expanded service credentials window, you may retrieve your newly created Elasticsearch Username and Password that you will need for the Elasticsearch REST commands using cURL. Next, you will need to download the certificate and export it, so cURL can authenticate with the IBM Cloud Elasticsearch instance. To do this: Navigate to the Management screen for your Elasticsearch instance, which will look something like this (ID field obscured in the ScreenCap for security): Scroll down your screen, and in the \u201cConnections\u201d panel, click on the \u201cCLI\u201d toggle. You will be using cURL to run commands from your local environment on the IBM Cloud Elasticsearch instance. See this page in the IBM Cloud Documentation for more info on Connecting to Elasticsearch with cURL . In the \u201cConnections\u201d panel, there is a section for TLS Certificate . You will want to save the text from the \u201cContents\u201d panel in that TLS Certificate section to a local file such as: / Users /[ yourLocalUserName ]/ certs / hri - documentstore . crt Use the contents of this file with cURL by exporting it to CURL_CA_BUNDLE . export CURL_CA_BUNDLE =/ local - path / to / file / hri - documentstore . crt Find the base url in the \u201cPublic CLI endpoint\u201d textbox. In this example it starts with https://8165307 : You can now interact with the Elasticsearch REST API using cURL. For example, you can get the status of the cluster by performing a GET on the _cluster/health?pretty endpoint. curl -X GET -u <username>:<password> \\ https://8165307e-6130-4581-942d-20fcfc4e795d.bkvfvtld0lmh0umkfi70.databases.appdomain.cloud:30600/_cluster/health?pretty You can get the default index template by performing a GET to the _index_template/batches endpoint. curl -X GET -u <username>:<password> \\ https://8165307e-6130-4581-942d-20fcfc4e795d.bkvfvtld0lmh0umkfi70.databases.appdomain.cloud:30600/_index_template/batches You can also get the mapping for existing indexes (one per tenant) at <index>/_mapping . curl -X GET -u <username>:<password> \\ https://8165307e-6130-4581-942d-20fcfc4e795d.bkvfvtld0lmh0umkfi70.databases.appdomain.cloud:30600/test-batches/_mapping","title":"Setup Curl with IBM Elasticsearch"}]}