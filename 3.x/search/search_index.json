{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Health Record Ingestion service \u00b6 The HRI is a deployment ready service for streaming Health-related data into the IBM Cloud. It provides a \u201cfront door\u201d for \u201cData Integrators\u201d to send data into the cloud, while supporting both batch-processing and data streaming workflows. It provides features to initiate and track the movement of a dataset for both \u201cData Integrators\u201d and \u201cData Consumers\u201d . The key features are: Streaming - all data is streamed Batch support - a collection of health data records can be streamed and processed together Validation - optional record level validation is performed on the data Multitenancy - supports segregation of data by tenant and Data Integrator Key Technologies \u00b6 Event Streams , an IBM Cloud-based Apache Kafka managed service, is the technology used for producing and consuming the data streams Elasticsearch is the distributed NoSQL data store that is used to store information about batches Flink is a stateful stream processing framework that is used to perform validation. IBM Cloud Dependencies \u00b6 The HRI was developed on the IBM Cloud and currently does not support running on other public or private clouds. However, as a part of Alvearie, the goal is to support other public and private cloud, which the team continues to work towards. Please see the Roadmap for additional details. Core Architecture \u00b6 Topics \u00b6 Health data, which may include PHI , is written to and read from the Kafka topics. There must be separate topics for each tenant and Data Integrator in order to meet data separability requirements. A set of four topics is used per Stream of data that flows through the HRI. Data Integrators write data to the *.in topic and Data Consumers read from the *.out topic. Batch status notifications are written to the *.notification topic and invalid record notifications are written to the *.invalid topic. Batches \u00b6 Health Record Datasets often have requirements to be processed together \u201cas a set\u201d (partially or in their entirety) when moving the data into the cloud. Hence, HRI has been built with support to process a dataset as a Batch . See Batch for a detailed definition. How much data goes in a batch is really up to the solution. The HRI Management API provides support for starting, completing, terminating, and searching for batches. Any change to a batch results in a message being written to the associated notification topic in Kafka. Data Format \u00b6 HRI does not impose any requirements on the format of the Health Data records written to Kafka. There is a separate effort to define a common FHIR model for PHI data. However, The HRI does require the batchId to be in the record header. Data Integrators may include any number of additional custom header values that they wish to pass onto data consumers. An example of a custom header value might be something like originating_producer_id , an originating data producer (or org) ID value that may need to be communicated to the data consumers. Additional Reading \u00b6 Processing Flows API specification Multi-tenancy Deployment Administration Authorization Validation Performance Monitoring & Logging Troubleshooting Releases Roadmap Glossary Questions \u00b6 Please contact these team members for further questions: David N. Perkins, Team Lead: david.n.perkins@ibm.com Aram S. Openden, Maintainer: aram.openden1@ibm.com Contributors \u00b6 Alisa Kotliarova: alisa@ibm.com Daniel Baxter: djbaxter@us.ibm.com Fred Ricci: fjricci@us.ibm.com","title":"Introduction"},{"location":"#health-record-ingestion-service","text":"The HRI is a deployment ready service for streaming Health-related data into the IBM Cloud. It provides a \u201cfront door\u201d for \u201cData Integrators\u201d to send data into the cloud, while supporting both batch-processing and data streaming workflows. It provides features to initiate and track the movement of a dataset for both \u201cData Integrators\u201d and \u201cData Consumers\u201d . The key features are: Streaming - all data is streamed Batch support - a collection of health data records can be streamed and processed together Validation - optional record level validation is performed on the data Multitenancy - supports segregation of data by tenant and Data Integrator","title":"Health Record Ingestion service"},{"location":"#key-technologies","text":"Event Streams , an IBM Cloud-based Apache Kafka managed service, is the technology used for producing and consuming the data streams Elasticsearch is the distributed NoSQL data store that is used to store information about batches Flink is a stateful stream processing framework that is used to perform validation.","title":"Key Technologies"},{"location":"#ibm-cloud-dependencies","text":"The HRI was developed on the IBM Cloud and currently does not support running on other public or private clouds. However, as a part of Alvearie, the goal is to support other public and private cloud, which the team continues to work towards. Please see the Roadmap for additional details.","title":"IBM Cloud Dependencies"},{"location":"#core-architecture","text":"","title":"Core Architecture"},{"location":"#topics","text":"Health data, which may include PHI , is written to and read from the Kafka topics. There must be separate topics for each tenant and Data Integrator in order to meet data separability requirements. A set of four topics is used per Stream of data that flows through the HRI. Data Integrators write data to the *.in topic and Data Consumers read from the *.out topic. Batch status notifications are written to the *.notification topic and invalid record notifications are written to the *.invalid topic.","title":"Topics"},{"location":"#batches","text":"Health Record Datasets often have requirements to be processed together \u201cas a set\u201d (partially or in their entirety) when moving the data into the cloud. Hence, HRI has been built with support to process a dataset as a Batch . See Batch for a detailed definition. How much data goes in a batch is really up to the solution. The HRI Management API provides support for starting, completing, terminating, and searching for batches. Any change to a batch results in a message being written to the associated notification topic in Kafka.","title":"Batches"},{"location":"#data-format","text":"HRI does not impose any requirements on the format of the Health Data records written to Kafka. There is a separate effort to define a common FHIR model for PHI data. However, The HRI does require the batchId to be in the record header. Data Integrators may include any number of additional custom header values that they wish to pass onto data consumers. An example of a custom header value might be something like originating_producer_id , an originating data producer (or org) ID value that may need to be communicated to the data consumers.","title":"Data Format"},{"location":"#additional-reading","text":"Processing Flows API specification Multi-tenancy Deployment Administration Authorization Validation Performance Monitoring & Logging Troubleshooting Releases Roadmap Glossary","title":"Additional Reading"},{"location":"#questions","text":"Please contact these team members for further questions: David N. Perkins, Team Lead: david.n.perkins@ibm.com Aram S. Openden, Maintainer: aram.openden1@ibm.com","title":"Questions"},{"location":"#contributors","text":"Alisa Kotliarova: alisa@ibm.com Daniel Baxter: djbaxter@us.ibm.com Fred Ricci: fjricci@us.ibm.com","title":"Contributors"},{"location":"admin/","text":"HRI Administration \u00b6 HRI Administration tasks include: Managing Tenants Onboarding Data Integrators Manually through the Event Streams UI Using the Management API Stream Endpoints Validation Processing HRI Management User Authorization Managing Tenants \u00b6 Every Tenant has a separate index in Elasticsearch . Indexes are named <tenantId>-batches . For example, if the tenant ID is 24 , the new index name will be 24-batches . Some solutions may include a tenant prefix, e.g. tenant24-batches . The tenant ID may contain any lowercase alphanumeric strings, - , and _ . Whatever pattern you use, this will determine the tenant ID path parameter required in most of the Management API endpoints , and will need to be communicated to Data Integrators for that tenant. If you are using an existing deployment, check with team managing the instance on naming conventions. There are four Management API endpoints that support Tenant management in Elasticsearch for HRI: Create, Get (all tenants), Get (specific tenant) and Delete. Please note that all four of these endpoints require IAM authentication - you will need to pass in an IAM Bearer token as part of the authorization header in the requests. Create Tenant \u00b6 Use the Management API Create Tenant endpoint to create new Tenants. This will create a new index for the Tenant in Elasticsearch. The Create Tenant endpoint takes in one path parameter tenantId , which may only contain lowercase alphanumeric characters, - , and _ . For example, for the tenantId \u201c24\u201d you would use the following curl command: curl -X POST \\ <hri_base_url>/tenants/24 \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json' Get Tenants \u00b6 The Get endpoint takes in no parameters and returns a list of all tenantIds that have an Elastic index. Assuming the above Create was run, then the following cURL command (HTTP/Get operation) would return a list containing the single tenantId \u201c24\u201d: curl -X GET \\ <hri_base_url>/tenants \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json' Get Tenant \u00b6 The GetTenant endpoint can also take in a tenantId and will return a list of information on the associated index. Assuming the above Create was run, then the following cURL command (HTTP/Get operation) would return a list of information on the index for \u201c24\u201d: curl -X GET \\ <hri_base_url>/tenants/24 \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json' Delete Tenant \u00b6 Like Create , the Delete Tenant endpoint takes in tenantId . The following curl command will delete the elastic index for 24 : curl -X DELETE \\ <hri_base_url>/tenants/24 \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json' Onboarding New Data Integrators \u00b6 Every unique combination of Tenant and Data Integrator must have a separate \u2018stream\u2019, path of data through the HRI, in order to satisfy HIPAA data isolation requirements. See Multi-tenancy for more details. Every stream includes two or four topics, depending on whether validation is enabled, and topics can be added manually through the Event Streams UI and API or automatically through the Management API. Topic Naming Conventions: \u00b6 Please note that HRI uses the following naming conventions for topics: ingest.<tenantId>.<dataIntegratorId>[.metadataTag].in ingest.<tenantId>.<dataIntegratorId>[.metadataTag].notification ingest.<tenantId>.<dataIntegratorId>[.metadataTag].out ingest.<tenantId>.<dataIntegratorId>[.metadataTag].invalid The metadataTag is an optional field that can be set to any user defined value. For example, with tenant id 24 , Data Integrator id data-int-1 , and metadata tag claims , the topics would be: ingest.24.data-int-1.claims.in ingest.24.data-int-1.claims.notification ingest.24.data-int-1.claims.out ingest.24.data-int-1.claims.invalid The tenant ID must be consistent with the Elasticsearch index tenant ID . Only use lowercase alphanumeric characters, - , and _ . Note: the *.out and *.invalid topics are only present when validation is enabled. Topic Sizing Recommendations \u00b6 Topic sizing mainly consists of the number of partitions and the retention policy (time and amount of data), and mainly depends on three factors: the size of batches the frequency of batches the throughput of Data Consumers The number of partitions determines how many Data Consumer processes can read and process the data in parallel, with one process per partition. The retention policy determines how much data the topic will store before removing it due to time or size constraints. The throughput of Data Consumers is mainly dependent on what is done with the data after reading it. At a minimum, the Data Consumers must be faster than the long term rate of incoming data. Otherwise, data may be removed from the topic before it is consumed, resulting in data loss. For example, if a 10 GB batch is written by Data Integrators every 4 hours, the Data Consumers must have a process rate greater than 2.5 GBs per hour. Otherwise, they will continually fall behind and eventually not process some data before it is removed from the topic. For initial sizing, estimate the peak data load size (could include multiple batches depending on your solution) and divide by 1 GB (compressed) to get the number of partitions. For example, if the peak data load is 10 GBs, then start with 10 partitions. Also set the retention size to 10 GBs. In production environments, the retention time is typically 3-7 days. With longer times, you may need to also increase the retention size. As the frequency and size of data loads increase, the number of partitions and retention policy should also increase. As the throughput of Data Consumers increases, the number of partitions and retention policy should decrease. See Performance Testing for additional details on our performance tests. NOTE: if creating topics manually, only 1 partition is needed for the *.notification and *.invalid topics. Manually through the Event Streams UI \u00b6 Create the required Kafka topics described above using the IBM Cloud Event Streams UI. Create the new Topic \u00b6 To Create the new topics, in your IBM Cloud account navigate to your Event Streams service. Click on the Topics tab/link on the left of your screen. Next, click on the \u201cCreate Topic\u201d button on the upper right-hand side of the screen: Enter your topic name, as defined by the naming conventions above, the number of partitions, and the retention time. Note that you must toggle Show all available options to see the partition and retention policy options. Using the Management API stream endpoints \u00b6 There are three Management API Stream endpoints: Create , Get , and Delete . Please note that all three of these endpoints require IAM authentication - you will need to pass in an IAM Bearer token as part of the authorization header in the requests. In the case of Create and Delete , the IAM bearer token must be associated with a user who has Manager role permissions. For Get , the bearer token must be associated with a user who has at least Reader role permissions. See Event Streams documentation for details on permissions. Create Stream \u00b6 Instead of manually creating the topics for the Tenant and Data Integrator pairing, you may choose to use the Management API Create Stream endpoint which will create the topics for you, and take into account if validation is enabled. The Create Stream endpoint takes in two path parameters, tenantId and streamId , where streamId is made up of the Data Integrator ID and an optional qualifier, delimited by \u2018.\u2019. Both tenantId and streamdId may only contain lowercase alphanumeric characters, - , and _ . streamdId may also contain one \u2018.\u2019. For example, for the tenantId \u201ctenant24\u201d, Data Integrator ID \u201cdata-int-1\u201d and optional qualifier \u201cqualifier1\u201d, you could use the following curl command: curl -X POST \\ <hri_base_url>/tenants/tenant24/streams/data-int-1.qualifier1 \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json' \\ -d '{ \"numPartitions\":1, \"retentionMs\":86400000 }' This will create the following topics: ingest.tenant24.data-int-1.qualifier1.in ingest.tenant24.data-int-1.qualifier1.notification ingest.tenant24.data-int-1.qualifier1.out ingest.tenant24.data-int-1.qualifier1.invalid Note: numPartitions and retentionMs topic configurations are required. There are other optional configurations that can also be passed in, see the Stream Api Spec for more details on these optional fields. The numPartitions parameter is applied to the *.in and *.out topics, but the *.notification and *.invalid topics are set to 1. *.out and *.invalid topics are only created when validation is enabled Get Streams \u00b6 The Get Streams endpoint takes in tenantId as a path parameter, and returns a list of all streamId \u2018s associated with that tenant. Assuming the above Create was run, then the following cURL command (HTTP/Get operation) would return a list containing the single streamId data-int-1.qualifier1 : curl -X GET \\ <hri_base_url>/tenants/tenant24/streams \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json' Delete Stream \u00b6 Like Create , the Delete Stream endpoint takes in two path parameters, tenantId and streamId . The following curl command will delete both the ingest.tenant24.data-int-1.qualifier1.in and ingest.tenant24.data-int-1.qualifier1.notification topics: curl -X DELETE \\ <hri_base_url>/tenants/tenant24/streams/data-int-1.qualifier1 \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json' Note that HRI topic naming conventions require topics to start with the prefix \u201cingest\u201d and end with the suffix \u201cin\u201d, \u201cnotification\u201d, \u201cout\u201d, or \u201cinvalid\u201d. Both the Get and Delete endpoints will ignore any topics that don\u2019t follow this convention. Creating Service Credentials for Kafka Permissions \u00b6 You have to create an Event Streams (Kafka) Service Credential for every client that will need to read from or write to one or more topics. Typically, every Data Integrator and downstream Data Consumer will need their own service credential. A service credential can be configured with IAM policies to just grant read and or write access to specific topics and consumer groups, so only one service credential is needed for each entity. You do not need to create a service credential for every topic. Each service credential will initially have read or write access to all topics when created depending on whether the \u2018Reader\u2019 or \u2018Writer\u2019 role is selected respectively. But they can be configured with IAM policies to just grant read and or write access to specific topics and consumer groups regardless of which role is selected. It\u2019s good practice to select \u2018Writer\u2019 for Data Integrators and \u2018Reader\u2019 for downstream consumers. To create a service credential, navigate to the Event Streams - Service Credentials page, and then click on the \u201cNew Credential\u201d button on the right-hand side of your screen: You can see in the Screenshot below an example of creating a Data Integrator service credential with the Writer role: Next, go to the IBM Cloud (Access) IAM Management tools to further restrict the service credential, by using the \u201cManage\u201d drop-down menu at the top of your screen and choosing \u2018Access (IAM)\u2019. Then select \u2018Service IDs\u2019 from the left menu. Next select the Service ID for the credential you created. If you selected \u2018Auto Generate\u2019 when creating the credential, it will have the same name, but be careful, because there can be multiple Service IDs with the same name. After selecting the Service ID, go to the \u2018Access policies\u2019 tab. You should see one policy that is created by default, which allows read or write access to all topics. To restrict access to particular topics, you have to modify the existing policy and create several new ones. Below are rules about what policies to create for specific access. Create a policy with \u2018Reader\u2019 service access and \u2018Resource type\u2019 set to cluster . This will allow the Service ID to access the Event Streams brokers. To allow read & write permissions to a particular topic, create a policy with \u2018Reader\u2019 and \u2018Writer\u2019 service access, \u2018Resource type\u2019 set to topic , and \u2018Resource ID\u2019 set to the topic name. To allow just read permissions to a particular topic, create a policy with \u2018Reader\u2019 service access, \u2018Resource type\u2019 set to topic , and \u2018Resource ID\u2019 set to the topic name. To allow subscribing to topics, the Service ID must be given permissions to create consumer groups. This is the standard way of consuming messages from Kafka. Create a policy with \u2018Reader\u2019 service access, \u2018Resource type\u2019 set to group , and \u2018Resource ID\u2019 set to a unique ID for this client followed by a * using \u2018string matches\u2019, e.g. data-int-1* . This allows the client to only create consumer groups that begin with this ID when connecting to Event Streams. This also prevents clients who are reading from the same topics from interfering with each other. To allow the use of transactions when writing to topics, create a policy with \u2018Writer\u2019 service access and the \u2018Resource type\u2019 set to txnid . We highly encourage the use of transactions for exactly-once write semantics. Duplicate messages will cause validation failures or problems for downstream consumers. Note : policies support wildcards at the beginning and/or end of the \u2018Resource ID\u2019 field when using the \u2018string matches\u2019 qualifier. This enables a single policy to allow access to multiple topics when they share a common substring. For example, ingest.24.data-int-1.* could be used to allow access to the ingest.24.data-int-1.in , ingest.24.data-int-1.notification , ingest.24.data-int-1.out , and ingest.24.data-int-1.invalid topics. The Data Integrator will need read & write access to the input topic, but only read access to the notification topic. This requires five IAM policies total. Below is an example. A downstream consumer will need just read access to the input and notification topics. This requires three IAM policies total. Below is an example. More detailed documentation on how to configure IAM policies for Event Streams can be found here . Validation Processing \u00b6 When Validation is enabled, every stream has a Flink job that performs the validation processing. When new streams are created, a validation Flink job also has to be created. See Validation and Processing Flows for more details. The standard HRI deployment comes with two validation jobs: FHIR Validation - this job validates that every record meets the FHIR v4.0.1 Bundle json schema. The IBM FHIR Server\u2019s FHIR model is used for validation. Passthrough Validation - this job does not perform any record validation and simply passes records to the *.out topic Note that all validation jobs also validate that the batch has the correct number of records. Solutions can also create their own custom Flink validation jobs, see Validation for more details. Job parameters \u00b6 The standard HRI validation jobs have the following parameters: Flag Description -b , --brokers Comma-separated list of Kafka brokers -i , --input Kafka input topic -p , --password Kafka password -d , --batch-completion-delay Amount of time to wait in milliseconds for extra records before completing a batch. Optional, and defaults to 5 minutes. -m , --mgmt-url Base Url for the Management API, e.g. https://hri-mgmt-api/hri -c , --client-id clientId for the HRI Internal Application associated with this job and defined in the OAuth service. More information on HRI Internal Applications can be found here . -s , --client-secret secret key for the HRI Internal Application defined in the OAuth service. More information on HRI Internal Applications can be found here . -a , --audience Audience for getting OAuth access tokens. For AppId this should be set to the HRI Application clientId -o , --oauth-url Base Url for the OAuth service. Managing Flink Jobs \u00b6 There are several administrative tasks that may need to be performed including: creating new jobs scaling jobs stopping and resuming jobs upgrading jobs There are several ways to perform these tasks: the UI, the Rest API, and a CLI. We recommend solutions use the Rest API to automate tasks such as creating new Flink jobs for new streams. Below we will describe each of these options. Flink Rest API \u00b6 Flink has a full featured REST API , which we encourage solutions to use for automation. Here are some useful endpoints: GET /jars - returns a list of all the uploaded jars POST /jars/upload - uploads a new jar POST /jars/:jarid/run - starts a new job POST /jobs/:jobid/stop - stops a job and creates a savepoint. Set drain to true to allow the job to finish processing any in-flight data. Do Not Cancel Flink Jobs \u00b6 The Flink Rest API contains a PATCH /jobs/:jobid endpoint that can cancel Flink jobs. This endpoint will not gracefully shutdown HRI validation jobs, and overusing it can cause an Out of Memory error. If a job needs to be stopped, call the [POST /jobs/:jobid/stop ] endpoint. Flink UI \u00b6 Flink has a UI where jars can be viewed, uploaded, and run, but has limited support for rescaling jobs and creating savepoints. To view a list of jars, select \u2018Submit New Job\u2019 from the right menu. Click \u2018+ Add New\u2019 to upload a new jar. To start a new job, select a jar from the list and fill in the parameters. Put the job parameters in the \u2018Program Arguments\u2019 field. You can also set the parallelism or enter a savepoint path to start from a prior job\u2019s savepoint. Flink CLI \u00b6 Flink has a CLI that be used to manually perform tasks or automate them. It supports all actions needed for managing jobs and savepoints but does not include all the monitoring endpoints of the REST API. It also has to be configured correctly to communicate with the Flink cluster. HRI Management User Authorization \u00b6 In your authorization service, create a new scope for this tenant and assign it to the Data Integrators and Consumers that need access. See Authorization for more details. If you are using the IBM App ID Service, please see Authorization - \u201cAdding Data Integrators and Consumers\u201d .","title":"Administration"},{"location":"admin/#hri-administration","text":"HRI Administration tasks include: Managing Tenants Onboarding Data Integrators Manually through the Event Streams UI Using the Management API Stream Endpoints Validation Processing HRI Management User Authorization","title":"HRI Administration"},{"location":"admin/#managing-tenants","text":"Every Tenant has a separate index in Elasticsearch . Indexes are named <tenantId>-batches . For example, if the tenant ID is 24 , the new index name will be 24-batches . Some solutions may include a tenant prefix, e.g. tenant24-batches . The tenant ID may contain any lowercase alphanumeric strings, - , and _ . Whatever pattern you use, this will determine the tenant ID path parameter required in most of the Management API endpoints , and will need to be communicated to Data Integrators for that tenant. If you are using an existing deployment, check with team managing the instance on naming conventions. There are four Management API endpoints that support Tenant management in Elasticsearch for HRI: Create, Get (all tenants), Get (specific tenant) and Delete. Please note that all four of these endpoints require IAM authentication - you will need to pass in an IAM Bearer token as part of the authorization header in the requests.","title":"Managing Tenants"},{"location":"admin/#create-tenant","text":"Use the Management API Create Tenant endpoint to create new Tenants. This will create a new index for the Tenant in Elasticsearch. The Create Tenant endpoint takes in one path parameter tenantId , which may only contain lowercase alphanumeric characters, - , and _ . For example, for the tenantId \u201c24\u201d you would use the following curl command: curl -X POST \\ <hri_base_url>/tenants/24 \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json'","title":"Create Tenant"},{"location":"admin/#get-tenants","text":"The Get endpoint takes in no parameters and returns a list of all tenantIds that have an Elastic index. Assuming the above Create was run, then the following cURL command (HTTP/Get operation) would return a list containing the single tenantId \u201c24\u201d: curl -X GET \\ <hri_base_url>/tenants \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json'","title":"Get Tenants"},{"location":"admin/#get-tenant","text":"The GetTenant endpoint can also take in a tenantId and will return a list of information on the associated index. Assuming the above Create was run, then the following cURL command (HTTP/Get operation) would return a list of information on the index for \u201c24\u201d: curl -X GET \\ <hri_base_url>/tenants/24 \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json'","title":"Get Tenant"},{"location":"admin/#delete-tenant","text":"Like Create , the Delete Tenant endpoint takes in tenantId . The following curl command will delete the elastic index for 24 : curl -X DELETE \\ <hri_base_url>/tenants/24 \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json'","title":"Delete Tenant"},{"location":"admin/#onboarding-new-data-integrators","text":"Every unique combination of Tenant and Data Integrator must have a separate \u2018stream\u2019, path of data through the HRI, in order to satisfy HIPAA data isolation requirements. See Multi-tenancy for more details. Every stream includes two or four topics, depending on whether validation is enabled, and topics can be added manually through the Event Streams UI and API or automatically through the Management API.","title":"Onboarding New Data Integrators"},{"location":"admin/#topic-naming-conventions","text":"Please note that HRI uses the following naming conventions for topics: ingest.<tenantId>.<dataIntegratorId>[.metadataTag].in ingest.<tenantId>.<dataIntegratorId>[.metadataTag].notification ingest.<tenantId>.<dataIntegratorId>[.metadataTag].out ingest.<tenantId>.<dataIntegratorId>[.metadataTag].invalid The metadataTag is an optional field that can be set to any user defined value. For example, with tenant id 24 , Data Integrator id data-int-1 , and metadata tag claims , the topics would be: ingest.24.data-int-1.claims.in ingest.24.data-int-1.claims.notification ingest.24.data-int-1.claims.out ingest.24.data-int-1.claims.invalid The tenant ID must be consistent with the Elasticsearch index tenant ID . Only use lowercase alphanumeric characters, - , and _ . Note: the *.out and *.invalid topics are only present when validation is enabled.","title":"Topic Naming Conventions:"},{"location":"admin/#topic-sizing-recommendations","text":"Topic sizing mainly consists of the number of partitions and the retention policy (time and amount of data), and mainly depends on three factors: the size of batches the frequency of batches the throughput of Data Consumers The number of partitions determines how many Data Consumer processes can read and process the data in parallel, with one process per partition. The retention policy determines how much data the topic will store before removing it due to time or size constraints. The throughput of Data Consumers is mainly dependent on what is done with the data after reading it. At a minimum, the Data Consumers must be faster than the long term rate of incoming data. Otherwise, data may be removed from the topic before it is consumed, resulting in data loss. For example, if a 10 GB batch is written by Data Integrators every 4 hours, the Data Consumers must have a process rate greater than 2.5 GBs per hour. Otherwise, they will continually fall behind and eventually not process some data before it is removed from the topic. For initial sizing, estimate the peak data load size (could include multiple batches depending on your solution) and divide by 1 GB (compressed) to get the number of partitions. For example, if the peak data load is 10 GBs, then start with 10 partitions. Also set the retention size to 10 GBs. In production environments, the retention time is typically 3-7 days. With longer times, you may need to also increase the retention size. As the frequency and size of data loads increase, the number of partitions and retention policy should also increase. As the throughput of Data Consumers increases, the number of partitions and retention policy should decrease. See Performance Testing for additional details on our performance tests. NOTE: if creating topics manually, only 1 partition is needed for the *.notification and *.invalid topics.","title":"Topic Sizing Recommendations"},{"location":"admin/#manually-through-the-event-streams-ui","text":"Create the required Kafka topics described above using the IBM Cloud Event Streams UI.","title":"Manually through the Event Streams UI"},{"location":"admin/#create-the-new-topic","text":"To Create the new topics, in your IBM Cloud account navigate to your Event Streams service. Click on the Topics tab/link on the left of your screen. Next, click on the \u201cCreate Topic\u201d button on the upper right-hand side of the screen: Enter your topic name, as defined by the naming conventions above, the number of partitions, and the retention time. Note that you must toggle Show all available options to see the partition and retention policy options.","title":"Create the new Topic"},{"location":"admin/#using-the-management-api-stream-endpoints","text":"There are three Management API Stream endpoints: Create , Get , and Delete . Please note that all three of these endpoints require IAM authentication - you will need to pass in an IAM Bearer token as part of the authorization header in the requests. In the case of Create and Delete , the IAM bearer token must be associated with a user who has Manager role permissions. For Get , the bearer token must be associated with a user who has at least Reader role permissions. See Event Streams documentation for details on permissions.","title":"Using the Management API stream endpoints"},{"location":"admin/#create-stream","text":"Instead of manually creating the topics for the Tenant and Data Integrator pairing, you may choose to use the Management API Create Stream endpoint which will create the topics for you, and take into account if validation is enabled. The Create Stream endpoint takes in two path parameters, tenantId and streamId , where streamId is made up of the Data Integrator ID and an optional qualifier, delimited by \u2018.\u2019. Both tenantId and streamdId may only contain lowercase alphanumeric characters, - , and _ . streamdId may also contain one \u2018.\u2019. For example, for the tenantId \u201ctenant24\u201d, Data Integrator ID \u201cdata-int-1\u201d and optional qualifier \u201cqualifier1\u201d, you could use the following curl command: curl -X POST \\ <hri_base_url>/tenants/tenant24/streams/data-int-1.qualifier1 \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json' \\ -d '{ \"numPartitions\":1, \"retentionMs\":86400000 }' This will create the following topics: ingest.tenant24.data-int-1.qualifier1.in ingest.tenant24.data-int-1.qualifier1.notification ingest.tenant24.data-int-1.qualifier1.out ingest.tenant24.data-int-1.qualifier1.invalid Note: numPartitions and retentionMs topic configurations are required. There are other optional configurations that can also be passed in, see the Stream Api Spec for more details on these optional fields. The numPartitions parameter is applied to the *.in and *.out topics, but the *.notification and *.invalid topics are set to 1. *.out and *.invalid topics are only created when validation is enabled","title":"Create Stream"},{"location":"admin/#get-streams","text":"The Get Streams endpoint takes in tenantId as a path parameter, and returns a list of all streamId \u2018s associated with that tenant. Assuming the above Create was run, then the following cURL command (HTTP/Get operation) would return a list containing the single streamId data-int-1.qualifier1 : curl -X GET \\ <hri_base_url>/tenants/tenant24/streams \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json'","title":"Get Streams"},{"location":"admin/#delete-stream","text":"Like Create , the Delete Stream endpoint takes in two path parameters, tenantId and streamId . The following curl command will delete both the ingest.tenant24.data-int-1.qualifier1.in and ingest.tenant24.data-int-1.qualifier1.notification topics: curl -X DELETE \\ <hri_base_url>/tenants/tenant24/streams/data-int-1.qualifier1 \\ -H 'Accept: application/json' \\ -H 'Authorization: Bearer <token>' \\ -H 'Content-Type: application/json' Note that HRI topic naming conventions require topics to start with the prefix \u201cingest\u201d and end with the suffix \u201cin\u201d, \u201cnotification\u201d, \u201cout\u201d, or \u201cinvalid\u201d. Both the Get and Delete endpoints will ignore any topics that don\u2019t follow this convention.","title":"Delete Stream"},{"location":"admin/#creating-service-credentials-for-kafka-permissions","text":"You have to create an Event Streams (Kafka) Service Credential for every client that will need to read from or write to one or more topics. Typically, every Data Integrator and downstream Data Consumer will need their own service credential. A service credential can be configured with IAM policies to just grant read and or write access to specific topics and consumer groups, so only one service credential is needed for each entity. You do not need to create a service credential for every topic. Each service credential will initially have read or write access to all topics when created depending on whether the \u2018Reader\u2019 or \u2018Writer\u2019 role is selected respectively. But they can be configured with IAM policies to just grant read and or write access to specific topics and consumer groups regardless of which role is selected. It\u2019s good practice to select \u2018Writer\u2019 for Data Integrators and \u2018Reader\u2019 for downstream consumers. To create a service credential, navigate to the Event Streams - Service Credentials page, and then click on the \u201cNew Credential\u201d button on the right-hand side of your screen: You can see in the Screenshot below an example of creating a Data Integrator service credential with the Writer role: Next, go to the IBM Cloud (Access) IAM Management tools to further restrict the service credential, by using the \u201cManage\u201d drop-down menu at the top of your screen and choosing \u2018Access (IAM)\u2019. Then select \u2018Service IDs\u2019 from the left menu. Next select the Service ID for the credential you created. If you selected \u2018Auto Generate\u2019 when creating the credential, it will have the same name, but be careful, because there can be multiple Service IDs with the same name. After selecting the Service ID, go to the \u2018Access policies\u2019 tab. You should see one policy that is created by default, which allows read or write access to all topics. To restrict access to particular topics, you have to modify the existing policy and create several new ones. Below are rules about what policies to create for specific access. Create a policy with \u2018Reader\u2019 service access and \u2018Resource type\u2019 set to cluster . This will allow the Service ID to access the Event Streams brokers. To allow read & write permissions to a particular topic, create a policy with \u2018Reader\u2019 and \u2018Writer\u2019 service access, \u2018Resource type\u2019 set to topic , and \u2018Resource ID\u2019 set to the topic name. To allow just read permissions to a particular topic, create a policy with \u2018Reader\u2019 service access, \u2018Resource type\u2019 set to topic , and \u2018Resource ID\u2019 set to the topic name. To allow subscribing to topics, the Service ID must be given permissions to create consumer groups. This is the standard way of consuming messages from Kafka. Create a policy with \u2018Reader\u2019 service access, \u2018Resource type\u2019 set to group , and \u2018Resource ID\u2019 set to a unique ID for this client followed by a * using \u2018string matches\u2019, e.g. data-int-1* . This allows the client to only create consumer groups that begin with this ID when connecting to Event Streams. This also prevents clients who are reading from the same topics from interfering with each other. To allow the use of transactions when writing to topics, create a policy with \u2018Writer\u2019 service access and the \u2018Resource type\u2019 set to txnid . We highly encourage the use of transactions for exactly-once write semantics. Duplicate messages will cause validation failures or problems for downstream consumers. Note : policies support wildcards at the beginning and/or end of the \u2018Resource ID\u2019 field when using the \u2018string matches\u2019 qualifier. This enables a single policy to allow access to multiple topics when they share a common substring. For example, ingest.24.data-int-1.* could be used to allow access to the ingest.24.data-int-1.in , ingest.24.data-int-1.notification , ingest.24.data-int-1.out , and ingest.24.data-int-1.invalid topics. The Data Integrator will need read & write access to the input topic, but only read access to the notification topic. This requires five IAM policies total. Below is an example. A downstream consumer will need just read access to the input and notification topics. This requires three IAM policies total. Below is an example. More detailed documentation on how to configure IAM policies for Event Streams can be found here .","title":"Creating Service Credentials for Kafka Permissions"},{"location":"admin/#validation-processing","text":"When Validation is enabled, every stream has a Flink job that performs the validation processing. When new streams are created, a validation Flink job also has to be created. See Validation and Processing Flows for more details. The standard HRI deployment comes with two validation jobs: FHIR Validation - this job validates that every record meets the FHIR v4.0.1 Bundle json schema. The IBM FHIR Server\u2019s FHIR model is used for validation. Passthrough Validation - this job does not perform any record validation and simply passes records to the *.out topic Note that all validation jobs also validate that the batch has the correct number of records. Solutions can also create their own custom Flink validation jobs, see Validation for more details.","title":"Validation Processing"},{"location":"admin/#job-parameters","text":"The standard HRI validation jobs have the following parameters: Flag Description -b , --brokers Comma-separated list of Kafka brokers -i , --input Kafka input topic -p , --password Kafka password -d , --batch-completion-delay Amount of time to wait in milliseconds for extra records before completing a batch. Optional, and defaults to 5 minutes. -m , --mgmt-url Base Url for the Management API, e.g. https://hri-mgmt-api/hri -c , --client-id clientId for the HRI Internal Application associated with this job and defined in the OAuth service. More information on HRI Internal Applications can be found here . -s , --client-secret secret key for the HRI Internal Application defined in the OAuth service. More information on HRI Internal Applications can be found here . -a , --audience Audience for getting OAuth access tokens. For AppId this should be set to the HRI Application clientId -o , --oauth-url Base Url for the OAuth service.","title":"Job parameters"},{"location":"admin/#managing-flink-jobs","text":"There are several administrative tasks that may need to be performed including: creating new jobs scaling jobs stopping and resuming jobs upgrading jobs There are several ways to perform these tasks: the UI, the Rest API, and a CLI. We recommend solutions use the Rest API to automate tasks such as creating new Flink jobs for new streams. Below we will describe each of these options.","title":"Managing Flink Jobs"},{"location":"admin/#flink-rest-api","text":"Flink has a full featured REST API , which we encourage solutions to use for automation. Here are some useful endpoints: GET /jars - returns a list of all the uploaded jars POST /jars/upload - uploads a new jar POST /jars/:jarid/run - starts a new job POST /jobs/:jobid/stop - stops a job and creates a savepoint. Set drain to true to allow the job to finish processing any in-flight data.","title":"Flink Rest API"},{"location":"admin/#do-not-cancel-flink-jobs","text":"The Flink Rest API contains a PATCH /jobs/:jobid endpoint that can cancel Flink jobs. This endpoint will not gracefully shutdown HRI validation jobs, and overusing it can cause an Out of Memory error. If a job needs to be stopped, call the [POST /jobs/:jobid/stop ] endpoint.","title":"Do Not Cancel Flink Jobs"},{"location":"admin/#flink-ui","text":"Flink has a UI where jars can be viewed, uploaded, and run, but has limited support for rescaling jobs and creating savepoints. To view a list of jars, select \u2018Submit New Job\u2019 from the right menu. Click \u2018+ Add New\u2019 to upload a new jar. To start a new job, select a jar from the list and fill in the parameters. Put the job parameters in the \u2018Program Arguments\u2019 field. You can also set the parallelism or enter a savepoint path to start from a prior job\u2019s savepoint.","title":"Flink UI"},{"location":"admin/#flink-cli","text":"Flink has a CLI that be used to manually perform tasks or automate them. It supports all actions needed for managing jobs and savepoints but does not include all the monitoring endpoints of the REST API. It also has to be configured correctly to communicate with the Flink cluster.","title":"Flink CLI"},{"location":"admin/#hri-management-user-authorization","text":"In your authorization service, create a new scope for this tenant and assign it to the Data Integrators and Consumers that need access. See Authorization for more details. If you are using the IBM App ID Service, please see Authorization - \u201cAdding Data Integrators and Consumers\u201d .","title":"HRI Management User Authorization"},{"location":"apispec/","text":"HRI API Specification \u00b6 The HRI consists of two separate APIs: the Management API and Apache Kafka. Management API Specification \u00b6 The Management API is defined using the OpenAPI 3.0 specification: management.yml . You can open the file directly or use a program such as IntelliJ or Swagger UI to view it. HRI Tenants & Elasticsearch Indices \u00b6 HRI has been designed with a multi-tenant cloud architecture . The API mainly contains methods for managing Tenants like creating, getting, and deleting. Each of these calls takes in the tenantId. The ID is appended with the suffix -batches to create an index in Elasticsearch, where all the batch metadata is stored. A Get call without a tenantId will return a list of all tenants. The Get call, when given a tenantId, will return information on the Elastic index of a specific tenant. Below is a table of the fields returned by this call: Field Description health health of the Elastic cluster status status of the index, can be open or closed index the name of the index, which will be the tenantId with -batches appended to it uuid universally unique identifier pri number of primary shards rep number of replicas docs.count number of batches documents stored in the index docs.deleted number of batches documents deleted from the index store.size store size taken by primary and replica shards pri.store.size store size taken only by primary shards Batches \u00b6 The API contains methods for managing batches like creating, getting, and updating. Below is a table of the fields: Field Description id auto generated unique ID name name of the batch, provided by the Data Integrator integratorId unique ID of the Data Integrator that created this batch topic Kafka topic that contains the data, provided by the Data Integrator dataType the type of data, provided by the Data Integrator status status of the batch: [ started, sendCompleted, completed, terminated, failed ] startDate the date and time the batch was started endDate the date and time the batch was completed, terminated, or failed expectedRecordCount the number of records in the batch, provided by the Data Integrator when calling \u2018sendComplete\u2019 recordCount (deprecated) the number of records in the batch, provided by the Data Integrator when calling \u2018sendComplete\u2019. Replaced by expectedRecordCount and deprecated in v2.0.0 actualRecordCount the number of records received, calculated by validation processing invalidThreshold the number of invalid records allowed in this batch before the batch fails validation, provided by the Data Integrator; defaults to -1 (infinite) invalidRecordCount the number of invalid records, calculated by validation processing metadata custom json value, optional Only the name , topic , and dataType fields are required when creating a batch. The invalidRecordCount field is used by validation processing, so that when this many invalid records are encountered, the HRI will have determined that the entire batch has Failed Validation . The expectedRecordCount is provided by the Data Integrator when calling the \u2018sendComplete\u2019 endpoint, and thus not always present. The recordCount field is identical and provides backward compatibility with older versions of HRI. recordCount is deprecated in release v2.0.0 and will be removed in a later release. The metadata field is optional and allows the Data Integrator to include any additional information about the batch that Data Consumers might request. This information will be included in all notification messages. All other fields are generated by the API. Streams \u00b6 The API also contains methods for managing Kafka topics like creating, getting, and deleting. Below is a table of the fields: Field Description id stream ID, consisting of a data integrator and optional qualifier, delimited by \u2018.\u2019 numPartitions the number of partitions on the topic retentionMs length of time in milliseconds before log segments are automatically discarded from a partition retentionBytes optional maximum size in bytes that a partition can grow before discarding log segments cleanupPolicy optional retention policy on old log segments segmentMs optional time in milliseconds after which Kafka will force the log to roll even if the segment file isn\u2019t full segmentBytes optional log segment file size in bytes segmentIndexBytes optional size in bytes of the index that maps offsets to file positions Only the numPartitions and retentionMs fields are required when creating a stream. The rest of the topic configurations ( retentionBytes , cleanupPolicy , segmentMs , segmentBytes , and segmentIndexBytes ) are optional. Below is a table of the default values and acceptable ranges for these optional fields: Field Default value Acceptable values/ranges retentionBytes 1073741824 [10485760..1073741824] cleanupPolicy delete [ delete, compact ] segmentMs nil [300000..2592000000] segmentBytes 536870912 [10485760..536870912] segmentIndexBytes nil [102400..104857600] If the cleanupPolicy field is set to compact, it will disable deletion based on time, ignoring the value set for the field retentionMs . Apache Kafka \u00b6 Apache Kafka has its own API and clients are available for most languages. If using IBM Event Streams, see their documentation for details on connection parameters. Below are the requirements on the records written to and read from Kafka. Health Input Data - FHIR Model \u00b6 HRI does not impose any requirements on the format of the content of the Health (data) records written to Kafka, although Alvearie has selected FHIR as the preferred data model for all Health Data. See their FHIR implementation guide for more details. Data Integrators and Data Consumers must work together to agree on the specifics of the input data such as format and frequency. HRI-Specific Requirements \u00b6 The HRI does have the following requirements and recommendations: Batch ID Header - every record must have a header entry with the batch ID that uses the key batchId . Data Integrators may include any additional header values, which will get passed downstream to consumers. Zstd Compression - use zstd compression when writing to Kafka by setting the compression.type producer configuration. Event Streams throttles network usage and limits Kafka messages to 1 MB. Using compression will help prevent an Event Streams bottleneck. 1 MB Message Limit - Event Streams limits messages to 1 MB. There is not a way to directly set the max message size after compression is applied in the Kafka producer. The max.request.size producer configuration is applied before compression. The batch.size producer configuration can be set to limit the batching of records, but it can also affect performance. We recommend doing performance testing to determine appropriate values based on your data. For records over 1 MB compressed, there are two strategies: External References - for records that have large binary attachments like images or pdfs, you may provide a reference to the resource in the message, rather than the (large) resource itself. For example, you could put a COS Object URL, or some other external data store URL, and key into the message. Splitting up Records - records can be split into smaller parts, sent through the HRI, and re-assembled by down stream consumers. Notification Messages \u00b6 The notification messages are json-encoded batches. They match the schema returned by the Management API described above, which is also defined here: batchNotification.json . Invalid Record Notifications \u00b6 When validation encounters an invalid record, an invalid record notification is written to the *.invalid topic. It contains a failure message, the batchId, and a pointer to the original record. Below is a table of the fields, and the json schema is defined here: invalidRecord.json . Field Description batchId Id of the Batch that the original record belongs to failure the description of why the original record was invalid topic the topic of the original record partition the partition of the original record offset the offset of the original record","title":"API Specification"},{"location":"apispec/#hri-api-specification","text":"The HRI consists of two separate APIs: the Management API and Apache Kafka.","title":"HRI API Specification"},{"location":"apispec/#management-api-specification","text":"The Management API is defined using the OpenAPI 3.0 specification: management.yml . You can open the file directly or use a program such as IntelliJ or Swagger UI to view it.","title":"Management API Specification"},{"location":"apispec/#hri-tenants-elasticsearch-indices","text":"HRI has been designed with a multi-tenant cloud architecture . The API mainly contains methods for managing Tenants like creating, getting, and deleting. Each of these calls takes in the tenantId. The ID is appended with the suffix -batches to create an index in Elasticsearch, where all the batch metadata is stored. A Get call without a tenantId will return a list of all tenants. The Get call, when given a tenantId, will return information on the Elastic index of a specific tenant. Below is a table of the fields returned by this call: Field Description health health of the Elastic cluster status status of the index, can be open or closed index the name of the index, which will be the tenantId with -batches appended to it uuid universally unique identifier pri number of primary shards rep number of replicas docs.count number of batches documents stored in the index docs.deleted number of batches documents deleted from the index store.size store size taken by primary and replica shards pri.store.size store size taken only by primary shards","title":"HRI Tenants &amp; Elasticsearch Indices"},{"location":"apispec/#batches","text":"The API contains methods for managing batches like creating, getting, and updating. Below is a table of the fields: Field Description id auto generated unique ID name name of the batch, provided by the Data Integrator integratorId unique ID of the Data Integrator that created this batch topic Kafka topic that contains the data, provided by the Data Integrator dataType the type of data, provided by the Data Integrator status status of the batch: [ started, sendCompleted, completed, terminated, failed ] startDate the date and time the batch was started endDate the date and time the batch was completed, terminated, or failed expectedRecordCount the number of records in the batch, provided by the Data Integrator when calling \u2018sendComplete\u2019 recordCount (deprecated) the number of records in the batch, provided by the Data Integrator when calling \u2018sendComplete\u2019. Replaced by expectedRecordCount and deprecated in v2.0.0 actualRecordCount the number of records received, calculated by validation processing invalidThreshold the number of invalid records allowed in this batch before the batch fails validation, provided by the Data Integrator; defaults to -1 (infinite) invalidRecordCount the number of invalid records, calculated by validation processing metadata custom json value, optional Only the name , topic , and dataType fields are required when creating a batch. The invalidRecordCount field is used by validation processing, so that when this many invalid records are encountered, the HRI will have determined that the entire batch has Failed Validation . The expectedRecordCount is provided by the Data Integrator when calling the \u2018sendComplete\u2019 endpoint, and thus not always present. The recordCount field is identical and provides backward compatibility with older versions of HRI. recordCount is deprecated in release v2.0.0 and will be removed in a later release. The metadata field is optional and allows the Data Integrator to include any additional information about the batch that Data Consumers might request. This information will be included in all notification messages. All other fields are generated by the API.","title":"Batches"},{"location":"apispec/#streams","text":"The API also contains methods for managing Kafka topics like creating, getting, and deleting. Below is a table of the fields: Field Description id stream ID, consisting of a data integrator and optional qualifier, delimited by \u2018.\u2019 numPartitions the number of partitions on the topic retentionMs length of time in milliseconds before log segments are automatically discarded from a partition retentionBytes optional maximum size in bytes that a partition can grow before discarding log segments cleanupPolicy optional retention policy on old log segments segmentMs optional time in milliseconds after which Kafka will force the log to roll even if the segment file isn\u2019t full segmentBytes optional log segment file size in bytes segmentIndexBytes optional size in bytes of the index that maps offsets to file positions Only the numPartitions and retentionMs fields are required when creating a stream. The rest of the topic configurations ( retentionBytes , cleanupPolicy , segmentMs , segmentBytes , and segmentIndexBytes ) are optional. Below is a table of the default values and acceptable ranges for these optional fields: Field Default value Acceptable values/ranges retentionBytes 1073741824 [10485760..1073741824] cleanupPolicy delete [ delete, compact ] segmentMs nil [300000..2592000000] segmentBytes 536870912 [10485760..536870912] segmentIndexBytes nil [102400..104857600] If the cleanupPolicy field is set to compact, it will disable deletion based on time, ignoring the value set for the field retentionMs .","title":"Streams"},{"location":"apispec/#apache-kafka","text":"Apache Kafka has its own API and clients are available for most languages. If using IBM Event Streams, see their documentation for details on connection parameters. Below are the requirements on the records written to and read from Kafka.","title":"Apache Kafka"},{"location":"apispec/#health-input-data-fhir-model","text":"HRI does not impose any requirements on the format of the content of the Health (data) records written to Kafka, although Alvearie has selected FHIR as the preferred data model for all Health Data. See their FHIR implementation guide for more details. Data Integrators and Data Consumers must work together to agree on the specifics of the input data such as format and frequency.","title":"Health Input Data - FHIR Model"},{"location":"apispec/#hri-specific-requirements","text":"The HRI does have the following requirements and recommendations: Batch ID Header - every record must have a header entry with the batch ID that uses the key batchId . Data Integrators may include any additional header values, which will get passed downstream to consumers. Zstd Compression - use zstd compression when writing to Kafka by setting the compression.type producer configuration. Event Streams throttles network usage and limits Kafka messages to 1 MB. Using compression will help prevent an Event Streams bottleneck. 1 MB Message Limit - Event Streams limits messages to 1 MB. There is not a way to directly set the max message size after compression is applied in the Kafka producer. The max.request.size producer configuration is applied before compression. The batch.size producer configuration can be set to limit the batching of records, but it can also affect performance. We recommend doing performance testing to determine appropriate values based on your data. For records over 1 MB compressed, there are two strategies: External References - for records that have large binary attachments like images or pdfs, you may provide a reference to the resource in the message, rather than the (large) resource itself. For example, you could put a COS Object URL, or some other external data store URL, and key into the message. Splitting up Records - records can be split into smaller parts, sent through the HRI, and re-assembled by down stream consumers.","title":"HRI-Specific Requirements"},{"location":"apispec/#notification-messages","text":"The notification messages are json-encoded batches. They match the schema returned by the Management API described above, which is also defined here: batchNotification.json .","title":"Notification Messages"},{"location":"apispec/#invalid-record-notifications","text":"When validation encounters an invalid record, an invalid record notification is written to the *.invalid topic. It contains a failure message, the batchId, and a pointer to the original record. Below is a table of the fields, and the json schema is defined here: invalidRecord.json . Field Description batchId Id of the Batch that the original record belongs to failure the description of why the original record was invalid topic the topic of the original record partition the partition of the original record offset the offset of the original record","title":"Invalid Record Notifications"},{"location":"auth/","text":"Authorization \u00b6 Apache Kafka Authorization \u00b6 Apache Kafka supports several types of authentication , but managed cloud services tend to limit it to one or two methods and implement their own authorization mechanism. For IBM Event Streams, see their connecting and security documentation and our administration page for more details. HRI Management API \u00b6 The Management API uses OAuth 2.0 , OIDC , and JWT standards for authorization. The specific types of authorization tokens used by the Management API endpoints are as follows: Endpoint Path Authorization Requirements tenant /tenants require IAM tokens with permissions to the Elasticsearch instance stream /tenants/<tenantId>/streams require IAM tokens with permissions to the Event Streams instance batch /tenants/<tenantId>/batches are optionally authorized by using JWT tokens from an OIDC-compliant authorization service, or with a custom authentication proxy service. More details on batch endpoint authorization below. healthcheck /healthcheck does not require any authorization Batch Endpoint Authorization \u00b6 Regarding batch endpoint authorization with JWT Tokens, the HRI does not include an authorization or authentication service, so users must provide one themselves and configure it as per the specification below. Oauth 2 and OIDC are widely used standards which allow solutions to use their own authorization & authentication. Your token issuer must be OIDC compliant, because the OIDC defined well-known endpoints are how the Management API will validate access tokens. HRI uses IBM Cloud App ID for its reference implementation of an OIDC-compliant authorization service. There are instructions provided below for how HRI users can configure their own instance of IBM Cloud App ID. Optionally, if solutions need to use a different authorization mechanism, authorization on the batch endpoints can be disabled. Solutions would deploy their own authorization proxy service, which would receive the initial request, perform authorization, and then proxy calls to the Management API service. Required Token Scopes: \u00b6 You must configure your authorization service to include HRI roles and tenant scopes in the access tokens (Note: scope is a standard claim that is a space-separated list of strings). HRI Role Scopes: \u00b6 hri_data_integrator - Data Integrator role; allows access to create, query, and update batches. Results are filtered to batches that the Data Integrator created. Note that the sub claim in the access token is used to identify the Data Integrator and stored in the batch integratorId field. hri_consumer - Data Consumer role; allows access to query batches. Results are not filtered. hri_internal - Internal HRI validation role; allows the Flink Validation job to call the batch processingComplete or fail endpoints. Tenant Scopes \u00b6 A Tenant scope matching the tenant ID is required every time you call a \u201cbatch\u201d endpoint in the Management API. The scope must have tenant_ as a prefix to the tenant ID. For example, if a data integrator tries to create a batch by making an HTTP POST call to tenants/24/batches , the token must contain scope tenant_24 , where the 24 is the tenantId. Also, see Multi-tenancy for more information about tenants and roles. See the API spec for more details about required roles for specific endpoints. Configuring the Token Issuer \u00b6 The token issuer is a configuration parameter of the Management API, and only one issuer may be set. The issuer must match the iss claim in the access token. See deployment for more details. Using App ID as Your Authorization Service \u00b6 IBM Cloud App ID is an IBM Cloud managed authorization and authentication service. App ID supports custom claims/scopes for client credentials by creating \u2018Applications\u2019 with scopes and assigning them to other \u2018Applications\u2019 via roles. Below are the manual steps to configure App ID for your reference, but you will probably want to automate some or all of these steps for your solution using the App ID API . (Note all examples here will be of the AppID-Landing App ID service instance.) Initial Configuration \u00b6 Create an \u201cHRI\u201d Application and its associated scopes. This application represents the HRI as a \u2018protected\u2019 resource (API) that other clients will be granted permissions to access. It is not a client that will connect to the HRI, and it\u2019s credentials should never be used. After opening your App ID service instance in your cloud account, note the App ID actions menu on the left-hand side of your screen: Click on the \u201cApplications\u201d menu item to see the Applications Screen: Click on the blue \u201cAdd application\u201d button on the top-right of the screen: Enter the Name ( HRI ), keep the default Type of \u201cRegular web application\u201d and add both of the required scopes using the \u201c+\u201d button ( hri_data_integrator , hri_consumer , hri_internal ; see HRI Scopes above) and click the \u201cSave\u201d button: Now you should be able to view your newly created \u201cHRI\u201d Application, and it\u2019s details (note: that you will now see the three scopes in the \u201cscopes\u201d section): Create Tenant scopes: In the App ID \u201cApplications\u201d screen, find the \u201cHRI\u201d Application created in step 1 above. Expand It\u2019s \u201c\u2026\u201d menu on the far right and select \u201cEdit\u201d. Using the scopes \u201c+\u201d button, create a scope with the naming convention tenant_<tenantId> for every current tenant in your solution, then click the \u201cSave\u201d button. Please note that we added a scope named \u201ctenant_test\u201d, where the test portion of this scope string exactly matches the actual Tenant Name that was created when Adding/Creating the new tenant : Create new roles for each of the scopes you created in steps 1 & 2. You will create roles for the HRI role scopes and for every existing Tenant in your solution. Click on the \u201cProfiles and roles\u201d sub-menu (under \u201cManage Authentication\u201d) and then on the \u201cRoles\u201d sub-menu under that: Click on the blue \u201cCreate Role\u201d button on the top-right of the screen. Enter a role name and description (optional). In the \u201cScopes\u201d field, enter the HRI Application name followed by a \u2018/\u2019 and then the scope name. Then click the \u201c+\u201d button. Each role will have a 1-to-1 relationship with one scope. Last, click the \u201cSave\u201d button. Below is example for the \u201ctest\u201d Tenant role: A example \u201ctest\u201d Tenant role: An example HRI Data Integrator role: An example HRI Consumer role: An example HRI Internal role: Adding Data Integrators and Consumers \u00b6 Create a new Application for every HRI client (Data Integrator or Data Consumer) that uses the Management API. For example, here is a screenshot that includes five (5) different user-instance Applications created for the HRI Integration Test (client): An example of what one of these new Applications would look like after created: Please note that the \u201cscopes\u201d property is empty and that this new \u201ccredential\u201d Application has been created for the \u201ctenant_test\u201d and a \u201cdata integrator\u201d. Hence, in Step 2 below, it will have the roles of \u201cTenant Test\u201d and \u201cHRI Data Integrator\u201d assigned to it. Assign roles to each of the Credential Applications you created in step 1 to grant access to the HRI and specific tenants. Currently, this can only be done via making HTTP calls directly to the App ID API, specifically the endpoint /management/v4/{tenantId}/applications/{clientId}/roles (see App ID API Specification section \u201cManagement API - Applications\u201d). For example, if you have just created a new Data Integrator credential Application, assign the HRI Data Integrator role, as well as a role for every tenant that data integrator produces data for, by taking the following actions: Using the command-line, Login to the IBM CLI and obtain an OAuth token: ibmcloud iam oauth-tokens That will return a message like this: IAM token: Bearer eyJraWQi.......{Very long string} Next, export that long string (starting after the \u201cBearer \u201d section) to a bash/shell variable named TOKEN: export TOKEN = eyJraWQi ....... { Very long string } Compose the correct HTTP/REST endpoint URL for the Assignment API call: Find the correct App ID HTTP/REST endpoint URL root for managing the instance. In service credentials, it is the value of the managementUrl field (e.g https://us-south.appid.cloud.ibm.com/management/v4/ec9bcf93-4863-46a7-bf66-19009b4e2ed8 ): To complete the URL you will need to add the following: /applications/[client_id_of_application_assigning_to]/roles where \u201cclient_id_of_application_assigning_to\u201d will be replaced with clientId value from the Application, such as the \u201chri_integration_tenant_test_data_integrator\u201d Application. That clientId ends in 5f7ea : The completed full example App ID HTTP PUT url would look like this: https://us-south.appid.cloud.ibm.com/management/v4/ec9bcf93-4863-46a7-bf66-19009b4e2ed8/applications/d55258ed-e765-4f83-939b-71dc7775f7ea/roles Construct your JSON string of roles that you will be associating with this \u201chri_integration_tenant_test_data_integrator\u201d Application. For our example, as the name of our new Credential Application suggests, we will be assigning the \u201cTenant Test\u201d and \u201cHRI Data Integrator\u201d roles to the \u201chri_integration_tenant_test_data_integrator\u201d Application. You will need to use the \u201cid\u201d field value of each Role you are associating with this particular application. In our case, that means we will need the role \u201cid\u201ds ending in d9c16 and 26d12 : Your roles JSON string would then look like the following: {\"roles\":{\"ids\":[\"0d1e0b8f-0c8d-499e-87bb-82e6dded9c16\",\"8dd46d09-eb58-4881-b6bf-fd32f4d26d12\"]}} (Note that this is a list of n-number of roles, to which you may add more role IDs.) Finally, you can assemble your API HTTP request. Using a tool such as cURL at the command-line or Postman , create your REST/HTTP request. Using cURL at the command-line (assumes you have curl installed; see https://curl.haxx.se/download.html ): curl -X PUT https://us-south.appid.cloud.ibm.com/management/v4/ec9bcf93-4863-46a7-bf66-19009b4e2ed8/applications/d55258ed-e765-4f83-939b-71dc7775f7ea/roles \\ -H \"Authorization: Bearer $TOKEN\" \\ -H 'Content-Type: application/json' \\ -d '{\"roles\":{\"ids\":[\"0d1e0b8f-0c8d-499e-87bb-82e6dded9c16\",\"8dd46d09-eb58-4881-b6bf-fd32f4d26d12\"]}}' Note that your roles JSON string should be surrounded by single-quotes (\u2018). HRI Internal Application Setup \u00b6 An HRI Internal Application is only needed if Validation is enabled. Each of these Applications represents one or more Flink jobs that validate batches of records. At a minimum, HRI Internal Applications need to be assigned a tenant role, the hri_internal role, and the hri_consumer role. The recommended approach is to assign all tenant roles to a single HRI Internal application. In this approach, the same client-id and client-secret will be used for every single Flink job (see Validation Processing ). Authorization Workflow Example \u00b6 To present an example, Data Integrators and Consumers using App ID would need to request an access token from the App ID service using the OAuth 2.0 \u201cclient credentials\u201d grant flow for reference, see IBM App ID Documentation . The request must include the desired scopes and the HRI Application ID as the audience. This is what an example cURL statement would look like to request the access token: curl -X POST https://us-south.appid.cloud.ibm.com/oauth/v4/ec9bcf93-4863-46a7-bf66-19009b4e2ed8/token \\ -H 'Content-Type: application/x-www-form-urlencoded' \\ -H 'Authorization: Basic <client_id:client_password>' \\ -d 'grant_type=client_credentials&scope=tenant_test hri_data_integrator&audience=b8f85fbe-b00a-4296-b54b-e9ec09a5b2f3' ## Notes: For basic authentication you have to base64 encode the client_id and password like this echo -n '<client_id>:<client_password>' | base64 And, here is an example access token for a Data Integrator Application with access to tenant \u2018test\u2019, produced by App ID: { \"iss\" : \"https://us-south.appid.cloud.ibm.com/oauth/v4/ec9bcf93-4863-46a7-bf66-19009b4e2ed8\" , \"exp\" : 1598459309 , \"aud\" : [ \"b8f85fbe-b00a-4296-b54b-e9ec09a5b2f3\" ], \"sub\" : \"d55258ed-e765-4f83-939b-71dc7775f7ea\" , \"amr\" : [ \"appid_client_credentials\" ], \"iat\" : 1598455709 , \"tenant\" : \"ec9bcf93-4863-46a7-bf66-19009b4e2ed8\" , \"scope\" : \"tenant_test hri_data_integrator\" } Note the included scope(s): tenant_test hri_data_integrator","title":"Authorization"},{"location":"auth/#authorization","text":"","title":"Authorization"},{"location":"auth/#apache-kafka-authorization","text":"Apache Kafka supports several types of authentication , but managed cloud services tend to limit it to one or two methods and implement their own authorization mechanism. For IBM Event Streams, see their connecting and security documentation and our administration page for more details.","title":"Apache Kafka Authorization"},{"location":"auth/#hri-management-api","text":"The Management API uses OAuth 2.0 , OIDC , and JWT standards for authorization. The specific types of authorization tokens used by the Management API endpoints are as follows: Endpoint Path Authorization Requirements tenant /tenants require IAM tokens with permissions to the Elasticsearch instance stream /tenants/<tenantId>/streams require IAM tokens with permissions to the Event Streams instance batch /tenants/<tenantId>/batches are optionally authorized by using JWT tokens from an OIDC-compliant authorization service, or with a custom authentication proxy service. More details on batch endpoint authorization below. healthcheck /healthcheck does not require any authorization","title":"HRI Management API"},{"location":"auth/#batch-endpoint-authorization","text":"Regarding batch endpoint authorization with JWT Tokens, the HRI does not include an authorization or authentication service, so users must provide one themselves and configure it as per the specification below. Oauth 2 and OIDC are widely used standards which allow solutions to use their own authorization & authentication. Your token issuer must be OIDC compliant, because the OIDC defined well-known endpoints are how the Management API will validate access tokens. HRI uses IBM Cloud App ID for its reference implementation of an OIDC-compliant authorization service. There are instructions provided below for how HRI users can configure their own instance of IBM Cloud App ID. Optionally, if solutions need to use a different authorization mechanism, authorization on the batch endpoints can be disabled. Solutions would deploy their own authorization proxy service, which would receive the initial request, perform authorization, and then proxy calls to the Management API service.","title":"Batch Endpoint Authorization"},{"location":"auth/#required-token-scopes","text":"You must configure your authorization service to include HRI roles and tenant scopes in the access tokens (Note: scope is a standard claim that is a space-separated list of strings).","title":"Required Token Scopes:"},{"location":"auth/#hri-role-scopes","text":"hri_data_integrator - Data Integrator role; allows access to create, query, and update batches. Results are filtered to batches that the Data Integrator created. Note that the sub claim in the access token is used to identify the Data Integrator and stored in the batch integratorId field. hri_consumer - Data Consumer role; allows access to query batches. Results are not filtered. hri_internal - Internal HRI validation role; allows the Flink Validation job to call the batch processingComplete or fail endpoints.","title":"HRI Role Scopes:"},{"location":"auth/#tenant-scopes","text":"A Tenant scope matching the tenant ID is required every time you call a \u201cbatch\u201d endpoint in the Management API. The scope must have tenant_ as a prefix to the tenant ID. For example, if a data integrator tries to create a batch by making an HTTP POST call to tenants/24/batches , the token must contain scope tenant_24 , where the 24 is the tenantId. Also, see Multi-tenancy for more information about tenants and roles. See the API spec for more details about required roles for specific endpoints.","title":"Tenant Scopes"},{"location":"auth/#configuring-the-token-issuer","text":"The token issuer is a configuration parameter of the Management API, and only one issuer may be set. The issuer must match the iss claim in the access token. See deployment for more details.","title":"Configuring the Token Issuer"},{"location":"auth/#using-app-id-as-your-authorization-service","text":"IBM Cloud App ID is an IBM Cloud managed authorization and authentication service. App ID supports custom claims/scopes for client credentials by creating \u2018Applications\u2019 with scopes and assigning them to other \u2018Applications\u2019 via roles. Below are the manual steps to configure App ID for your reference, but you will probably want to automate some or all of these steps for your solution using the App ID API . (Note all examples here will be of the AppID-Landing App ID service instance.)","title":"Using App ID as Your Authorization Service"},{"location":"auth/#initial-configuration","text":"Create an \u201cHRI\u201d Application and its associated scopes. This application represents the HRI as a \u2018protected\u2019 resource (API) that other clients will be granted permissions to access. It is not a client that will connect to the HRI, and it\u2019s credentials should never be used. After opening your App ID service instance in your cloud account, note the App ID actions menu on the left-hand side of your screen: Click on the \u201cApplications\u201d menu item to see the Applications Screen: Click on the blue \u201cAdd application\u201d button on the top-right of the screen: Enter the Name ( HRI ), keep the default Type of \u201cRegular web application\u201d and add both of the required scopes using the \u201c+\u201d button ( hri_data_integrator , hri_consumer , hri_internal ; see HRI Scopes above) and click the \u201cSave\u201d button: Now you should be able to view your newly created \u201cHRI\u201d Application, and it\u2019s details (note: that you will now see the three scopes in the \u201cscopes\u201d section): Create Tenant scopes: In the App ID \u201cApplications\u201d screen, find the \u201cHRI\u201d Application created in step 1 above. Expand It\u2019s \u201c\u2026\u201d menu on the far right and select \u201cEdit\u201d. Using the scopes \u201c+\u201d button, create a scope with the naming convention tenant_<tenantId> for every current tenant in your solution, then click the \u201cSave\u201d button. Please note that we added a scope named \u201ctenant_test\u201d, where the test portion of this scope string exactly matches the actual Tenant Name that was created when Adding/Creating the new tenant : Create new roles for each of the scopes you created in steps 1 & 2. You will create roles for the HRI role scopes and for every existing Tenant in your solution. Click on the \u201cProfiles and roles\u201d sub-menu (under \u201cManage Authentication\u201d) and then on the \u201cRoles\u201d sub-menu under that: Click on the blue \u201cCreate Role\u201d button on the top-right of the screen. Enter a role name and description (optional). In the \u201cScopes\u201d field, enter the HRI Application name followed by a \u2018/\u2019 and then the scope name. Then click the \u201c+\u201d button. Each role will have a 1-to-1 relationship with one scope. Last, click the \u201cSave\u201d button. Below is example for the \u201ctest\u201d Tenant role: A example \u201ctest\u201d Tenant role: An example HRI Data Integrator role: An example HRI Consumer role: An example HRI Internal role:","title":"Initial Configuration"},{"location":"auth/#adding-data-integrators-and-consumers","text":"Create a new Application for every HRI client (Data Integrator or Data Consumer) that uses the Management API. For example, here is a screenshot that includes five (5) different user-instance Applications created for the HRI Integration Test (client): An example of what one of these new Applications would look like after created: Please note that the \u201cscopes\u201d property is empty and that this new \u201ccredential\u201d Application has been created for the \u201ctenant_test\u201d and a \u201cdata integrator\u201d. Hence, in Step 2 below, it will have the roles of \u201cTenant Test\u201d and \u201cHRI Data Integrator\u201d assigned to it. Assign roles to each of the Credential Applications you created in step 1 to grant access to the HRI and specific tenants. Currently, this can only be done via making HTTP calls directly to the App ID API, specifically the endpoint /management/v4/{tenantId}/applications/{clientId}/roles (see App ID API Specification section \u201cManagement API - Applications\u201d). For example, if you have just created a new Data Integrator credential Application, assign the HRI Data Integrator role, as well as a role for every tenant that data integrator produces data for, by taking the following actions: Using the command-line, Login to the IBM CLI and obtain an OAuth token: ibmcloud iam oauth-tokens That will return a message like this: IAM token: Bearer eyJraWQi.......{Very long string} Next, export that long string (starting after the \u201cBearer \u201d section) to a bash/shell variable named TOKEN: export TOKEN = eyJraWQi ....... { Very long string } Compose the correct HTTP/REST endpoint URL for the Assignment API call: Find the correct App ID HTTP/REST endpoint URL root for managing the instance. In service credentials, it is the value of the managementUrl field (e.g https://us-south.appid.cloud.ibm.com/management/v4/ec9bcf93-4863-46a7-bf66-19009b4e2ed8 ): To complete the URL you will need to add the following: /applications/[client_id_of_application_assigning_to]/roles where \u201cclient_id_of_application_assigning_to\u201d will be replaced with clientId value from the Application, such as the \u201chri_integration_tenant_test_data_integrator\u201d Application. That clientId ends in 5f7ea : The completed full example App ID HTTP PUT url would look like this: https://us-south.appid.cloud.ibm.com/management/v4/ec9bcf93-4863-46a7-bf66-19009b4e2ed8/applications/d55258ed-e765-4f83-939b-71dc7775f7ea/roles Construct your JSON string of roles that you will be associating with this \u201chri_integration_tenant_test_data_integrator\u201d Application. For our example, as the name of our new Credential Application suggests, we will be assigning the \u201cTenant Test\u201d and \u201cHRI Data Integrator\u201d roles to the \u201chri_integration_tenant_test_data_integrator\u201d Application. You will need to use the \u201cid\u201d field value of each Role you are associating with this particular application. In our case, that means we will need the role \u201cid\u201ds ending in d9c16 and 26d12 : Your roles JSON string would then look like the following: {\"roles\":{\"ids\":[\"0d1e0b8f-0c8d-499e-87bb-82e6dded9c16\",\"8dd46d09-eb58-4881-b6bf-fd32f4d26d12\"]}} (Note that this is a list of n-number of roles, to which you may add more role IDs.) Finally, you can assemble your API HTTP request. Using a tool such as cURL at the command-line or Postman , create your REST/HTTP request. Using cURL at the command-line (assumes you have curl installed; see https://curl.haxx.se/download.html ): curl -X PUT https://us-south.appid.cloud.ibm.com/management/v4/ec9bcf93-4863-46a7-bf66-19009b4e2ed8/applications/d55258ed-e765-4f83-939b-71dc7775f7ea/roles \\ -H \"Authorization: Bearer $TOKEN\" \\ -H 'Content-Type: application/json' \\ -d '{\"roles\":{\"ids\":[\"0d1e0b8f-0c8d-499e-87bb-82e6dded9c16\",\"8dd46d09-eb58-4881-b6bf-fd32f4d26d12\"]}}' Note that your roles JSON string should be surrounded by single-quotes (\u2018).","title":"Adding Data Integrators and Consumers"},{"location":"auth/#hri-internal-application-setup","text":"An HRI Internal Application is only needed if Validation is enabled. Each of these Applications represents one or more Flink jobs that validate batches of records. At a minimum, HRI Internal Applications need to be assigned a tenant role, the hri_internal role, and the hri_consumer role. The recommended approach is to assign all tenant roles to a single HRI Internal application. In this approach, the same client-id and client-secret will be used for every single Flink job (see Validation Processing ).","title":"HRI Internal Application Setup"},{"location":"auth/#authorization-workflow-example","text":"To present an example, Data Integrators and Consumers using App ID would need to request an access token from the App ID service using the OAuth 2.0 \u201cclient credentials\u201d grant flow for reference, see IBM App ID Documentation . The request must include the desired scopes and the HRI Application ID as the audience. This is what an example cURL statement would look like to request the access token: curl -X POST https://us-south.appid.cloud.ibm.com/oauth/v4/ec9bcf93-4863-46a7-bf66-19009b4e2ed8/token \\ -H 'Content-Type: application/x-www-form-urlencoded' \\ -H 'Authorization: Basic <client_id:client_password>' \\ -d 'grant_type=client_credentials&scope=tenant_test hri_data_integrator&audience=b8f85fbe-b00a-4296-b54b-e9ec09a5b2f3' ## Notes: For basic authentication you have to base64 encode the client_id and password like this echo -n '<client_id>:<client_password>' | base64 And, here is an example access token for a Data Integrator Application with access to tenant \u2018test\u2019, produced by App ID: { \"iss\" : \"https://us-south.appid.cloud.ibm.com/oauth/v4/ec9bcf93-4863-46a7-bf66-19009b4e2ed8\" , \"exp\" : 1598459309 , \"aud\" : [ \"b8f85fbe-b00a-4296-b54b-e9ec09a5b2f3\" ], \"sub\" : \"d55258ed-e765-4f83-939b-71dc7775f7ea\" , \"amr\" : [ \"appid_client_credentials\" ], \"iat\" : 1598455709 , \"tenant\" : \"ec9bcf93-4863-46a7-bf66-19009b4e2ed8\" , \"scope\" : \"tenant_test hri_data_integrator\" } Note the included scope(s): tenant_test hri_data_integrator","title":"Authorization Workflow Example"},{"location":"deployment/","text":"HRI Dependencies Configuration/Setup \u00b6 This section is intended to help guide you to configure the dependent services that HRI uses in your own (public/private) IBM Cloud account. Create Elasticsearch cloud resource \u00b6 HRI Requires an Elasticsearch service deployment in your IBM Cloud account. Navigate to the Resource List in your Cloud account. Click the Create resource button in the top right corner. Enter \u201cElasticsearch\u201d in the catalog search bar and then select the Databases for Elasticsearch tile. Select the appropriate region and then configure the resource by providing a service name and resource group (Note: for all configuration examples below, the Resource Group is \u201cYOUR_Resource_GRP\u201d). You will also need to specify the desired resource allocations for Elasticsearch. Depending on your expected usage, your values may differ, but the values shown below will be sufficient in most cases. Then click the Create button. Once the Elasticsearch instance becomes active, you will need to set an \u201cadmin\u201d password. This is done from the Settings page of the Elasticsearch instance. Click the Service credentials link, and then click the New credential button. Provide a name for the service credential and then add it. This will be needed by the HRI Management API deployment. Create Event Streams cloud resource \u00b6 HRI also Requires an Event Streams(Kafka) service deployment in your IBM Cloud account. Navigate to the Resource List in your Cloud account. If an instance of Event Streams already exists in your Cloud account, then the HRI may be able to share that existing instance. If an Event Streams instance does not already exist, then create one by clicking the Create resource button in the top right corner. Enter \u201cEvent Streams\u201d in the catalog search bar and then select the Event Streams tile. Fill in an appropriate region , service name , and resource group . The Enterprise pricing plan (with custom key management via Key Protect) is required for HIPAA data processing. After creating an Enterprise instance of Event Streams, custom key management via Key Protect will need to be explicitly enabled (See Event Streams documentation ). NOTE: The Event Streams Enterprise plan is expensive, which is why we recommend sharing an instance, if possible. In non-Production environments, a Standard plan may be used for testing with non-HIPAA data if your organization\u2019s security team approves. Click the Service credentials link, and then click the New credential button to create a service credential with writer permissions. Provide a name for the service credential. This will be needed by the HRI Management API deployment. Create Authorization Service \u00b6 The HRI Management API requires an authorization service. Integration testing has been performed with IBM Cloud App ID , but any compliant service can be used. See Authorization for more details about the requirements and how to set up an App ID cloud service. Deploy the HRI Management API to Kubernetes \u00b6 The Management API is packaged in a docker container for Kubernetes deployments and available on GitHub . The service is stateless, so you can scale up to as many pods as needed or even deploy multiple instances that use the same Elasticsearch, Kafka, and Authorization services. The Management API process only needs a valid configuration, which can be provided by any combination of command line arguments, environment variables, and a yaml file. Conflicts are resolved in the order: command line arguments (highest priority), environment variables, and yaml file(lowest priority). Below is a table of the configuration options. CLI Flag Description Required -config-path Path to a config file, default \u201c./config.yml\u201d -elastic-crn Elasticsearch service CRN Yes -elastic-url The base url to the Elasticsearch instance Yes -elastic-cert Elasticsearch TLS public certificate Yes -elastic-username Elasticsearch user name Yes -elastic-password Elasticsearch password Yes -kafka-admin-url Kafka administration url Yes -kafka-brokers A list of Kafka brokers, separated by \u201c,\u201d Yes -kafka-properties A list of Kafka properties, entries separated by \u201c,\u201d, key value pairs separated by \u201c:\u201d Yes -auth-disabled true to disable OAuth Authorization, default: false -oidc-issuer The base URL of the OIDC issuer to use for OAuth authentication (e.g. https://us-south.appid.cloud.ibm.com/oauth/v4/ ). If -auth-disabled is false. -jwt-audience-id The ID of the HRI Management API within your authorization service. If -auth-disabled is false. -tls-enabled Toggle enabling an encrypted connection via TLS, default: false -tls-cert-path Path to the TLS certificate. If -tls-enabled is true. -tls-key-path Path to the TLS key. If -tls-enabled is true. -new-relic-enabled True to enable New Relic monitoring, default: false -new-relic-app-name Application name to aggregate data under in New Relic. If -new-relic-enabled is true. -new-relic-license-key New Relic license key. If -new-relic-enabled is true. -validation True to enable record validation, default false -log-level Minimum Log Level for logging output. Available levels are: Trace, Debug, Info, Warning, Error, Fatal and Panic. Default \u201cinfo\u201d) The associated environment variable is named by removing the leading - , capitalizing all characters, and replacing remaining - with _ . For example, the -auth-disabled flag has the AUTH_DISABLED environment variable. Yaml file entries are named by removing the leading - . For example, the -auth-disabled flag has the auth-disabled yaml entry. Validation Processing \u00b6 Validation processing is an optional feature that validates the data as it flows through the HRI. See Validation for details on the validation performed and customization options. See Processing Flows for details on how it fits into the overall architecture. Validation processing is built on Apache Flink , a highly available stateful stream processing framework, and requires a \u2018Session\u2019 cluster when enabled. See their deployment documentation for details on how to deploy a cluster. See the Administration page for details about how to deploy validation jobs. What\u2019s Next \u00b6 To set up your first Tenant and Data Integrator go to the Administration page. For detailed info on how the concept of Tenants and the Data Integrator role underpin the HRI Multitenancy approach, see the Multitenancy page.","title":"Deployment"},{"location":"deployment/#hri-dependencies-configurationsetup","text":"This section is intended to help guide you to configure the dependent services that HRI uses in your own (public/private) IBM Cloud account.","title":"HRI Dependencies Configuration/Setup"},{"location":"deployment/#create-elasticsearch-cloud-resource","text":"HRI Requires an Elasticsearch service deployment in your IBM Cloud account. Navigate to the Resource List in your Cloud account. Click the Create resource button in the top right corner. Enter \u201cElasticsearch\u201d in the catalog search bar and then select the Databases for Elasticsearch tile. Select the appropriate region and then configure the resource by providing a service name and resource group (Note: for all configuration examples below, the Resource Group is \u201cYOUR_Resource_GRP\u201d). You will also need to specify the desired resource allocations for Elasticsearch. Depending on your expected usage, your values may differ, but the values shown below will be sufficient in most cases. Then click the Create button. Once the Elasticsearch instance becomes active, you will need to set an \u201cadmin\u201d password. This is done from the Settings page of the Elasticsearch instance. Click the Service credentials link, and then click the New credential button. Provide a name for the service credential and then add it. This will be needed by the HRI Management API deployment.","title":"Create Elasticsearch cloud resource"},{"location":"deployment/#create-event-streams-cloud-resource","text":"HRI also Requires an Event Streams(Kafka) service deployment in your IBM Cloud account. Navigate to the Resource List in your Cloud account. If an instance of Event Streams already exists in your Cloud account, then the HRI may be able to share that existing instance. If an Event Streams instance does not already exist, then create one by clicking the Create resource button in the top right corner. Enter \u201cEvent Streams\u201d in the catalog search bar and then select the Event Streams tile. Fill in an appropriate region , service name , and resource group . The Enterprise pricing plan (with custom key management via Key Protect) is required for HIPAA data processing. After creating an Enterprise instance of Event Streams, custom key management via Key Protect will need to be explicitly enabled (See Event Streams documentation ). NOTE: The Event Streams Enterprise plan is expensive, which is why we recommend sharing an instance, if possible. In non-Production environments, a Standard plan may be used for testing with non-HIPAA data if your organization\u2019s security team approves. Click the Service credentials link, and then click the New credential button to create a service credential with writer permissions. Provide a name for the service credential. This will be needed by the HRI Management API deployment.","title":"Create Event Streams cloud resource"},{"location":"deployment/#create-authorization-service","text":"The HRI Management API requires an authorization service. Integration testing has been performed with IBM Cloud App ID , but any compliant service can be used. See Authorization for more details about the requirements and how to set up an App ID cloud service.","title":"Create Authorization Service"},{"location":"deployment/#deploy-the-hri-management-api-to-kubernetes","text":"The Management API is packaged in a docker container for Kubernetes deployments and available on GitHub . The service is stateless, so you can scale up to as many pods as needed or even deploy multiple instances that use the same Elasticsearch, Kafka, and Authorization services. The Management API process only needs a valid configuration, which can be provided by any combination of command line arguments, environment variables, and a yaml file. Conflicts are resolved in the order: command line arguments (highest priority), environment variables, and yaml file(lowest priority). Below is a table of the configuration options. CLI Flag Description Required -config-path Path to a config file, default \u201c./config.yml\u201d -elastic-crn Elasticsearch service CRN Yes -elastic-url The base url to the Elasticsearch instance Yes -elastic-cert Elasticsearch TLS public certificate Yes -elastic-username Elasticsearch user name Yes -elastic-password Elasticsearch password Yes -kafka-admin-url Kafka administration url Yes -kafka-brokers A list of Kafka brokers, separated by \u201c,\u201d Yes -kafka-properties A list of Kafka properties, entries separated by \u201c,\u201d, key value pairs separated by \u201c:\u201d Yes -auth-disabled true to disable OAuth Authorization, default: false -oidc-issuer The base URL of the OIDC issuer to use for OAuth authentication (e.g. https://us-south.appid.cloud.ibm.com/oauth/v4/ ). If -auth-disabled is false. -jwt-audience-id The ID of the HRI Management API within your authorization service. If -auth-disabled is false. -tls-enabled Toggle enabling an encrypted connection via TLS, default: false -tls-cert-path Path to the TLS certificate. If -tls-enabled is true. -tls-key-path Path to the TLS key. If -tls-enabled is true. -new-relic-enabled True to enable New Relic monitoring, default: false -new-relic-app-name Application name to aggregate data under in New Relic. If -new-relic-enabled is true. -new-relic-license-key New Relic license key. If -new-relic-enabled is true. -validation True to enable record validation, default false -log-level Minimum Log Level for logging output. Available levels are: Trace, Debug, Info, Warning, Error, Fatal and Panic. Default \u201cinfo\u201d) The associated environment variable is named by removing the leading - , capitalizing all characters, and replacing remaining - with _ . For example, the -auth-disabled flag has the AUTH_DISABLED environment variable. Yaml file entries are named by removing the leading - . For example, the -auth-disabled flag has the auth-disabled yaml entry.","title":"Deploy the HRI Management API to Kubernetes"},{"location":"deployment/#validation-processing","text":"Validation processing is an optional feature that validates the data as it flows through the HRI. See Validation for details on the validation performed and customization options. See Processing Flows for details on how it fits into the overall architecture. Validation processing is built on Apache Flink , a highly available stateful stream processing framework, and requires a \u2018Session\u2019 cluster when enabled. See their deployment documentation for details on how to deploy a cluster. See the Administration page for details about how to deploy validation jobs.","title":"Validation Processing"},{"location":"deployment/#whats-next","text":"To set up your first Tenant and Data Integrator go to the Administration page. For detailed info on how the concept of Tenants and the Data Integrator role underpin the HRI Multitenancy approach, see the Multitenancy page.","title":"What's Next"},{"location":"glossary/","text":"HRI Glossary of Terms \u00b6 A listing of key terms and phrases to help one\u2019s understanding of the Health Record Ingestion service. Batch: \u00b6 A Batch in an HRI context represents a collection of Health Data records that must be processed together in order for that dataset to be ingested correctly into a cloud based solution. Only processing some of the data would result in a bad state for the data consumer. Likewise, if there is an error with processing part of the data, the entire batch may need to be rejected. Batch ID: \u00b6 A unique identifier in HRI, referring to one specific batch. Data Consumer: \u00b6 A \u201cdownstream\u201d service, process, or application receiving and further processing the data passing through the HRI. One such example might be an HRI Pipeline Adapter (based on NiFi) that persists data to COS (Cloud Object Storage). Data Integrator: \u00b6 An \u201cupstream\u201d service, process or application that \u201csends\u201d data into the HRI for processing. Echo: \u00b6 Go web framework used by the Management API. See the Echo website for more details. Elasticsearch: \u00b6 A distributed, open-source document store used to store various types of data. HRI uses Elasticsearch as its primary data store for metadata about batches. Event Streams: \u00b6 The IBM Cloud Event Streams service is a \u201cManaged\u201d service instance of Apache Kafka customized to work with the IBM Cloud. HRI: \u00b6 The Health Record Ingestion service provides a \u201cfront door\u201d for Data Integrators to send data into the cloud, thereby allowing that data to be used by authorized applications that are part of that cloud account. HRI Management API: \u00b6 A RESTful service layer with an OpenAPI 3.0 compliant REST API. The \u201cManagement API\u201d is the external API layer for accessing all public HRI operations that handle Batch, Streams, and Tenant management. Multitenancy: \u00b6 In the context of HRI, multitenancy is a software architecture pattern that allows multiple users or customers to use a single instance of a specific service, sharing computing resources across those customers. For reference, see this link . PHI: \u00b6 Protected Health Information is a term defined by the HIPAA Law and Privacy Rule which provides that \u201ccovered entities\u201d must protect certain sensitive personal information of patients and that patients have certain rights to that information. See this page for HIPAA definition and this page for more in-depth info on HIPAA Privacy Rule and Protected Health Information. Stream: \u00b6 An HRI Stream represents the entire flow through the HRI for a given tenant and Data Integrator. A Stream always has two kafka topics associated with it: an .in topic and a .notification topic. Tenant: \u00b6 A tenant represents one customer or organization that is a \u201cuser\u201d of HRI, on whose behalf a set of Health data is being processed. A tenant can be internal or external to the organization deploying HRI (e.g. IBM Watson Health). All data is isolated between tenants. Terraform: \u00b6 Terraform is an infrastructure as code tool that can be used to set up a cloud environment for an HRI deployment.","title":"Glossary of Terms"},{"location":"glossary/#hri-glossary-of-terms","text":"A listing of key terms and phrases to help one\u2019s understanding of the Health Record Ingestion service.","title":"HRI Glossary of Terms"},{"location":"glossary/#batch","text":"A Batch in an HRI context represents a collection of Health Data records that must be processed together in order for that dataset to be ingested correctly into a cloud based solution. Only processing some of the data would result in a bad state for the data consumer. Likewise, if there is an error with processing part of the data, the entire batch may need to be rejected.","title":"Batch:"},{"location":"glossary/#batch-id","text":"A unique identifier in HRI, referring to one specific batch.","title":"Batch ID:"},{"location":"glossary/#data-consumer","text":"A \u201cdownstream\u201d service, process, or application receiving and further processing the data passing through the HRI. One such example might be an HRI Pipeline Adapter (based on NiFi) that persists data to COS (Cloud Object Storage).","title":"Data Consumer:"},{"location":"glossary/#data-integrator","text":"An \u201cupstream\u201d service, process or application that \u201csends\u201d data into the HRI for processing.","title":"Data Integrator:"},{"location":"glossary/#echo","text":"Go web framework used by the Management API. See the Echo website for more details.","title":"Echo:"},{"location":"glossary/#elasticsearch","text":"A distributed, open-source document store used to store various types of data. HRI uses Elasticsearch as its primary data store for metadata about batches.","title":"Elasticsearch:"},{"location":"glossary/#event-streams","text":"The IBM Cloud Event Streams service is a \u201cManaged\u201d service instance of Apache Kafka customized to work with the IBM Cloud.","title":"Event Streams:"},{"location":"glossary/#hri","text":"The Health Record Ingestion service provides a \u201cfront door\u201d for Data Integrators to send data into the cloud, thereby allowing that data to be used by authorized applications that are part of that cloud account.","title":"HRI:"},{"location":"glossary/#hri-management-api","text":"A RESTful service layer with an OpenAPI 3.0 compliant REST API. The \u201cManagement API\u201d is the external API layer for accessing all public HRI operations that handle Batch, Streams, and Tenant management.","title":"HRI Management API:"},{"location":"glossary/#multitenancy","text":"In the context of HRI, multitenancy is a software architecture pattern that allows multiple users or customers to use a single instance of a specific service, sharing computing resources across those customers. For reference, see this link .","title":"Multitenancy:"},{"location":"glossary/#phi","text":"Protected Health Information is a term defined by the HIPAA Law and Privacy Rule which provides that \u201ccovered entities\u201d must protect certain sensitive personal information of patients and that patients have certain rights to that information. See this page for HIPAA definition and this page for more in-depth info on HIPAA Privacy Rule and Protected Health Information.","title":"PHI:"},{"location":"glossary/#stream","text":"An HRI Stream represents the entire flow through the HRI for a given tenant and Data Integrator. A Stream always has two kafka topics associated with it: an .in topic and a .notification topic.","title":"Stream:"},{"location":"glossary/#tenant","text":"A tenant represents one customer or organization that is a \u201cuser\u201d of HRI, on whose behalf a set of Health data is being processed. A tenant can be internal or external to the organization deploying HRI (e.g. IBM Watson Health). All data is isolated between tenants.","title":"Tenant:"},{"location":"glossary/#terraform","text":"Terraform is an infrastructure as code tool that can be used to set up a cloud environment for an HRI deployment.","title":"Terraform:"},{"location":"monitorlog/","text":"HRI Monitoring & Logging \u00b6 HRI Management API \u00b6 Logging \u00b6 The Management API writes logs to standard out and can be configured to output these levels: Trace, Debug, Info, Warning, Error, Fatal and Panic. The default level is Info. At Info and below, every request\u2019s response code is logged. At Debug and below, the request and responses body is also logged. In addition, every request is assigned a unique ID, which is included is most log messages and all error responses as the errorEventId . This helps facilitate investigation of issues with specific calls. If the client experiencing the issue can provide the errorEventId , the logs can be easily searched for that ID to find all relevant log messages. Monitoring \u00b6 The Management API has two endpoints to facilitate monitoring its health. There is an /alive that is not documented in the API specification as it not intended to be part of the exposed API. This endpoint is for Kubernetes liveness and readiness probes and ensures the Echo web server is up and responding to requests. It requires no authentication and responds with a 200 code and yes . There is an /hri/healthcheck endpoint that checks the health of Elasticsearch and Kafka. It requires no authentication and responds with a 200 code and an empty body on success. If there are any issues a 500 code and body describing the issue is returned. Event Streams \u00b6 Monitoring Topics and Consumers \u00b6 Event Streams provides minimal monitoring with SysDig. Captured metrics include the number of topics, the rate of bytes written and read from each topic, stable consumers, and inactive consumers. See Monitoring Event Streams metrics using IBM Cloud Monitoring with Sysdig for more details. User Data Access Logging \u00b6 HIPAA regulations require logging all access to PHI data . In the HRI, HIPAA data is only persisted in Event Streams, which will automatically log topic creation and deletion to Activity Tracker, see Activity Tracker events for more information. To view the access logs, go to the Activity Tracker instance for your account. It has a LogDNA interface where you can filter logs by source and/or application. Below is a screenshot of a topic creation log entry. When using the HRI to process PHI , additional audit events must be enabled, which requires the Enterprise plan. Audit events for read, write, and delete actions must be enabled on the *.in and *.out Kafka topics, which will result in events being created when that action is taken on the topic. This allows an offering team to audit Event Streams events relevant to potential PHI access. Information about how to enable message audit events can be found here .","title":"Monitoring & Logging"},{"location":"monitorlog/#hri-monitoring-logging","text":"","title":"HRI Monitoring &amp; Logging"},{"location":"monitorlog/#hri-management-api","text":"","title":"HRI Management API"},{"location":"monitorlog/#logging","text":"The Management API writes logs to standard out and can be configured to output these levels: Trace, Debug, Info, Warning, Error, Fatal and Panic. The default level is Info. At Info and below, every request\u2019s response code is logged. At Debug and below, the request and responses body is also logged. In addition, every request is assigned a unique ID, which is included is most log messages and all error responses as the errorEventId . This helps facilitate investigation of issues with specific calls. If the client experiencing the issue can provide the errorEventId , the logs can be easily searched for that ID to find all relevant log messages.","title":"Logging"},{"location":"monitorlog/#monitoring","text":"The Management API has two endpoints to facilitate monitoring its health. There is an /alive that is not documented in the API specification as it not intended to be part of the exposed API. This endpoint is for Kubernetes liveness and readiness probes and ensures the Echo web server is up and responding to requests. It requires no authentication and responds with a 200 code and yes . There is an /hri/healthcheck endpoint that checks the health of Elasticsearch and Kafka. It requires no authentication and responds with a 200 code and an empty body on success. If there are any issues a 500 code and body describing the issue is returned.","title":"Monitoring"},{"location":"monitorlog/#event-streams","text":"","title":"Event Streams"},{"location":"monitorlog/#monitoring-topics-and-consumers","text":"Event Streams provides minimal monitoring with SysDig. Captured metrics include the number of topics, the rate of bytes written and read from each topic, stable consumers, and inactive consumers. See Monitoring Event Streams metrics using IBM Cloud Monitoring with Sysdig for more details.","title":"Monitoring Topics and Consumers"},{"location":"monitorlog/#user-data-access-logging","text":"HIPAA regulations require logging all access to PHI data . In the HRI, HIPAA data is only persisted in Event Streams, which will automatically log topic creation and deletion to Activity Tracker, see Activity Tracker events for more information. To view the access logs, go to the Activity Tracker instance for your account. It has a LogDNA interface where you can filter logs by source and/or application. Below is a screenshot of a topic creation log entry. When using the HRI to process PHI , additional audit events must be enabled, which requires the Enterprise plan. Audit events for read, write, and delete actions must be enabled on the *.in and *.out Kafka topics, which will result in events being created when that action is taken on the topic. This allows an offering team to audit Event Streams events relevant to potential PHI access. Information about how to enable message audit events can be found here .","title":"User Data Access Logging"},{"location":"multitenancy/","text":"Multi-Tenancy \u00b6 Tenants \u00b6 The HRI supports the concept of multi-tenancy . Each Tenant represents an organization on whose behalf Health data is being processed. Each Tenant\u2019s data is always separated logically in storage and during processing. Separate indices are used in Elasticsearch, separate topics in Kafka, and separate jobs in Flink. Data Integrators \u00b6 Additionally, data is isolated between Data Integrators. A Data Integrator is the organization supplying the Health data on behalf of one or more tenants. Separate Kafka topics are used for every unique combination of Tenant and Data Integrator, and the HRI Management API prevents Data Integrators from accessing batches they did not create. Data Consumers \u00b6 Data Consumers are downstream processes (created by some \u201ccustomer\u201d org) that read data from the HRI. The HRI is designed so that a single data consumer would read data for a single tenant, but it does not prevent a consumer from reading data for multiple tenants. Data Consumers can access batches created by any Data Integrator . Multitenancy Architecture \u00b6 This diagram shows the flow of two different tenant\u2019s data through the HRI via coloring. Red indicates Tenant 1\u2019s data and blue indicates Tenant 2\u2019s data. Data Integrator B is both red and blue, because it\u2019s processing data for both tenants. Please note that authorization is used to control access to tenant\u2019s data both in Kafka and in the Management API. See the sections below for more details. Streams \u00b6 A Stream is a path of data through the HRI and consists of Kafka topics and a validation processing job, when enabled. There must be at least one stream for every unique combination of tenant and Data Integrator to satisfy HIPAA requirements. This logically separates the data and enables access to be restricted by tenant and Data Integrator. Note that different Data Integrators for the same tenant are not allowed to access each other\u2019s data. To facilitate this, topics and validation jobs are named using the tenant and Data Integrator\u2019s name, e.g. ingest.tenant.data-integrator.* . In the example above, Integrator B is processing data from two tenants and writes data to two topics, separating them by tenant. Credentials provided to Data Integrators must be locked down to specific topics. Data Types \u00b6 HRI is agnostic to the type of data being written to Kafka. In practice, a Data Integrator often provides a specific type of data (claims, clinical, imagery, etc.) to the HRI. Users/Consumers of HRI may also want to separate provided data by type. This can be done by creating additional topics and including another (data type) identifier at the end of the topic name before .in . For example, ingest.t1.di1.claims.in . Note that Inbound topics must end with .in . HRI Management API \u00b6 The Management API stores metadata about batches in separate indexes (in its Elasticsearch data store). All API endpoints include a tenant ID to support data segregation by tenant. Batch operations are segregated by Data Integrator such that one Integrator cannot access or modify the batches that another Integrator owns. See the Authorization page for more details.","title":"Multi-Tenancy"},{"location":"multitenancy/#multi-tenancy","text":"","title":"Multi-Tenancy"},{"location":"multitenancy/#tenants","text":"The HRI supports the concept of multi-tenancy . Each Tenant represents an organization on whose behalf Health data is being processed. Each Tenant\u2019s data is always separated logically in storage and during processing. Separate indices are used in Elasticsearch, separate topics in Kafka, and separate jobs in Flink.","title":"Tenants"},{"location":"multitenancy/#data-integrators","text":"Additionally, data is isolated between Data Integrators. A Data Integrator is the organization supplying the Health data on behalf of one or more tenants. Separate Kafka topics are used for every unique combination of Tenant and Data Integrator, and the HRI Management API prevents Data Integrators from accessing batches they did not create.","title":"Data Integrators"},{"location":"multitenancy/#data-consumers","text":"Data Consumers are downstream processes (created by some \u201ccustomer\u201d org) that read data from the HRI. The HRI is designed so that a single data consumer would read data for a single tenant, but it does not prevent a consumer from reading data for multiple tenants. Data Consumers can access batches created by any Data Integrator .","title":"Data Consumers"},{"location":"multitenancy/#multitenancy-architecture","text":"This diagram shows the flow of two different tenant\u2019s data through the HRI via coloring. Red indicates Tenant 1\u2019s data and blue indicates Tenant 2\u2019s data. Data Integrator B is both red and blue, because it\u2019s processing data for both tenants. Please note that authorization is used to control access to tenant\u2019s data both in Kafka and in the Management API. See the sections below for more details.","title":"Multitenancy Architecture"},{"location":"multitenancy/#streams","text":"A Stream is a path of data through the HRI and consists of Kafka topics and a validation processing job, when enabled. There must be at least one stream for every unique combination of tenant and Data Integrator to satisfy HIPAA requirements. This logically separates the data and enables access to be restricted by tenant and Data Integrator. Note that different Data Integrators for the same tenant are not allowed to access each other\u2019s data. To facilitate this, topics and validation jobs are named using the tenant and Data Integrator\u2019s name, e.g. ingest.tenant.data-integrator.* . In the example above, Integrator B is processing data from two tenants and writes data to two topics, separating them by tenant. Credentials provided to Data Integrators must be locked down to specific topics.","title":"Streams"},{"location":"multitenancy/#data-types","text":"HRI is agnostic to the type of data being written to Kafka. In practice, a Data Integrator often provides a specific type of data (claims, clinical, imagery, etc.) to the HRI. Users/Consumers of HRI may also want to separate provided data by type. This can be done by creating additional topics and including another (data type) identifier at the end of the topic name before .in . For example, ingest.t1.di1.claims.in . Note that Inbound topics must end with .in .","title":"Data Types"},{"location":"multitenancy/#hri-management-api","text":"The Management API stores metadata about batches in separate indexes (in its Elasticsearch data store). All API endpoints include a tenant ID to support data segregation by tenant. Batch operations are segregated by Data Integrator such that one Integrator cannot access or modify the batches that another Integrator owns. See the Authorization page for more details.","title":"HRI Management API"},{"location":"performance/","text":"Performance & Sizing \u00b6 The key performance metric for the HRI is how much Health-related data can be streamed through the system, which is highly dependent on the Apache Kafka instance being used by the HRI. In the IBM Cloud, this is typically Event Streams . Thus, our performance testing has focused on measuring the throughput of Event Streams and not other components like the HRI Management API or Elasticsearch . Without Validation \u00b6 If validation is not enabled, then the throughput of Event Streams is the only factor. We decided not to test the throughput of Event Streams service, as it is already in the IBM Cloud documentation . Additionally, the consumption rate is usually the limiting factor and mostly dependent on what processing is done to the data. For example, a Data Consumer that writes the records to a database or file storage system will typically be constrained by how fast it can write the data, not how fast it can read from Event Streams. Every solution will need to test and determine the throughput of their pipeline. With Validation \u00b6 When validation is enabled, the Flink validation jobs copy the data from an input topic to an output topic as it validates the data. In summary, our tests revealed that the Flink jobs were able to process the data as fast as it could be written to Event Streams. The throughput varies per run between 10 - 18 records per second and 9 - 10 MB per second normalized by the number of Kafka partitions, using a \u2018standard\u2019 plan Event Streams instance. Testing Methodology \u00b6 Test Data \u00b6 We use a static set of Synthetic Mass records for performance testing our FHIR validation. The original dataset is available here (the \u201c1K sample Synthetic Patient Records FHIR R4\u201d download.) Records larger than 5 MB were removed and \u2018zstd\u2019 compression was enabled in order to prevent exceeding the 1 MB maximum message size imposed by Event Streams. Any invalid records were also removed. What remains is 1135 files totaling 1.02 GB stored in a COS bucket. Test Performed \u00b6 The performance test runs nightly and follows these basic steps. Download the test data and sorts it by filename Start a batch Write all the test data to the input topic Read all the messages from the output topic Wait for the Batch \u2018completed\u2019 notification in the notification topic Compute the job throughput in records and megabytes per second using the batch start and end times The test uses two partitions for the input and output topics, and a Flink job parallelism of two. The data is written by a Kubernetes job with two pods. Tests were run against a \u2018standard\u2019 plan Event Streams instance.","title":"Performance"},{"location":"performance/#performance-sizing","text":"The key performance metric for the HRI is how much Health-related data can be streamed through the system, which is highly dependent on the Apache Kafka instance being used by the HRI. In the IBM Cloud, this is typically Event Streams . Thus, our performance testing has focused on measuring the throughput of Event Streams and not other components like the HRI Management API or Elasticsearch .","title":"Performance &amp; Sizing"},{"location":"performance/#without-validation","text":"If validation is not enabled, then the throughput of Event Streams is the only factor. We decided not to test the throughput of Event Streams service, as it is already in the IBM Cloud documentation . Additionally, the consumption rate is usually the limiting factor and mostly dependent on what processing is done to the data. For example, a Data Consumer that writes the records to a database or file storage system will typically be constrained by how fast it can write the data, not how fast it can read from Event Streams. Every solution will need to test and determine the throughput of their pipeline.","title":"Without Validation"},{"location":"performance/#with-validation","text":"When validation is enabled, the Flink validation jobs copy the data from an input topic to an output topic as it validates the data. In summary, our tests revealed that the Flink jobs were able to process the data as fast as it could be written to Event Streams. The throughput varies per run between 10 - 18 records per second and 9 - 10 MB per second normalized by the number of Kafka partitions, using a \u2018standard\u2019 plan Event Streams instance.","title":"With Validation"},{"location":"performance/#testing-methodology","text":"","title":"Testing Methodology"},{"location":"performance/#test-data","text":"We use a static set of Synthetic Mass records for performance testing our FHIR validation. The original dataset is available here (the \u201c1K sample Synthetic Patient Records FHIR R4\u201d download.) Records larger than 5 MB were removed and \u2018zstd\u2019 compression was enabled in order to prevent exceeding the 1 MB maximum message size imposed by Event Streams. Any invalid records were also removed. What remains is 1135 files totaling 1.02 GB stored in a COS bucket.","title":"Test Data"},{"location":"performance/#test-performed","text":"The performance test runs nightly and follows these basic steps. Download the test data and sorts it by filename Start a batch Write all the test data to the input topic Read all the messages from the output topic Wait for the Batch \u2018completed\u2019 notification in the notification topic Compute the job throughput in records and megabytes per second using the batch start and end times The test uses two partitions for the input and output topics, and a Flink job parallelism of two. The data is written by a Kubernetes job with two pods. Tests were run against a \u2018standard\u2019 plan Event Streams instance.","title":"Test Performed"},{"location":"processflow/","text":"Processing Flows \u00b6 With Validation \u00b6 This diagram depicts the \u201chappy-path\u201d flow of health data through the HRI to a consumer for a single batch, when validation is enabled. A batch is a collection of data that must be processed together . Only processing some of the data would result in a bad state for the data consumer. Likewise, if there is an error with processing part of the data, typically the entire batch must be rejected. Steps \u00b6 The Data Integrator creates a new batch. The Management API writes a \u2018started\u2019 batch notification message to the associated notification topic. The Data Consumer receives the batch notification message and prepares for the incoming data. Data Integrator writes the data to the Kafka *.in topic. Validation processing validates the data and writes the data to the Kafka *.out topic. The Data Consumer may now begin reading the data from the Kafka topic but can choose to wait until step 11 to begin reading the data. The Data Integrator completes writing all data contained in this batch and then signals to the Management API that it completed sending the data. The Management API writes a \u2018sendCompleted\u2019 batch notification message to the associated notification topic. The Validation processing finishes validating all the Kafka records for this batch and then signals to the Management API that it completed processing the data. The Management API writes a \u2018completed\u2019 batch notification message to the associated notification topic. The Data Consumer receives the \u2018completed\u2019 batch notification message. Alternate Flows \u00b6 Batch Termination \u00b6 If the Data Integrator encounters an error after creating a batch in step 1, they may send a request to the Management API to terminate the batch. The Management API will then write a \u2018terminated\u2019 batch notification message to the associated notification topic. Validation processing will stop forwarding records for this batch, but any records that have already been written to the *.out topic are not deleted. Invalid Records \u00b6 If the Validation processing encounters an invalid record, an invalid record notification is written to the *.invalid topic. This record contains a failure message and the topic, partition, and offset of the original message. The contents of the original message are not included. Batch Failures \u00b6 Validation processing can fail Batches for two reasons: the number of invalid records reaches the Batch invalid threshold, or the number of records does not match the Batch expected record count . When this happens, a \u2018failed\u2019 batch notification message will be written to the associated notification topic. Validation processing will also stop forwarding records for this batch, but any records that have already been written to the *.out topic are not deleted. Without Validation \u00b6 If validation is disabled, the architecture is simpler, and the flow is shortened. Below is the simplified architecture diagram. There\u2019s no validation processing and only the *.in and *.notification topics are present. Below is the shortened flow. Steps \u00b6 The Data Integrator creates a new batch. The Management API writes a batch notification message to the associated notification topic. The Data Consumer receives the batch notification message. Data Integrator writes the data to the correct Kafka *.in topic. The Data Consumer may now begin reading the data from the Kafka topic but can choose to wait until step 8 to begin reading the data. The Data Integrator completes writing all data contained in this batch and it then signals to the Management API that it completed sending the data for the batch. The Management API writes a batch notification message to the associated notification topic. The Data Consumer receives the batch notification message. Batch Status Transitions \u00b6 Batches can go through several status changes, and for each change a notification message is written to the *.notification topic. Below is a state diagram indicating all the states the transition events. Note that the transitions are slightly different depending on whether validation has been enabled. Interleaved Batches \u00b6 The HRI does not prevent the Data Integrator from writing multiples batches into the same topic at the same time. Every record will have a header value that specifies the \u201cbatchId\u201d , which is returned from the Management API (see hri-api-spec/management-api/management.yml ), so the Data Consumer can distinguish each one. In practice, the Data Integrator may only write one batch at a time. As necessary, additional input topics can be created to prevent the interleaving of batches or data types. However, please note that, in general, Kafka performs better with a small number of large topics .","title":"Processing Flows"},{"location":"processflow/#processing-flows","text":"","title":"Processing Flows"},{"location":"processflow/#with-validation","text":"This diagram depicts the \u201chappy-path\u201d flow of health data through the HRI to a consumer for a single batch, when validation is enabled. A batch is a collection of data that must be processed together . Only processing some of the data would result in a bad state for the data consumer. Likewise, if there is an error with processing part of the data, typically the entire batch must be rejected.","title":"With Validation"},{"location":"processflow/#steps","text":"The Data Integrator creates a new batch. The Management API writes a \u2018started\u2019 batch notification message to the associated notification topic. The Data Consumer receives the batch notification message and prepares for the incoming data. Data Integrator writes the data to the Kafka *.in topic. Validation processing validates the data and writes the data to the Kafka *.out topic. The Data Consumer may now begin reading the data from the Kafka topic but can choose to wait until step 11 to begin reading the data. The Data Integrator completes writing all data contained in this batch and then signals to the Management API that it completed sending the data. The Management API writes a \u2018sendCompleted\u2019 batch notification message to the associated notification topic. The Validation processing finishes validating all the Kafka records for this batch and then signals to the Management API that it completed processing the data. The Management API writes a \u2018completed\u2019 batch notification message to the associated notification topic. The Data Consumer receives the \u2018completed\u2019 batch notification message.","title":"Steps"},{"location":"processflow/#alternate-flows","text":"","title":"Alternate Flows"},{"location":"processflow/#batch-termination","text":"If the Data Integrator encounters an error after creating a batch in step 1, they may send a request to the Management API to terminate the batch. The Management API will then write a \u2018terminated\u2019 batch notification message to the associated notification topic. Validation processing will stop forwarding records for this batch, but any records that have already been written to the *.out topic are not deleted.","title":"Batch Termination"},{"location":"processflow/#invalid-records","text":"If the Validation processing encounters an invalid record, an invalid record notification is written to the *.invalid topic. This record contains a failure message and the topic, partition, and offset of the original message. The contents of the original message are not included.","title":"Invalid Records"},{"location":"processflow/#batch-failures","text":"Validation processing can fail Batches for two reasons: the number of invalid records reaches the Batch invalid threshold, or the number of records does not match the Batch expected record count . When this happens, a \u2018failed\u2019 batch notification message will be written to the associated notification topic. Validation processing will also stop forwarding records for this batch, but any records that have already been written to the *.out topic are not deleted.","title":"Batch Failures"},{"location":"processflow/#without-validation","text":"If validation is disabled, the architecture is simpler, and the flow is shortened. Below is the simplified architecture diagram. There\u2019s no validation processing and only the *.in and *.notification topics are present. Below is the shortened flow.","title":"Without Validation"},{"location":"processflow/#steps_1","text":"The Data Integrator creates a new batch. The Management API writes a batch notification message to the associated notification topic. The Data Consumer receives the batch notification message. Data Integrator writes the data to the correct Kafka *.in topic. The Data Consumer may now begin reading the data from the Kafka topic but can choose to wait until step 8 to begin reading the data. The Data Integrator completes writing all data contained in this batch and it then signals to the Management API that it completed sending the data for the batch. The Management API writes a batch notification message to the associated notification topic. The Data Consumer receives the batch notification message.","title":"Steps"},{"location":"processflow/#batch-status-transitions","text":"Batches can go through several status changes, and for each change a notification message is written to the *.notification topic. Below is a state diagram indicating all the states the transition events. Note that the transitions are slightly different depending on whether validation has been enabled.","title":"Batch Status Transitions"},{"location":"processflow/#interleaved-batches","text":"The HRI does not prevent the Data Integrator from writing multiples batches into the same topic at the same time. Every record will have a header value that specifies the \u201cbatchId\u201d , which is returned from the Management API (see hri-api-spec/management-api/management.yml ), so the Data Consumer can distinguish each one. In practice, the Data Integrator may only write one batch at a time. As necessary, additional input topics can be created to prevent the interleaving of batches or data types. However, please note that, in general, Kafka performs better with a small number of large topics .","title":"Interleaved Batches"},{"location":"releases/","text":"HRI Releases \u00b6 This page lists the releases with notes for the HRI with information about how to upgrade from one version to the next. If upgrading multiple versions, check the upgrade notes of all versions in between. Note that the HRI has an overall release number, which is included here. Individual components of the HRI use the overall major and minor release number as a prefix to their release number, in order to make it easy to identify which versions of components are compatible with each other. For example, v2.1-2.1.5 of the Management API is compatible with v2.1-1.0.1 of Flink validation FHIR , because they both have the v2.1 prefix. Unless stated otherwise in the release notes of a specific version, upgrading the HRI should be achievable without downtime. If you are only upgrading to a new patch version, simply upgrade your existing deployment with the patched version. Otherwise, the new version can be deployed and configured separately in a different namespace while the old HRI version is still active, and the old HRI version can be deleted when migration is complete. In this case, be sure to use the same Elasticsearch and Event Streams instances for both of the HRI versions. Again, please see the upgrade notes for all versions between your current and target versions for any additional requirements. v3.x \u00b6 Version 3.x is the latest version focused on making the HRI a cloud-portable service. The Management API was moved from IBM Functions to a standard REST Web server and is packaged into a docker image for Kubernetes based deployments. v3.1.0 \u00b6 Release notes \u00b6 An HRI minor release using public GitHub Alvearie resources. There were also the following changes: Management API Switched Kafka client library to confluent-kafka-go in order to support other Kafka authentication methods. This also caused the configuration options to switch to a list of Kafka connection properties. Upgraded several dependencies Validation(Flink) Upgraded to Flink 1.10.3 Upgrading \u00b6 This release does not contain breaking changes, but there are some changes to the Management API configuration properties. -kafka-properties takes a list of all connection properties. See https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md for the full list. v3.0.0 \u00b6 Release notes \u00b6 Initial open source release of v3.x . This major HRI release ports the Management API from IBM Cloud Functions to a standard REST Web server and is packaged into a docker image for Kubernetes based deployments. This release supports the goal of making the HRI a cloud-portable service. Upgrading \u00b6 This release does not contain breaking changes to the API specification, but there are steps required to migrate the Management API to a Kubernetes based deployment. The same IBM Cloud services (Elasticsearch, Event Streams, AppID, etc.) can be shared between a v2.x deployment and a new v3.x deployment. Below are the migration steps: Deploy v3.0.0 to Kubernetes. The same namespace of an older deployment can be used. If using validation, also update the Flink and Zookeeper deployments. They are backwards compatible with v2.x. Use the same OIDC settings, so that existing configurations are used in the new Management API Kubernetes deployment. If using validation, migrate existing Flink jobs to the latest validation job jars. Stop existing validation jobs with a savepoint. Restart each job from the savepoint using the latest validation job jar and change the Management API URL to the new Kubernetes deployment. All other parameters should remain the same. Have Data Integrators and Consumers migrate from the old Management API (URL) to the new Kubernetes deployment endpoints. When all clients have migrated, delete the IBM Functions namespace that held the old Management API. v2.x \u00b6 Version 2.x uses IBM Functions to deploy the Management API and includes validation processing. It is scheduled for deprecation in Q4 of 2022. Until then security updates and bug fixes will still be made, but no new features will be added. Please upgrade to the latest version at your earliest convenience. v2.1.6 \u00b6 Release notes \u00b6 First Alvearie release using new GitHub Actions workflow. Upgraded to Flink 1.10.3, other minor dependency updates. v2.1.5 \u00b6 Release notes \u00b6 Initial open source release of v2.x . Upgrading \u00b6 This release does not contain breaking changes. Data Integrators and Consumers will maintain access to old batches. However, please note that there are changes to the Elasticsearch mapping template, which must be applied to all existing indices (one for every tenant). Below is a series of upgrade tasks. The Elasticsearch index mappings need to be updated to account for the new expectedRecordCount , actualRecordCount , invalidThreshold , and invalidRecordCount fields. recordCount is considered deprecated as of v2.0.0 and expectedRecordCount should be used instead. Download the new index template from GitHub Follow these instructions to upgrade all the existing Elasticsearch indices with the batch.json index template downloaded in the prior step. The expectedRecordCount , actualRecordCount , invalidThreshold , and invalidRecordCount fields have also been added to the Batch API and Notification Message model, so Data Integrators and Consumers may need to update their integration tools to prevent parsing errors. Additional Batch Notification messages and status values have also been added. For more information, refer to the Batch Status Transitions documentation. v1.x \u00b6 Version 1.x uses IBM Functions to deploy the Management API and does not include validation processing. It is scheduled for deprecation in Q2 of 2022. Until then security updates and bug fixes will still be made, but no new features will be added. Please upgrade to the latest version at your earliest convenience. v1.2.6 \u00b6 Release notes \u00b6 First Alvearie release using new GitHub Actions workflow. There were no substantive changes to the API or deployed code. v1.2.5 \u00b6 Release notes \u00b6 Initial open source release of v1.x .","title":"Releases"},{"location":"releases/#hri-releases","text":"This page lists the releases with notes for the HRI with information about how to upgrade from one version to the next. If upgrading multiple versions, check the upgrade notes of all versions in between. Note that the HRI has an overall release number, which is included here. Individual components of the HRI use the overall major and minor release number as a prefix to their release number, in order to make it easy to identify which versions of components are compatible with each other. For example, v2.1-2.1.5 of the Management API is compatible with v2.1-1.0.1 of Flink validation FHIR , because they both have the v2.1 prefix. Unless stated otherwise in the release notes of a specific version, upgrading the HRI should be achievable without downtime. If you are only upgrading to a new patch version, simply upgrade your existing deployment with the patched version. Otherwise, the new version can be deployed and configured separately in a different namespace while the old HRI version is still active, and the old HRI version can be deleted when migration is complete. In this case, be sure to use the same Elasticsearch and Event Streams instances for both of the HRI versions. Again, please see the upgrade notes for all versions between your current and target versions for any additional requirements.","title":"HRI Releases"},{"location":"releases/#v3x","text":"Version 3.x is the latest version focused on making the HRI a cloud-portable service. The Management API was moved from IBM Functions to a standard REST Web server and is packaged into a docker image for Kubernetes based deployments.","title":"v3.x"},{"location":"releases/#v310","text":"","title":"v3.1.0"},{"location":"releases/#release-notes","text":"An HRI minor release using public GitHub Alvearie resources. There were also the following changes: Management API Switched Kafka client library to confluent-kafka-go in order to support other Kafka authentication methods. This also caused the configuration options to switch to a list of Kafka connection properties. Upgraded several dependencies Validation(Flink) Upgraded to Flink 1.10.3","title":"Release notes"},{"location":"releases/#upgrading","text":"This release does not contain breaking changes, but there are some changes to the Management API configuration properties. -kafka-properties takes a list of all connection properties. See https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md for the full list.","title":"Upgrading"},{"location":"releases/#v300","text":"","title":"v3.0.0"},{"location":"releases/#release-notes_1","text":"Initial open source release of v3.x . This major HRI release ports the Management API from IBM Cloud Functions to a standard REST Web server and is packaged into a docker image for Kubernetes based deployments. This release supports the goal of making the HRI a cloud-portable service.","title":"Release notes"},{"location":"releases/#upgrading_1","text":"This release does not contain breaking changes to the API specification, but there are steps required to migrate the Management API to a Kubernetes based deployment. The same IBM Cloud services (Elasticsearch, Event Streams, AppID, etc.) can be shared between a v2.x deployment and a new v3.x deployment. Below are the migration steps: Deploy v3.0.0 to Kubernetes. The same namespace of an older deployment can be used. If using validation, also update the Flink and Zookeeper deployments. They are backwards compatible with v2.x. Use the same OIDC settings, so that existing configurations are used in the new Management API Kubernetes deployment. If using validation, migrate existing Flink jobs to the latest validation job jars. Stop existing validation jobs with a savepoint. Restart each job from the savepoint using the latest validation job jar and change the Management API URL to the new Kubernetes deployment. All other parameters should remain the same. Have Data Integrators and Consumers migrate from the old Management API (URL) to the new Kubernetes deployment endpoints. When all clients have migrated, delete the IBM Functions namespace that held the old Management API.","title":"Upgrading"},{"location":"releases/#v2x","text":"Version 2.x uses IBM Functions to deploy the Management API and includes validation processing. It is scheduled for deprecation in Q4 of 2022. Until then security updates and bug fixes will still be made, but no new features will be added. Please upgrade to the latest version at your earliest convenience.","title":"v2.x"},{"location":"releases/#v216","text":"","title":"v2.1.6"},{"location":"releases/#release-notes_2","text":"First Alvearie release using new GitHub Actions workflow. Upgraded to Flink 1.10.3, other minor dependency updates.","title":"Release notes"},{"location":"releases/#v215","text":"","title":"v2.1.5"},{"location":"releases/#release-notes_3","text":"Initial open source release of v2.x .","title":"Release notes"},{"location":"releases/#upgrading_2","text":"This release does not contain breaking changes. Data Integrators and Consumers will maintain access to old batches. However, please note that there are changes to the Elasticsearch mapping template, which must be applied to all existing indices (one for every tenant). Below is a series of upgrade tasks. The Elasticsearch index mappings need to be updated to account for the new expectedRecordCount , actualRecordCount , invalidThreshold , and invalidRecordCount fields. recordCount is considered deprecated as of v2.0.0 and expectedRecordCount should be used instead. Download the new index template from GitHub Follow these instructions to upgrade all the existing Elasticsearch indices with the batch.json index template downloaded in the prior step. The expectedRecordCount , actualRecordCount , invalidThreshold , and invalidRecordCount fields have also been added to the Batch API and Notification Message model, so Data Integrators and Consumers may need to update their integration tools to prevent parsing errors. Additional Batch Notification messages and status values have also been added. For more information, refer to the Batch Status Transitions documentation.","title":"Upgrading"},{"location":"releases/#v1x","text":"Version 1.x uses IBM Functions to deploy the Management API and does not include validation processing. It is scheduled for deprecation in Q2 of 2022. Until then security updates and bug fixes will still be made, but no new features will be added. Please upgrade to the latest version at your earliest convenience.","title":"v1.x"},{"location":"releases/#v126","text":"","title":"v1.2.6"},{"location":"releases/#release-notes_4","text":"First Alvearie release using new GitHub Actions workflow. There were no substantive changes to the API or deployed code.","title":"Release notes"},{"location":"releases/#v125","text":"","title":"v1.2.5"},{"location":"releases/#release-notes_5","text":"Initial open source release of v1.x .","title":"Release notes"},{"location":"roadmap/","text":"Roadmap \u00b6 Below are the additional features that are scheduled for development. They are ordered by priority, but the ordering may change in the future. 1. Azure Cloud Support \u00b6 This includes the required changes to enable HRI to be functional on an MS Azure cloud instance. Red Hat OpenShift will be used as a standard platform on which HRI\u2019s internal services will run. External service dependencies such as Elastic, Kafka, OAuth 2.0 implementation will target either a specific MS Azure services or a generic cross-cloud solution, based on what makes the most sense for future Azure deployments. 2. Archive & Replay \u00b6 Because the HRI leverages Apache Kafka , it has the capability to store and replay data over a short period of time. The amount of time is configurable, but it is not designed for long term storage and replay, and it does not support Batch semantics. The plan is to develop a long-term storage (archive) mechanism for batches, which could then be selectively replayed at any time in the future. Replaying the data would stream it back through the HRI as if it were sent by a Data Integrator. This would enable downstream services to recover from processing errors without requiring the Data Integrator to resend the data through the HRI. It can also serve as a record of the original data sent to the cloud for data lineage purposes.","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"Below are the additional features that are scheduled for development. They are ordered by priority, but the ordering may change in the future.","title":"Roadmap"},{"location":"roadmap/#1-azure-cloud-support","text":"This includes the required changes to enable HRI to be functional on an MS Azure cloud instance. Red Hat OpenShift will be used as a standard platform on which HRI\u2019s internal services will run. External service dependencies such as Elastic, Kafka, OAuth 2.0 implementation will target either a specific MS Azure services or a generic cross-cloud solution, based on what makes the most sense for future Azure deployments.","title":"1. Azure Cloud Support"},{"location":"roadmap/#2-archive-replay","text":"Because the HRI leverages Apache Kafka , it has the capability to store and replay data over a short period of time. The amount of time is configurable, but it is not designed for long term storage and replay, and it does not support Batch semantics. The plan is to develop a long-term storage (archive) mechanism for batches, which could then be selectively replayed at any time in the future. Replaying the data would stream it back through the HRI as if it were sent by a Data Integrator. This would enable downstream services to recover from processing errors without requiring the Data Integrator to resend the data through the HRI. It can also serve as a record of the original data sent to the cloud for data lineage purposes.","title":"2. Archive &amp; Replay"},{"location":"troubleshooting/","text":"Troubleshooting \u00b6 HRI Management API Issues \u00b6 Authentication is possible but has failed or not yet been provided \u00b6 NOTE: Only valid for versions prior to v3.0.0 Issue: the Management API responds with this error: { \"code\" : \"2cde977c23b3ab78befed56a6aac3820\" , \"error\" : \"Authentication is possible but has failed or not yet been provided.\" } Cause: the IBM Functions API Gateway is unable to authenticate with the backend IBM Function Actions. Actions have an API key and the API Gateway must be configured to use that API key. Typically, this is due to the API Gateway missing or having the wrong API key. Fix: Option 1 . Try redeploying the Management API. This will generate a new API key and set it on the Actions and the API Gateway endpoints. Option 2 . Manually update the API endpoints. You will need the IBM Cloud CLI and Functions plugin. See these instructions for installing them. Set the CLI resource group to where the Management API is deployed. ibmcloud target -g <resource_group> Set the CLI Functions namespace to where the Management API is deployed. ibmcloud fn namespace target <namespace> Recreate all the API methods. Below is an example of the create batch endpoint. Reference mgmt-api-manifest.yml for a complete list of the endpoints and their associated actions. ibmcloud fn api create /hri /tenants/{tenantId}/batches post hri_mgmt_api/create_batch --response-type http Option 3 . Manually update the API gateway JSON configuration. Follow steps 1-3 above to set up the IBM CLI. Download the API json configuration. ibmcloud fn api get hri-batches > api.json Get the API key set for the Actions. Run ibmcloud fn package get hri_mgmt_api and the output should have a \"require-whisk-auth\" entry for each action. It should be the same value for every action. E.g: { \"key\": \"require-whisk-auth\", \"value\": \"hIadvQ8w4nkJeWa3i7OPDb9WqTAUV9d6\" }, Edit the api.json file and add or replace the API key. There will be a x-ibm-configuration element, and a couple of layers inside, an array of execute elements, one for each endpoint. Each of these needs to have a set-variable.actions element that sets message.headers.X-Require-Whisk-Auth to the API key. Make sure there is one for every endpoint and that they match the API key from step 3. { \"execute\" : [ { \"invoke\" : { \"target-url\" : \"https://us-south.functions.cloud.ibm.com/api/v1/web/a98e053a-4a77-46b3-9791-53d4dfa370fb/hri_mgmt_api/get_batches.http$(request.path)\" , \"verb\" : \"keep\" } }, { \"set-variable\" : { \"actions\" : [ { \"set\" : \"message.headers.X-Require-Whisk-Auth\" , \"value\" : \"hIadvQ8w4nkJeWa3i7OPDb9WqTAUV9d6\" } ] } } ], \"operations\" : [ \"getTenantsTenantidBatches\" ] } Event Streams Issues \u00b6 SSL Certificate Issues \u00b6 Issue: when attempting to connect to Event Streams you receive SSL errors. In Java, you might get this exception sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target . With curl, you get this error SSL Certificate: Invalid certificate chain . Cause: the Enterprise Event Streams brokers use a certificate signed by issuer=C = US, O = Let\u2019s Encrypt, CN = Let\u2019s Encrypt Authority X3 which is different from Standard versions. Whatever client is connecting to Event Streams is missing the Let\u2019s Encrypt public certificate. For Java, this certificate was added in version 8u101 . Fix: add the Let\u2019s Encrypt public certificate to your trusted root certificates. This varies depending on what technology you are using. Java comes with its own \u2018truststore\u2019 with root certificates and all operating systems also store root certificates. The Let\u2019s Encrypt certificates can be downloaded here . There are several guides online for adding certificates to the Java trust store. Here\u2019s one from Oracle. Here is a guide for several operating systems. Elasticsearch \u00b6 Setup Curl with IBM Elasticsearch \u00b6 Below are instructions for setting up Curl to interact with the Elasticsearch API directly. This can be useful when investigating issues or in some update scenarios like modifying existing index templates. Note: this requires the use of the command-line cURL tool In your IBM Cloud Elasticsearch (Document Store) account, you will need to have either: an existing Service Credential you can access OR create a new Service Credential that you will use below for the $USERNAME and $PASSWORD needed to run the Elasticsearch REST commands To create a new Service Credential: Navigate to the Elasticsearch service instance in your IBM Cloud account . Then click on the \u201cService Credentials\u201d link on the left-hand side Elasticsearch service menu: On the Service Credentials page click the New Credential button: You will name your new credential and click the \u201cAdd\u201d button. After that, your credential will be generated for you and you will be able to click on the \u201cView Credentials\u201d link. From this expanded service credentials window, you may retrieve your newly created Elasticsearch Username and Password that you will need for the Elasticsearch REST commands using cURL. Next, you will need to download the certificate and export it, so cURL can authenticate with the IBM Cloud Elasticsearch instance. To do this: Navigate to the Management screen for your Elasticsearch instance, which will look something like this (ID field obscured in the ScreenCap for security): Scroll down your screen, and in the \u201cConnections\u201d panel, click on the \u201cCLI\u201d toggle. You will be using cURL to run commands from your local environment on the IBM Cloud Elasticsearch instance. See this page in the IBM Cloud Documentation for more info on Connecting to Elasticsearch with cURL . In the \u201cConnections\u201d panel, there is a section for TLS Certificate . You will want to save the text from the \u201cContents\u201d panel in that TLS Certificate section to a local file such as: / Users /[ yourLocalUserName ]/ certs / hri - documentstore . crt Use the contents of this file with cURL by exporting it to CURL_CA_BUNDLE . export CURL_CA_BUNDLE =/ local - path / to / file / hri - documentstore . crt Find the base url in the \u201cPublic CLI endpoint\u201d textbox. In this example it starts with https://8165307 : You can now interact with the Elasticsearch REST API using cURL. For example, you can get the status of the cluster by performing a GET on the _cluster/health?pretty endpoint. curl -X GET -u <username>:<password> \\ https://8165307e-6130-4581-942d-20fcfc4e795d.bkvfvtld0lmh0umkfi70.databases.appdomain.cloud:30600/_cluster/health?pretty You can get the default index template by performing a GET to the _index_template/batches endpoint. curl -X GET -u <username>:<password> \\ https://8165307e-6130-4581-942d-20fcfc4e795d.bkvfvtld0lmh0umkfi70.databases.appdomain.cloud:30600/_index_template/batches You can also get the mapping for existing indexes (one per tenant) at <index>/_mapping . curl -X GET -u <username>:<password> \\ https://8165307e-6130-4581-942d-20fcfc4e795d.bkvfvtld0lmh0umkfi70.databases.appdomain.cloud:30600/test-batches/_mapping Upgrade Existing Elasticsearch Indices \u00b6 Follow the steps to setup curl with IBM Elasticsearch . You will also need a set of credentials, Elasticsearch host and port, and the Elasticsearch certificate. These will be referenced in the following steps using $USERNAME , $PASSWORD , $ELASTIC_HOST , and $ELASTIC_PORT . Download the batches.json index template from GitHub. The latest version can be found here . Make sure you switch the \u2018Branch\u2019 to the release tag for the version you are upgrading to. For example, here is the template for the v2.1-2.1.5 release. The downloaded batches.json index template needs to be updated to only include the mapping properties. Remove lines 2-7 of the file which should include the index_patterns , settings , mapping , and batch fields. Next remove the 2nd and 3rd to last } \u2018s at the end of the file. The file should now just contain a the properties field with a list of field names. It should look like: { \"properties\" : { \"name\" : { \"type\" : \"keyword\" }, ... \"metadata\" : { \"type\" : \"object\" , \"enabled\" : \"false\" } } } List all the Elasticsearch indices curl -u $USERNAME : $PASSWORD https:// $ELASTIC_HOST : $ELASTIC_PORT /_cat/indices/*-batches For each index listed above, update its mapping using the command below replacing <index> with the name of the index. curl -X POST -u $USERNAME : $PASSWORD \\ https:// $ELASTIC_HOST : $ELASTIC_PORT /<index>/_mapping \\ -H 'Content-Type: application/json' \\ -d '@batches.json' Upgrading Elasticsearch Versions \u00b6 The process for upgrading Elasticsearch versions on the IBM Cloud is fairly straightforward. Below are the high level steps: Create a new, higher version of Elasticsearch from an existing backup. See the IBM Cloud documentation for more details. This will copy all your existing tenants and batches. Enable firewall access to the new Elasticsearch instance from your Kubernetes cluster if using the public endpoint. Deploy the new HRI version that supports the upgraded Elasticsearch. See deployment documentation for more details. NOTE: During the upgrade process, the HRI Management API should not be used. Any changes after the Elasticsearch backup is created in step 1, will be lost. This process may be prohibitively long for some solutions due to the amount of time required to change Kubernetes firewall rules. Pre v3.x , Elasticsearch can be configured manually in order to temporarily skip the Kubernetes firewall changes when validation is not enabled . Replace step 2 above with the following: Download the Elasticsearch template and setup script from GitHub. Ensure you select the release tag that matches your HRI version before downloading. The links below default to support-2.x branch versions. batches.json index template elastic.sh setup script Open a IBM Cloud shell: https://cloud.ibm.com/shell Upload the index template and setup script downloaded from step 1 Change elastic.sh to be executable: chmod +x elastic.sh Set ELASTIC_INSTANCE to the name of your new Elasticsearch instance: export ELASTIC_INSTANCE=<instance name> Set ELASTIC_SVC_ACCOUNT to the name of your Elasticsearch service credential: export ELASTIC_SVC_ACCOUNT=<service credential name> Run the elastic.sh script. Below is an example: $ ./elastic.sh Looking up Elasticsearch connection information and credentials ES id: \"crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/49f48a067ac4433a911740653049e83d:81ce8a78-d119-4ef0-a65a-9b4ba741d52d::\" ES baseUrl: https://81ce8a78-d119-4ef0-a65a-9b4ba741d52d.c00no9sd0hobi6kj68i0.databases.appdomain.cloud:31798 Setting Elasticsearch auto index creation to false { \"acknowledged\" :true, \"persistent\" : { \"action\" : { \"auto_create_index\" : \"false\" }} , \"transient\" : {}} Setting Elasticsearch Batches index template { \"acknowledged\" :true } Elasticsearch configuration complete Then proceed with deploying the new HRI version. The deployment scripts will still try to configure Elasticsearch and report a failure for the smoke test, but the new Management API will still be deployed and function correctly. You can verify this by ensuring the /hri/healthcheck endpoint returns a 200 . You should still enable firewall access to the new Elasticsearch instance for future deploys. Migrating to another Elasticsearch Instance \u00b6 The IBM Cloud does not provide an automated process for migrating an Elasticsearch instance to another account, but it can be done manually using Elasticsearch\u2019s snapshot and restore functionality with the S3 plugin. These enable creating a snapshot of an Elasticsearch instance, storing it in a COS bucket, and then restoring the snapshot to a different Elasticsearch instance. The migration should be performed after installing the HRI in the target account. Note : this does require downtime for the HRI application . The steps are detailed below. In the new account, create a COS bucket to store the Elasticsearch snapshots and a set of HMAC credentials with read and write access. Use the standard storage class, and if the destination Elasticsearch instance is in a different region than the source Elasticsearch instance, ensure you select \u2018 Cross Region \u2018 resiliency. Create the new Elasticsearch instance that you want to migrate to. This will be referred to as the \u201cdestination instance\u201d. If desired, you can also upgrade to the next major Elasticsearch version. You cannot upgrade more than one major version. Setup curl to communicate with the source Elasticsearch instance. See the instructions above for more details. The examples below will use SRC and DEST prefixes on environment variables to differentiate between the source and destination Elasticsearch connection parameters, e.g. $SRC_USERNAME , $SRC_PASSWORD , $SRC_HOST , and $SRC_PORT . Register the COS bucket from step 1 with the source Elasticsearch instance using the curl statement below. You will need to fill in the bucket name, endpoint, and HMAC credentials. We recommend using the private endpoint. curl -X POST -u $SRC_USERNAME:$SRC_PASSWORD https://$SRC_HOST:$SRC_PORT/_snapshot/migration \\ -H 'Content-Type: application/json' -d' { \"type\": \"s3\", \"settings\": { \"endpoint\": \"s3.private.us.cloud-object-storage.appdomain.cloud\", \"bucket\": \"bucket_name\", \"access_key\": \"xxxxx\", \"secret_key\": \"xxxxx\" } }' Repeat the prior two steps with the destination Elasticsearch instance. You may want to setup curl in a different terminal as you will need to run commands against both instances. When registering the COS bucket, add \"readonly\": true . Below is an example. curl -X POST -u $DEST_USERNAME:$DEST_PASSWORD https://$DEST_HOST:$DEST_PORT/_snapshot/migration \\ -H 'Content-Type: application/json' -d' { \"type\": \"s3\", \"settings\": { \"readonly\": true, \"endpoint\": \"s3.private.us.cloud-object-storage.appdomain.cloud\", \"bucket\": \"bucket_name\", \"access_key\": \"xxxxx\", \"secret_key\": \"xxxxx\" } }' Get a list of the indices that need to be migrated. curl -X GET -u $SRC_USERNAME:$SRC_PASSWORD https://$SRC_HOST:$SRC_PORT/_cat/indices/*-batches At this point you must stop operational use of the source HRI/Elasticsearch instance. Create a snapshot using the curl statement below. The example below uses backup-2021-04-23 , but you can use any name for the snapshot. curl -X PUT -u $SRC_USERNAME:$SRC_PASSWORD \"https://$SRC_HOST:$SRC_PORT/_snapshot/migration/backup-2021-04-23?wait_for_completion=true\" \\ -H 'Content-Type: application/json' -d '{\"include_global_state\": false}' Restore the snapshot on the destination Elasticsearch instance. You must list the indices that need to be restored (see step 6), because the snapshot includes an authorization index that cannot be overwritten. curl -X POST -u $DEST_USERNAME:$DEST_PASSWORD \"https://$DEST_HOST:$DEST_PORT/_snapshot/migration/backup-2021-04-23/_restore?wait_for_completion=true\" \\ -H 'Content-Type: application/json' -d '{\"include_global_state\": false, \"indices\": \"1234-batches,5678-batches\"}' You can list the indices to verify they were restored. They will be in a \u2018yellow\u2019 state until all the shards have been replicated. curl -X GET -u $DEST_USERNAME:$DEST_PASSWORD https://$DEST_HOST:$DEST_PORT/_cat/indices/*-batches In addition, each index has a _recovery endpoint that will list additional details about an ongoing restore. curl -X GET -u $DEST_USERNAME:$DEST_PASSWORD https://$DEST_HOST:$DEST_PORT/1234-batches/_recovery?pretty You can now begin using the HRI instance in the new account. If you also migrated to a new OIDC server (e.g. AppID), and want to preserve Data Integrators access to historical batches, see the next section for additional steps required. Additional resources: Here is an example script that can be a starting place for automating the Elasticsearch commands. Elasticsearch snapshot and restore documentation Migrating OIDC or AppID instances \u00b6 When migrating to a new OIDC server such as AppID steps are required to preserve Data Integrator access to historical batches. The HRI prevents Data Integrators from accessing batches that they did not create. This prevents them from accessing information about other Data Integrator\u2019s batches or making modifications to them. This is implemented by taking the sub (subject) claim from the JWT OAuth access token and storing it in the integratorId field of each batch. When Data Integrators access the Management API, the data is filtered to batches where their sub claim matches the integratorId field. If the OIDC instance is changed, the sub claim will also likely change, which means they will not be able to access any of the batches they created using the prior OIDC instance. In AppID, the sub claim is equal to their client ID. Data Consumers are not affected by this, because their access is not filtered. To fix this, the integratorId field will need to be updated on all batches from the source OIDC sub field to the new destination OIDC sub for all Data Integrators. This processes can be rather involved, so please reach out to the HRI development team if you need help creating the necessary Elasticsearch scripts for this update.","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"troubleshooting/#hri-management-api-issues","text":"","title":"HRI Management API Issues"},{"location":"troubleshooting/#authentication-is-possible-but-has-failed-or-not-yet-been-provided","text":"NOTE: Only valid for versions prior to v3.0.0 Issue: the Management API responds with this error: { \"code\" : \"2cde977c23b3ab78befed56a6aac3820\" , \"error\" : \"Authentication is possible but has failed or not yet been provided.\" } Cause: the IBM Functions API Gateway is unable to authenticate with the backend IBM Function Actions. Actions have an API key and the API Gateway must be configured to use that API key. Typically, this is due to the API Gateway missing or having the wrong API key. Fix: Option 1 . Try redeploying the Management API. This will generate a new API key and set it on the Actions and the API Gateway endpoints. Option 2 . Manually update the API endpoints. You will need the IBM Cloud CLI and Functions plugin. See these instructions for installing them. Set the CLI resource group to where the Management API is deployed. ibmcloud target -g <resource_group> Set the CLI Functions namespace to where the Management API is deployed. ibmcloud fn namespace target <namespace> Recreate all the API methods. Below is an example of the create batch endpoint. Reference mgmt-api-manifest.yml for a complete list of the endpoints and their associated actions. ibmcloud fn api create /hri /tenants/{tenantId}/batches post hri_mgmt_api/create_batch --response-type http Option 3 . Manually update the API gateway JSON configuration. Follow steps 1-3 above to set up the IBM CLI. Download the API json configuration. ibmcloud fn api get hri-batches > api.json Get the API key set for the Actions. Run ibmcloud fn package get hri_mgmt_api and the output should have a \"require-whisk-auth\" entry for each action. It should be the same value for every action. E.g: { \"key\": \"require-whisk-auth\", \"value\": \"hIadvQ8w4nkJeWa3i7OPDb9WqTAUV9d6\" }, Edit the api.json file and add or replace the API key. There will be a x-ibm-configuration element, and a couple of layers inside, an array of execute elements, one for each endpoint. Each of these needs to have a set-variable.actions element that sets message.headers.X-Require-Whisk-Auth to the API key. Make sure there is one for every endpoint and that they match the API key from step 3. { \"execute\" : [ { \"invoke\" : { \"target-url\" : \"https://us-south.functions.cloud.ibm.com/api/v1/web/a98e053a-4a77-46b3-9791-53d4dfa370fb/hri_mgmt_api/get_batches.http$(request.path)\" , \"verb\" : \"keep\" } }, { \"set-variable\" : { \"actions\" : [ { \"set\" : \"message.headers.X-Require-Whisk-Auth\" , \"value\" : \"hIadvQ8w4nkJeWa3i7OPDb9WqTAUV9d6\" } ] } } ], \"operations\" : [ \"getTenantsTenantidBatches\" ] }","title":"Authentication is possible but has failed or not yet been provided"},{"location":"troubleshooting/#event-streams-issues","text":"","title":"Event Streams Issues"},{"location":"troubleshooting/#ssl-certificate-issues","text":"Issue: when attempting to connect to Event Streams you receive SSL errors. In Java, you might get this exception sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target . With curl, you get this error SSL Certificate: Invalid certificate chain . Cause: the Enterprise Event Streams brokers use a certificate signed by issuer=C = US, O = Let\u2019s Encrypt, CN = Let\u2019s Encrypt Authority X3 which is different from Standard versions. Whatever client is connecting to Event Streams is missing the Let\u2019s Encrypt public certificate. For Java, this certificate was added in version 8u101 . Fix: add the Let\u2019s Encrypt public certificate to your trusted root certificates. This varies depending on what technology you are using. Java comes with its own \u2018truststore\u2019 with root certificates and all operating systems also store root certificates. The Let\u2019s Encrypt certificates can be downloaded here . There are several guides online for adding certificates to the Java trust store. Here\u2019s one from Oracle. Here is a guide for several operating systems.","title":"SSL Certificate Issues"},{"location":"troubleshooting/#elasticsearch","text":"","title":"Elasticsearch"},{"location":"troubleshooting/#setup-curl-with-ibm-elasticsearch","text":"Below are instructions for setting up Curl to interact with the Elasticsearch API directly. This can be useful when investigating issues or in some update scenarios like modifying existing index templates. Note: this requires the use of the command-line cURL tool In your IBM Cloud Elasticsearch (Document Store) account, you will need to have either: an existing Service Credential you can access OR create a new Service Credential that you will use below for the $USERNAME and $PASSWORD needed to run the Elasticsearch REST commands To create a new Service Credential: Navigate to the Elasticsearch service instance in your IBM Cloud account . Then click on the \u201cService Credentials\u201d link on the left-hand side Elasticsearch service menu: On the Service Credentials page click the New Credential button: You will name your new credential and click the \u201cAdd\u201d button. After that, your credential will be generated for you and you will be able to click on the \u201cView Credentials\u201d link. From this expanded service credentials window, you may retrieve your newly created Elasticsearch Username and Password that you will need for the Elasticsearch REST commands using cURL. Next, you will need to download the certificate and export it, so cURL can authenticate with the IBM Cloud Elasticsearch instance. To do this: Navigate to the Management screen for your Elasticsearch instance, which will look something like this (ID field obscured in the ScreenCap for security): Scroll down your screen, and in the \u201cConnections\u201d panel, click on the \u201cCLI\u201d toggle. You will be using cURL to run commands from your local environment on the IBM Cloud Elasticsearch instance. See this page in the IBM Cloud Documentation for more info on Connecting to Elasticsearch with cURL . In the \u201cConnections\u201d panel, there is a section for TLS Certificate . You will want to save the text from the \u201cContents\u201d panel in that TLS Certificate section to a local file such as: / Users /[ yourLocalUserName ]/ certs / hri - documentstore . crt Use the contents of this file with cURL by exporting it to CURL_CA_BUNDLE . export CURL_CA_BUNDLE =/ local - path / to / file / hri - documentstore . crt Find the base url in the \u201cPublic CLI endpoint\u201d textbox. In this example it starts with https://8165307 : You can now interact with the Elasticsearch REST API using cURL. For example, you can get the status of the cluster by performing a GET on the _cluster/health?pretty endpoint. curl -X GET -u <username>:<password> \\ https://8165307e-6130-4581-942d-20fcfc4e795d.bkvfvtld0lmh0umkfi70.databases.appdomain.cloud:30600/_cluster/health?pretty You can get the default index template by performing a GET to the _index_template/batches endpoint. curl -X GET -u <username>:<password> \\ https://8165307e-6130-4581-942d-20fcfc4e795d.bkvfvtld0lmh0umkfi70.databases.appdomain.cloud:30600/_index_template/batches You can also get the mapping for existing indexes (one per tenant) at <index>/_mapping . curl -X GET -u <username>:<password> \\ https://8165307e-6130-4581-942d-20fcfc4e795d.bkvfvtld0lmh0umkfi70.databases.appdomain.cloud:30600/test-batches/_mapping","title":"Setup Curl with IBM Elasticsearch"},{"location":"troubleshooting/#upgrade-existing-elasticsearch-indices","text":"Follow the steps to setup curl with IBM Elasticsearch . You will also need a set of credentials, Elasticsearch host and port, and the Elasticsearch certificate. These will be referenced in the following steps using $USERNAME , $PASSWORD , $ELASTIC_HOST , and $ELASTIC_PORT . Download the batches.json index template from GitHub. The latest version can be found here . Make sure you switch the \u2018Branch\u2019 to the release tag for the version you are upgrading to. For example, here is the template for the v2.1-2.1.5 release. The downloaded batches.json index template needs to be updated to only include the mapping properties. Remove lines 2-7 of the file which should include the index_patterns , settings , mapping , and batch fields. Next remove the 2nd and 3rd to last } \u2018s at the end of the file. The file should now just contain a the properties field with a list of field names. It should look like: { \"properties\" : { \"name\" : { \"type\" : \"keyword\" }, ... \"metadata\" : { \"type\" : \"object\" , \"enabled\" : \"false\" } } } List all the Elasticsearch indices curl -u $USERNAME : $PASSWORD https:// $ELASTIC_HOST : $ELASTIC_PORT /_cat/indices/*-batches For each index listed above, update its mapping using the command below replacing <index> with the name of the index. curl -X POST -u $USERNAME : $PASSWORD \\ https:// $ELASTIC_HOST : $ELASTIC_PORT /<index>/_mapping \\ -H 'Content-Type: application/json' \\ -d '@batches.json'","title":"Upgrade Existing Elasticsearch Indices"},{"location":"troubleshooting/#upgrading-elasticsearch-versions","text":"The process for upgrading Elasticsearch versions on the IBM Cloud is fairly straightforward. Below are the high level steps: Create a new, higher version of Elasticsearch from an existing backup. See the IBM Cloud documentation for more details. This will copy all your existing tenants and batches. Enable firewall access to the new Elasticsearch instance from your Kubernetes cluster if using the public endpoint. Deploy the new HRI version that supports the upgraded Elasticsearch. See deployment documentation for more details. NOTE: During the upgrade process, the HRI Management API should not be used. Any changes after the Elasticsearch backup is created in step 1, will be lost. This process may be prohibitively long for some solutions due to the amount of time required to change Kubernetes firewall rules. Pre v3.x , Elasticsearch can be configured manually in order to temporarily skip the Kubernetes firewall changes when validation is not enabled . Replace step 2 above with the following: Download the Elasticsearch template and setup script from GitHub. Ensure you select the release tag that matches your HRI version before downloading. The links below default to support-2.x branch versions. batches.json index template elastic.sh setup script Open a IBM Cloud shell: https://cloud.ibm.com/shell Upload the index template and setup script downloaded from step 1 Change elastic.sh to be executable: chmod +x elastic.sh Set ELASTIC_INSTANCE to the name of your new Elasticsearch instance: export ELASTIC_INSTANCE=<instance name> Set ELASTIC_SVC_ACCOUNT to the name of your Elasticsearch service credential: export ELASTIC_SVC_ACCOUNT=<service credential name> Run the elastic.sh script. Below is an example: $ ./elastic.sh Looking up Elasticsearch connection information and credentials ES id: \"crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/49f48a067ac4433a911740653049e83d:81ce8a78-d119-4ef0-a65a-9b4ba741d52d::\" ES baseUrl: https://81ce8a78-d119-4ef0-a65a-9b4ba741d52d.c00no9sd0hobi6kj68i0.databases.appdomain.cloud:31798 Setting Elasticsearch auto index creation to false { \"acknowledged\" :true, \"persistent\" : { \"action\" : { \"auto_create_index\" : \"false\" }} , \"transient\" : {}} Setting Elasticsearch Batches index template { \"acknowledged\" :true } Elasticsearch configuration complete Then proceed with deploying the new HRI version. The deployment scripts will still try to configure Elasticsearch and report a failure for the smoke test, but the new Management API will still be deployed and function correctly. You can verify this by ensuring the /hri/healthcheck endpoint returns a 200 . You should still enable firewall access to the new Elasticsearch instance for future deploys.","title":"Upgrading Elasticsearch Versions"},{"location":"troubleshooting/#migrating-to-another-elasticsearch-instance","text":"The IBM Cloud does not provide an automated process for migrating an Elasticsearch instance to another account, but it can be done manually using Elasticsearch\u2019s snapshot and restore functionality with the S3 plugin. These enable creating a snapshot of an Elasticsearch instance, storing it in a COS bucket, and then restoring the snapshot to a different Elasticsearch instance. The migration should be performed after installing the HRI in the target account. Note : this does require downtime for the HRI application . The steps are detailed below. In the new account, create a COS bucket to store the Elasticsearch snapshots and a set of HMAC credentials with read and write access. Use the standard storage class, and if the destination Elasticsearch instance is in a different region than the source Elasticsearch instance, ensure you select \u2018 Cross Region \u2018 resiliency. Create the new Elasticsearch instance that you want to migrate to. This will be referred to as the \u201cdestination instance\u201d. If desired, you can also upgrade to the next major Elasticsearch version. You cannot upgrade more than one major version. Setup curl to communicate with the source Elasticsearch instance. See the instructions above for more details. The examples below will use SRC and DEST prefixes on environment variables to differentiate between the source and destination Elasticsearch connection parameters, e.g. $SRC_USERNAME , $SRC_PASSWORD , $SRC_HOST , and $SRC_PORT . Register the COS bucket from step 1 with the source Elasticsearch instance using the curl statement below. You will need to fill in the bucket name, endpoint, and HMAC credentials. We recommend using the private endpoint. curl -X POST -u $SRC_USERNAME:$SRC_PASSWORD https://$SRC_HOST:$SRC_PORT/_snapshot/migration \\ -H 'Content-Type: application/json' -d' { \"type\": \"s3\", \"settings\": { \"endpoint\": \"s3.private.us.cloud-object-storage.appdomain.cloud\", \"bucket\": \"bucket_name\", \"access_key\": \"xxxxx\", \"secret_key\": \"xxxxx\" } }' Repeat the prior two steps with the destination Elasticsearch instance. You may want to setup curl in a different terminal as you will need to run commands against both instances. When registering the COS bucket, add \"readonly\": true . Below is an example. curl -X POST -u $DEST_USERNAME:$DEST_PASSWORD https://$DEST_HOST:$DEST_PORT/_snapshot/migration \\ -H 'Content-Type: application/json' -d' { \"type\": \"s3\", \"settings\": { \"readonly\": true, \"endpoint\": \"s3.private.us.cloud-object-storage.appdomain.cloud\", \"bucket\": \"bucket_name\", \"access_key\": \"xxxxx\", \"secret_key\": \"xxxxx\" } }' Get a list of the indices that need to be migrated. curl -X GET -u $SRC_USERNAME:$SRC_PASSWORD https://$SRC_HOST:$SRC_PORT/_cat/indices/*-batches At this point you must stop operational use of the source HRI/Elasticsearch instance. Create a snapshot using the curl statement below. The example below uses backup-2021-04-23 , but you can use any name for the snapshot. curl -X PUT -u $SRC_USERNAME:$SRC_PASSWORD \"https://$SRC_HOST:$SRC_PORT/_snapshot/migration/backup-2021-04-23?wait_for_completion=true\" \\ -H 'Content-Type: application/json' -d '{\"include_global_state\": false}' Restore the snapshot on the destination Elasticsearch instance. You must list the indices that need to be restored (see step 6), because the snapshot includes an authorization index that cannot be overwritten. curl -X POST -u $DEST_USERNAME:$DEST_PASSWORD \"https://$DEST_HOST:$DEST_PORT/_snapshot/migration/backup-2021-04-23/_restore?wait_for_completion=true\" \\ -H 'Content-Type: application/json' -d '{\"include_global_state\": false, \"indices\": \"1234-batches,5678-batches\"}' You can list the indices to verify they were restored. They will be in a \u2018yellow\u2019 state until all the shards have been replicated. curl -X GET -u $DEST_USERNAME:$DEST_PASSWORD https://$DEST_HOST:$DEST_PORT/_cat/indices/*-batches In addition, each index has a _recovery endpoint that will list additional details about an ongoing restore. curl -X GET -u $DEST_USERNAME:$DEST_PASSWORD https://$DEST_HOST:$DEST_PORT/1234-batches/_recovery?pretty You can now begin using the HRI instance in the new account. If you also migrated to a new OIDC server (e.g. AppID), and want to preserve Data Integrators access to historical batches, see the next section for additional steps required. Additional resources: Here is an example script that can be a starting place for automating the Elasticsearch commands. Elasticsearch snapshot and restore documentation","title":"Migrating to another Elasticsearch Instance"},{"location":"troubleshooting/#migrating-oidc-or-appid-instances","text":"When migrating to a new OIDC server such as AppID steps are required to preserve Data Integrator access to historical batches. The HRI prevents Data Integrators from accessing batches that they did not create. This prevents them from accessing information about other Data Integrator\u2019s batches or making modifications to them. This is implemented by taking the sub (subject) claim from the JWT OAuth access token and storing it in the integratorId field of each batch. When Data Integrators access the Management API, the data is filtered to batches where their sub claim matches the integratorId field. If the OIDC instance is changed, the sub claim will also likely change, which means they will not be able to access any of the batches they created using the prior OIDC instance. In AppID, the sub claim is equal to their client ID. Data Consumers are not affected by this, because their access is not filtered. To fix this, the integratorId field will need to be updated on all batches from the source OIDC sub field to the new destination OIDC sub for all Data Integrators. This processes can be rather involved, so please reach out to the HRI development team if you need help creating the necessary Elasticsearch scripts for this update.","title":"Migrating OIDC or AppID instances"},{"location":"validation/","text":"Validation Processing \u00b6 The HRI performs both batch and record level validation, when enabled. See Deployment for information on how to enable it, and Processing Flows for information on how it fits into the overall architecture. Batch Validation \u00b6 Batch validation is the same for all jobs and includes the following checks. The number of records received for each batch must equal the expected number of records, which is provided by the Data Integrator. Because Data Integrators could send additional messages at any time, the validation processing will wait a configurable amount of time (the \u201cbatch completion delay\u201d) for extra messages. Then if the number of received records still equals the expected amount, the batch is completed and future records are ignored. The default batch completion delay is 5 minutes, but can be configured for each Flink validation job. The number of invalid records must be lower than the invalid threshold, which is provided by the Data Integrator. Error Handling in Validation \u00b6 During batch validation, HRI Management API calls are made to get batches by batchId and tenantId, mark batches as processing complete, or mark batches as failed. If these API calls fail, the validation process may exit with an exception, retry the request, or continue on ignoring the failure depending on the HTTP status code that was returned. The batch status tracker will call processingComplete or fail for batches. If an HTTP status code of 409 is received, indicating that the batch is already in the desired state, a warning message will be logged but no other errors will be raised. If any other HTTP status code in the 400 s (e.g. 400-Bad Request , 401-Unauthorized ) is returned, the attempt to update the status of the batch will immediately fail since this indicates something is wrong with the status update request, and there is little point in retrying the update. For any other HTTP error status code, the Management API sink will retry the status update for 24 hours, with an exponentially increasing amount of time between each attempt. When the validation processor attempts to get a batch by batchId and tenantId, if an HTTP error status code of 404 is returned indicating that the batch ID or tenant ID was not found, a warning message will be logged, and the records will be written to the *.invalid topic, but no other errors will be raised. This is because such an error is likely due to a mistake by a data integrator, and does not warrant a more aggressive failure by the validation processor. If any other HTTP status code in the 400 s (e.g. 400-Bad Request , 401-Unauthorized ) is returned, the validation will immediately fail. This indicates something is wrong with the HTTP requests, and there is little point in retrying. For any other HTTP error status code, the validation processor will retry for 24 hours, with an exponentially increasing amount of time between each attempt. Record Validation \u00b6 The record level validation is specific to the job type, and the HRI provides two standard validation jobs that can be used by solutions without any additional development. FHIR Validation - validates that every record meets the FHIR v4.0.1 bundle json schema Passthrough Validation - does not perform any record validation and simply passes records through Custom Record Validation \u00b6 Solutions can create their own validation jobs to meet their specific needs. The flink-pipeline-core library provides the Flink job structure and is hosted in GitHub packages. Using this library, solutions only have to implement a custom validator and an entry main method, and then package the code into a deployable jar. It can be written in Java or Scala, and the two standard HRI validators can be used as references. Below is a class diagram of the Flink pipeline core library, the FHIR validation job, and a custom validation job. It shows the class dependencies between the projects and what a custom validation job has to implement. Additional Resources: Flink Pipeline Core - Base Flink library GitHub repo: https://github.com/Alvearie/hri-flink-pipeline-core Dependency declaration: org.alvearie.hri.flink:hri-flink-pipeline-core:3.0-1.0.2 Flink Validation FHIR - FHIR schema validation written in Scala. The FHIR validator is also published as a library in GitHub packages. GitHub repo: https://github.com/Alvearie/hri-flink-validation-fhir FHIR schema validator dependency declaration: org.alvearie..hri.flink:hri-flink-validator-fhir:3.0-1.0.2 Flink Validation Passthrough - Passthrough validation written in Java GitHub repo: https://github.com/Alvearie/hri-flink-validation-passthrough","title":"Validation"},{"location":"validation/#validation-processing","text":"The HRI performs both batch and record level validation, when enabled. See Deployment for information on how to enable it, and Processing Flows for information on how it fits into the overall architecture.","title":"Validation Processing"},{"location":"validation/#batch-validation","text":"Batch validation is the same for all jobs and includes the following checks. The number of records received for each batch must equal the expected number of records, which is provided by the Data Integrator. Because Data Integrators could send additional messages at any time, the validation processing will wait a configurable amount of time (the \u201cbatch completion delay\u201d) for extra messages. Then if the number of received records still equals the expected amount, the batch is completed and future records are ignored. The default batch completion delay is 5 minutes, but can be configured for each Flink validation job. The number of invalid records must be lower than the invalid threshold, which is provided by the Data Integrator.","title":"Batch Validation"},{"location":"validation/#error-handling-in-validation","text":"During batch validation, HRI Management API calls are made to get batches by batchId and tenantId, mark batches as processing complete, or mark batches as failed. If these API calls fail, the validation process may exit with an exception, retry the request, or continue on ignoring the failure depending on the HTTP status code that was returned. The batch status tracker will call processingComplete or fail for batches. If an HTTP status code of 409 is received, indicating that the batch is already in the desired state, a warning message will be logged but no other errors will be raised. If any other HTTP status code in the 400 s (e.g. 400-Bad Request , 401-Unauthorized ) is returned, the attempt to update the status of the batch will immediately fail since this indicates something is wrong with the status update request, and there is little point in retrying the update. For any other HTTP error status code, the Management API sink will retry the status update for 24 hours, with an exponentially increasing amount of time between each attempt. When the validation processor attempts to get a batch by batchId and tenantId, if an HTTP error status code of 404 is returned indicating that the batch ID or tenant ID was not found, a warning message will be logged, and the records will be written to the *.invalid topic, but no other errors will be raised. This is because such an error is likely due to a mistake by a data integrator, and does not warrant a more aggressive failure by the validation processor. If any other HTTP status code in the 400 s (e.g. 400-Bad Request , 401-Unauthorized ) is returned, the validation will immediately fail. This indicates something is wrong with the HTTP requests, and there is little point in retrying. For any other HTTP error status code, the validation processor will retry for 24 hours, with an exponentially increasing amount of time between each attempt.","title":"Error Handling in Validation"},{"location":"validation/#record-validation","text":"The record level validation is specific to the job type, and the HRI provides two standard validation jobs that can be used by solutions without any additional development. FHIR Validation - validates that every record meets the FHIR v4.0.1 bundle json schema Passthrough Validation - does not perform any record validation and simply passes records through","title":"Record Validation"},{"location":"validation/#custom-record-validation","text":"Solutions can create their own validation jobs to meet their specific needs. The flink-pipeline-core library provides the Flink job structure and is hosted in GitHub packages. Using this library, solutions only have to implement a custom validator and an entry main method, and then package the code into a deployable jar. It can be written in Java or Scala, and the two standard HRI validators can be used as references. Below is a class diagram of the Flink pipeline core library, the FHIR validation job, and a custom validation job. It shows the class dependencies between the projects and what a custom validation job has to implement. Additional Resources: Flink Pipeline Core - Base Flink library GitHub repo: https://github.com/Alvearie/hri-flink-pipeline-core Dependency declaration: org.alvearie.hri.flink:hri-flink-pipeline-core:3.0-1.0.2 Flink Validation FHIR - FHIR schema validation written in Scala. The FHIR validator is also published as a library in GitHub packages. GitHub repo: https://github.com/Alvearie/hri-flink-validation-fhir FHIR schema validator dependency declaration: org.alvearie..hri.flink:hri-flink-validator-fhir:3.0-1.0.2 Flink Validation Passthrough - Passthrough validation written in Java GitHub repo: https://github.com/Alvearie/hri-flink-validation-passthrough","title":"Custom Record Validation"}]}