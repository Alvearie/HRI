# API specification

The specification consists of two separate APIs: 

- [Management API specification](#management-api-specification)
- [Data specification](#data-specification)

## Management API specification

This API is defined using the OpenAPI 3.0 specification: [management.yml on GitHub](https://github.com/Alvearie/hri-api-spec/tree/master/management-api/management.yml). To view it, you can either open the file directly or use a program such as [Swagger UI](https://swagger.io/tools/swagger-ui/download/). 

### Elastic tenants

The Health Record Ingestion service has been designed with a [multi-tenant cloud architecture](multitenancy.md). The API mainly contains methods for managing [tenants](glossary.md#tenant), for example, creating, getting, and deleting. Each of these calls takes in the tenantId. To create the index, the Id is appended with the suffix `-batches`.

A `Get` call without a tenantId returns a list of all tenants. The `Get` call, when given a tenantId, returns information on the elastic index of a specific tenant. 

**Table: Fields returned by the Get call**
| Field          | Description                                                        |
| -------------- | ------------------------------------------------------------------ |
| health         | Health of the elastic cluster                                      |
| status         | Status of the index, can be open or closed                         |
| index          | The name of the index: the tenantId with `-batches` appended to it |
| uuid           | Universally-unique identifier                                      |
| pri            | Number of primary shards                                           |
| rep            | Number of replicas                                                 |
| docs.count     | Number of batches documents stored in the index                    |
| docs.deleted   | Number of batches documents deleted from the index                 |
| store.size     | Store size taken by primary and replica shards                     |
| pri.store.size | Store size taken only by primary shards                            |

### Batches

The API contains methods for managing [batches](glossary.md#batch), for example, creating, getting, and updating. Only the `name`, `topic`, and `dataType` fields are required when creating a batch. All other fields are generated by the API.

**Table: Fields for batches**
| Field       | Description                                                                          |
| ----------- | ------------------------------------------------------------------------------------ |
| id          | Automatically generated unique ID                                                    |
| name        | Name of the batch, provided by the Data Integrator                                   |
| topic       | Event Streams (Kafka) topic that contains the data, provided by the Data Integrator  |
| dataType    | The type of data, provided by the Data Integrator                                    |
| status      | Status of the batch: [ started, completed, terminated ]                              |
| startDate   | The date and time the batch was started                                              |
| endDate     | The date and time the batch was completed or terminated                              |
| recordCount | The number of records in the batch, provided by the Data Integrator when completed\* |
| metadata    | Optional custom JSON value\*\*                                                       |

\* The `recordCount` is provided by the Data Integrator when the batch is completed, and thus not always present.

\*\* The `metadata` field is optional and allows the Data Integrator to include any additional information about the batch that Data Consumers might request. This information is included in all notification messages.

### Streams

The API also contains methods for managing Event Streams topics, for example,  creating, getting, and deleting. When creating a stream, only the `numPartitions` and `retentionMs` fields are required. The rest of the topic configurations are optional. 

**Table: Fields for streams**
| Field             | Description                                                                                                 |
| ----------------- | ----------------------------------------------------------------------------------------------------------- |
| id                | Stream ID consisting of a Data Integrator and optional qualifier, delimited by a period (\.\)               |
| numPartitions     | The number of partitions on the topic                                                                       |
| retentionMs       | Length of time in milliseconds before log segments are automatically discarded from a partition             |
| retentionBytes    | Optional maximum size in bytes that a partition can grow before discarding log segments                     |
| cleanupPolicy     | Optional retention policy on old log segments                                                               |
| segmentMs         | Optional time in milliseconds after which Kafka forces the log to roll, even if the segment file isn't full |
| segmentBytes      | Optional log segment file size in bytes                                                                     |
| segmentIndexBytes | Optional size in bytes of the index that maps offsets to file positions                                     |

**Table: Default values and acceptable ranges for optional fields**

| Field             | Default value | Acceptable values, ranges |
| ----------------- | ------------- | ------------------------- |
| retentionBytes    | 1073741824    | [10485760..1073741824]    |
| cleanupPolicy     | delete        | [ delete, compact ]       |
| segmentMs         | nil           | [300000..2592000000]      |
| segmentBytes      | 536870912     | [10485760..536870912]     |
| segmentIndexBytes | nil           | [102400..104857600]       |

If the `cleanupPolicy` field is set to compact, it disables deletion based on time, and ignores the value set for the `retentionMs` field.

Event Streams is an IBM&reg; Cloud-managed version of Apache Kafka. It uses standard Kafka libraries to read data from and write data to topics. For details on connection parameters, see the IBM documentation [Configuring your Kafka API client](https://cloud.ibm.com/docs/EventStreams?topic=EventStreams-kafka_using#kafka_api_client).  

## Data specification

### Health input data: FHIR model

The Health Record Ingestion service does **not** impose any requirements on the format of the content of the health (data) records written to Kafka. IBM Watson Health has selected Fast Healthcare Interoperability Resource (FHIR) as the preferred data model for all health data. [Data Integrators](glossary.md#data-integrator) and [Consumers](glossary.md#data-consumer) must work together to agree on the specifics of the input data, for example, the format and frequency.

### Health Record Ingestion service requirements

The Health Record Ingestion service has the following requirements and recommendations:

- **Batch Id header**: Every record **must** have a header entry with the [batch id](glossary.md#batch-id) that uses the key `batchId`. Data Integrators can include any additional header values, which will get passed downstream to consumers.

- **Zstd compression**: Use `zstd` compression when writing to Kafka by setting the [compression.type](https://kafka.apache.org/documentation/#compression.type) producer configuration. Event Streams throttles network usage and limits Kafka messages to 1 MB. Using compression helps prevent an Event Streams bottleneck.

- **1 MB message limit**: 
  Event Streams limits messages to 1 MB. To prevent sending records over the limit, set the [message.max.bytes](https://kafka.apache.org/documentation/#message.max.bytes) producer configuration to `1000000`. For records greater than 1 MB compressed, there are two strategies:
1. **External references:** For records that have large binary attachments, for example, images or PDF documents, you can provide a reference to the large resource that is included in the message, rather than the (large) resource itself. For example, you could put a Cloud Object Storage (COS) Object URL or another external data store URL, and key into the message.
2. **Splitting up records:** You can split records into smaller parts, send them through the Health Record Ingestion service, and reassemble them using downstream consumers. 

### Notification messages

The notification messages are JSON-encoded batches. They match the schema returned by the Management API described above. For more information, see [batchNotification.json on GitHub](https://github.com/Alvearie/hri-api-spec/tree/master/notifications/batchNotification.json).
